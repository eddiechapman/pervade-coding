Paul Pavlou,,pavlou@temple.edu,Temple University,Secure &Trustworthy Cyberspace,SBE: Small: Protecting Privacy in Cyberspace: From Neuroscience Investigations to Behavioral Interventions,"A key characteristic of cyberspace is the collection of large amounts of data, and people's privacy becomes vulnerable given the hyper-connectivity of cyberspace and the ease of accessing data. This project aims to enhance the safety and trustworthiness of cyberspace by designing choice architecture interventions informed by the neural processes underlying privacy to help people make better decisions about their privacy in cyberspace. Choice architecture is gaining popularity among academics and policy makers as a means to design the presentation of options to promote decisions that benefit both individuals and society. This project aims to uncover the neural correlates of privacy concerns to identify the cognitive and emotional processes that guide privacy behaviors in cyberspace. By using functional Magnetic Resonance Imaging (fMRI) to characterize the neural processes underlying privacy, this project aims to expand the toolbox of privacy researchers by introducing new theories and methods from neuroscience. By investigating the neural correlates of privacy concerns, this project seeks to inform future choice architecture interventions to encourage people to protect their privacy. Results will be disseminated broadly through conferences across several disciplines. Special focus will be given on disseminating findings beyond academics to non-research audiences, especially with respect to informing policy. Finally, this project aims to advance research and teaching at Temple University by providing a mentoring environment for students at all levels, including under-represented groups, to study privacy in cyberspace. \n\nPrivacy concerns are unintended consequences of the Internet that prevent a safe and trustworthy cyberspace. There is a tradeoff between privacy risks associated with disclosing private information and the benefits from such disclosure (termed ""privacy calculus""). Moreover, the ""privacy paradox"" states that although people self-report that privacy is critical to them, they often disclose more private information online, inconsistent with their privacy concerns. However, the neural processes that underlie people's decisions to protect their privacy are unclear. This project breaks new ground in privacy research by uncovering the neural correlates of privacy concerns with fMRI and using these insights to design choice architecture interventions to protect people's privacy in cyberspace. Specifically, it examines the neural processes underlying privacy concerns to help people better balance their privacy risks versus disclosure benefits in their privacy calculus to overcome the privacy paradox. This project will characterize the psychological processes that guide people's decisions to disclose their private information by identifying the neural correlates of privacy concerns with fMRI. This work will inform choice architecture interventions to encourage people to make decisions about their privacy in cyberspace to better match their privacy concerns.",1619108,3,private information
Cindy Bethel,,cbethel@cse.msstate.edu,Mississippi State University,"Cyber-Human Systems (CHS), Unallocated Program Costs",CHS: Medium: The Use of Robots as Intermediaries to Gather Sensitive Information from Children,"A recent report entitled ""Child Maltreatment 2011"" issued by the U.S. Department of Health & Human Services Administration for Children and Families estimated that more than 3.7 million children were the subjects of at least one report of maltreatment, and that over 681,000 children were found to be unique victims of child maltreatment in the United States. This multidisciplinary research will compare the effectiveness of robot vs. human interviewers for gathering sensitive information from children, using situations in which this would commonly occur: cases of child eyewitness memory and child reports of bullying. The PI argues that the use of a robot as an intermediary during these so-called forensic interviews could reduce or eliminate unintentional cues observed in human interviewers that result in inaccurate reports by children. To validate her hypothesis the PI and her team will develop a systems architecture, an interactive user interface and an interactive robotic toolkit for interviewers, and perform a series of six studies involving children ages 8-11. The interdisciplinary research team is comprised of experts in human-computer interaction, human-robot interaction, robotics, psychology, sociology, and social work, and the project will make contributions to each of these domains. The team further includes a member of the legal profession as a consultant, who will iteratively evaluate the potential for extending the research findings to real-world legal proceedings and investigations. Preliminary research conducted by the PI has attracted attention from the law enforcement and legal communities, so if successful this project has the potential to transform information gathering for investigative purposes. The PI and her colleagues have been actively involved in community outreach in local middle schools and Boys and Girls Clubs with respect to the use of robots for eliciting information related to bullying, and this outreach will be extended to elementary school children involved in the current research. \n\nThe research goals for this project will be accomplished through the development of an integrated robotic toolkit based on a novel Interactive Social Engagement Architecture (ISEA) and a unique interactive user interface. ISEA provides a framework for the autonomous generation of robot behaviors for self-preservation and to convey social intelligence. The toolkit will be designed to integrate behavior-based robotics, human behavior models, cognitive architectures, and user input to increase social engagement between a human and system (robot, avatar, etc.). The interactive user interface will provide interviewers with the ability to use the robot as an intermediary for gathering sensitive information. ISEA has three primary parallel paths for processing robot behaviors: (1) verbal behaviors based on expert user input from the interactive user interface; (2) autonomous self-preservation behaviors if the robot is threatened that consist of both verbal and non-verbal responses; and (3) non-verbal autonomous behaviors generated from sensor data coming from the environment, the current internal state of the robot, user input, and prior knowledge from the knowledge base/long-term memory. As part of the research, six human studies will be conducted that use typical situations in which gathering sensitive information from children might occur. Three of these experiments will examine whether child eyewitness memory is more accurate when a robot rather than a human presents misleading information during an interview, while the other three will examine whether children who have been victimized by bullying will be more likely to disclose that victimization to a robot as opposed to a human interviewer. Some of these experiments will examine the role of gender both for humanoid robots and adult interviewers, using established forensic interview protocols, while others will examine whether interviewers high in social intelligence elicit more accurate child eyewitness memory and reports of bullying than those low in social intelligence (where social intelligence is defined by the use of gestural and facial non-verbal behaviors).",1408672,20,sensitive information
Aaron Wagner,,wagner@ece.cornell.edu,Cornell University,"COMM & INFORMATION FOUNDATIONS, Secure &Trustworthy Cyberspace",CIF:Medium:Collaborative Research:Maximal Leakage and Active Receivers for Side- and Covert Channel Analysis,"A side-channel is any process that inevitably and unintentionally leaks information to an unauthorized user in a computer or communication system. A covert channel is any mechanism through which two parties simultaneously accessing the same computer intentionally communicate, despite this mechanism being disallowed for communication. Examples of both channels abound, and their presence makes our computation and communication infrastructure less secure: side channels allow attackers to glean sensitive information that they would not otherwise be able to access, and covert channels enable unauthorized disclosure between users. \n\nThis project enables engineers to attenuate side- and convert channels, thereby making our electronic infrastructure less vulnerable to external attack and internal leaks. In the process, the project enables a new collaboration between researchers in information theory, computer architecture, and communication networks. The project broadens participation of under-represented groups, especially women, through research opportunities and an improved mentoring program for a professional society.\n\nThis project approaches the study of side- and covert channels information theoretically. Historically, information theory has provided relatively little assistance toward mitigating side- and covert channels. This is partly because it is unclear how to quantify the amount of information leaked through a side channel and partly because many side- and covert channels involve a receiver that can actively control the channel. Such receivers are not well understood.\n\nTo address these two impediments, the project develops a recently-proposed metric for measuring leakage in side-channels that is operationally justified and easily computed. The project also develops results that enable one to predict how active receivers will act. The project culminates in the application of these results to three specific scenarios spanning communication networks and multi-core processors.",1704443,3,sensitive information
Chen Li,,chenli@ics.uci.edu,University of California-Irvine,INFO INTEGRATION & INFORMATICS,EAGER: Supporting GUI-Based Text Analytics on Social Media Data by Non-Technical Users,"A wealth of information is being created at an increasingly fast rate from social media sources such as Twitter. Many researchers need text analytics on social media data to obtain domain-specific insights, by doing various computational tasks such as keyword search, regular expression, natural language processing, and sentiment analysis. A main challenge is their lack of IT background, making it hard for them to conduct research efficiently. In addition, very often they need to use machine learning models to do deep analysis, and the data-preparation process for generating labeled instances for training models can be time consuming and labor intensive. \n\nThis project studies how to support text analytics on social media data by users who do not have a strong IT background. It develops an open-source system with the following goals: (1) modularizing common text computation as basic operators; (2) providing a graphic user interface (GUI) for users to form a workflow plan declaratively without writing code; (3) supporting text analytics as a Web-based service; and (4) supporting indexing whenever possible to improve performance. The system has been under development on Github for more than 15 months with more than 30 committers and 21 thousand lines of high-quality source code. An initial prototype is setup and used by Public Health researchers at UC Irvine to analyze Twitter data. The techniques are general-purpose, systems-oriented, and can benefit many other domains as well.",1745673,19,"social media data, twitter data"
Dominic Duggan,,dduggan@stevens.edu,Stevens Institute of Technology,Secure &Trustworthy Cyberspace,TWC: Small: Workflows and Relationships for End-to-End Data Security in Collaborative Applications,"Access control refers to mechanisms for protecting access to confidential information, such as sensitive medical data. Management of access control policies, in applications that involve several collaborating parties, poses several challenges. One of these is in ensuring that each party in such a collaboration only obtains the minimal set of access permissions that they require for the collaboration. In a domain such as healthcare, it may be critical that access be minimized in this way, rather than allowing all parties equal access to the sensitive information. In practice, it is often difficult to manage access control policies to achieve this goal of minimality. This research will investigate new approaches to describing and reasoning about access control policies, to ensure that information that is shared in collaborative applications is only accessible to those parties in the application that require it. This work will develop both the theoretical foundations of, and prototype tools for, ensuring end-to-end security of data that is shared between organizations, with sharing of medical data as a particular source for use-case scenarios.\n\nOn one dimension, the project will investigate the extension of workflow languages to describe protocols for sharing information in collaborate applications. On another dimension, relationship-based access control can describe access to information based on parties' roles in a collaboration. The project will investigate the use of spatial and temporal logics for relating these two, and user interfaces for presenting and reasoning about this information. Since ultimately software programs will perform the access and transfer of patient medical data, formal specifications will be extracted from workflow specifications, and used as a basis for checking software programs for their compliance with these access control policies. The broader impact of this research will be facilitated by the P.I.'s involvement with the NIH International Epidemiologic Databases on AIDS for Central Africa project (CA-IeDEA). The fruits of the research will be disseminated through interaction with open source communities involved in healthcare IT for low and middle income countries, as well as through interactions with lead healthcare IT decision-makers in the countries involved in CA-IeDEA.",1320798,3,sensitive information
Franziska Roesner,,franzi@cs.washington.edu,University of Washington,Secure &Trustworthy Cyberspace,CRII: SaTC: Improving Computer Security Technologies through Analyzing Security Needs and Practices of Journalists,"Advances in digital communication technologies, and their proliferation in recent decades, have had a remarkable impact on journalism. Security weaknesses in these technologies have put journalists and their sources increasingly at risk, hindering efforts at investigative reporting, transparency, and whistleblowing. Because of their willingness to be early adopters, and to openly communicate their issues, journalists provide an opportunity to identify security issues and requirements in new communication methods. This project will study the current computer security mental models and practices of journalists and their sources, and to ultimately design technology to better meet their needs. The resulting tools will be released as open source and deployed to directly benefit journalists and their sources, as well as other users of technology, ultimately helping to protect and improve freedoms of speech and of the press both nationally and internationally.\n\nThe vulnerabilities in how journalists (and users in general) communicate and store sensitive information result from several factors, including usability problems with existing tools, a mismatch between mental models and the real security properties of the tools they use, and technological gaps in the tools available. To evaluate and address these issues, this project will proceed in two phases: first, studying the current practices of journalists via in-depth interviews and large-scale surveys, and second, designing new technologies to better meet their computer security needs. The second phase will leverage findings from the first phase to inform technical research, including the development of more usable tools for journalists leveraging existing computer security techniques, as well as the development of new computer security technologies (e.g., protocols or systems) where necessary. For example, this project will consider technical challenges such as protecting the metadata of communications and bootstrapping secure communications at first contact.\n\nFor further information, see the project web site at: https://journosec.cs.washington.edu",1463968,3,sensitive information
Jeff Phillips,,jeffp@cs.utah.edu,University of Utah,ALGORITHMIC FOUNDATIONS,CAREER: Foundations for Geometric Analysis of Noisy Data,"An important role of computational geometry is to understand and formalize the structure of data. And as data is becoming a central currency of modern science, this role is growing in importance. However, much of classical computational geometry inherently assumes that all aspects of data are known and precise. This is rarely the case in practice. This project focuses on building the foundations for two extensions to classic geometric settings pertinent to noisy data. \n\n1. The PI will study locational uncertainty in point sets, where the location of each data point is described by a probability distribution. Given such an input, the goal is to formalize how to construct, approximate, and concisely represent the distribution of geometric queries on this uncertain data. \n\n2. The PI will study the geometric consequences of applying a statistical kernel (e.g. a Gaussian kernel) to a data set. He will investigate how this process can smooth data, remove degeneracies, and implicitly simplify and regularize algorithms. Moreover, he will explore the geometric structure of the resulting kernel density estimate, and how it relates to algorithms for the data and approximate representations of the data. \n\nThe PI will lead the development of a data-focused educational program around the themes of data analysis, algorithmics, and visualization. The PI is developing a model course for this program on data mining; it focuses on the geometric, statistical, and algorithmic properties of data. An extensive set of course notes is being compiled, accompanied with videotaped lectures freely available online. This class and program attract many interdisciplinary and diverse students and observers. This program is part of a larger effort to make relevant data analysis techniques from computational geometry available to a broader data-rich audience.",1350888,27,freely available online
Charalampos Papamanthou,,cpap@umd.edu,University of Maryland College Park,Secure &Trustworthy Cyberspace,CAREER: Towards Practical Systems for Trustworthy Cloud Computing,"An increasing number of individuals, enterprises, and governments \nmigrate their data and applications to the cloud. Computations delegated \nto the cloud may return erroneous results, outsourced files may be lost \nor discarded, and sensitive information may be arbitrarily accessed for \nadvertising purposes. This project enhances the ability of cloud companies \nto integrate advanced protection mechanisms into their products, benefiting \nthe online safety of cloud clients. The project also aims to improve security \neducation via new undergraduate and graduate curriculum and outreach to high school\nstudents.\n\nThis project addresses foundational problems on the security, availability, and \nprivacy of cloud computing by using an application-driven approach and demonstrates \nthe broad applicability of this theory in practice. The project develops low bandwidth and\nsub-linear computation protocols for securely recovering correct data blocks and computation \nresults. The researchers investigate practical and expressive privacy preserving algorithms\nwith reduced leakage profiles and low I/O complexity for advanced queries on encrypted \nstatic and dynamic data. They also develop new fundamental techniques for verifying \nfrequently recurrent classes of delegated computations\n",1652259,3,sensitive information
Zhi Wang,,zwang@cs.fsu.edu,Florida State University,Secure &Trustworthy Cyberspace,CAREER: Towards Trustworthy Operating Systems,"An operating system is the key software of a computer system that manages the hardware and software resources and provides essential services to computer programs. It plays a critical role in the security of the whole system. Unfortunately, modern operating systems are often bloated with millions of lines of source code, and serious vulnerabilities are routinely being discovered and exploited from them. Researchers have proposed various novel solutions based on the ""one-layer-below"" approach, in which a more privileged software component (i.e., a hypervisor) is introduced to monitor and/or regulate the operating system?s behavior. However, the large trusted computing base of modern hypervisors and the recent attacks against them put this approach into question. This project aims at developing a systematic approach to improve the trustworthiness of operating systems by enabling their self-defense, without resorting to other software layers that may be vulnerable themselves.\n\nThe goal of this project is being achieved in three key steps: first, the project develops a kernel-level security enclave that will provide a trusted, secure execution environment for other security systems and mechanisms. Second, based on the strong isolation provided by the enclave, the researchers design and implement several self-defense techniques for operating system kernels. Third, a cold-boot attack is a powerful physical attack that can extract sensitive information from the physical memory of a lost or stolen computer (including mobile devices). It has become a major security concern for corporations and governments. This project investigates a comprehensive defense against cold-boot attacks by encrypting the whole memory of a sensitive program on commodity hardware platforms. The results from this project could substantially improve our defensive capabilities against malicious and stealthy kernel-level malware and cold-boot attacks, and thus significantly improving the trustworthiness of computer systems. Research results are disseminated through publications, releasing of the tools developed, and integrating into the educational activities at both the graduate and undergraduate levels.",1453020,3,sensitive information
Shriram Krishnamurthi,,sk+17@cs.brown.edu,Brown University,Secure &Trustworthy Cyberspace,"EAGER: By the People, For the People: Community Ratings for App Privacy","Application stores use sophisticated user interfaces to help users understand the permissions sought by applications. Unfortunately, these interfaces are complex and may fail to address their goal of helping users give informed consent. As a result, users may inadvertently surrender private information or open themselves up to security attacks.\n\nThis project tackles the problem of improving the nature of information provided by these interfaces. It focuses both on new interface designs that will better represent this information, and on techniques to bootstrap the provision of that information. On the user interface side, it designs new interfaces that help untrained users recognize the security and privacy consequences of the sought permissions. To populate this information, it posits the use of crowdsourcing to obtain information at scale about the suitability of permissions. The goal is to eventually populate an application store with this information to encourage adoption and additional feedback from users. The project will study how effectively and accurately crowdsourcing can be used to gather this information.",1449236,3,private information
Sandra Carpenter,,carpens@uah.edu,University of Alabama in Huntsville,Secure &Trustworthy Cyberspace,SBES: Small: Developing Countermeasures to Mitigate Psychology Cyber-Attacks on Personal Identity Information,"Approximately six million Americans are targets of identity theft each year. Many of the attacks on identity privacy use psychological influence strategies (""psychological attacks?) to induce individuals to provide their private information. Although people are appropriately concerned about their privacy, they often unnecessarily disclose information that could be used to their disadvantage. Our studies have shown that people?s privacy exposure behaviors may be severely affected by psychological attacks. Unfortunately, research from a psychological perspective to mitigate the attacks is scarce. This research identifies critical aspects of warnings for a sub-set of psychological cyber-attacks on privacy and provides guidelines for developing effective mitigations against other types of psychological cyber-attacks. We create computer-mediated countermeasures. We also ascertain the extent to which the warnings capture attention, are understood, are memorable, increase perceptions of risk, decrease trust, and lead to compliance under conditions of psychological attacks. \n\nIntellectual Merit. This research is a first investigation of whether theoretical models developed to reduce risky behaviors (e.g., health-related behaviors) can be extended to the domain of computer privacy. The research determines whether warnings can have significant impact on people?s decisions about disclosure of their private information. The effectiveness of our mitigation approach is tested on hand-held devices and web sites with the goal of increasing compliance with the warnings. \n\nBroader Impacts. This research provides mitigation strategies for private information exposure and provides guidelines for software developers to use when designing privacy preserving software. Potentially, the results can be generalized to mitigate other current and future psychological privacy attacks. Research findings are disseminated in both social psychology and computer science. In addition, a website is developed to share the research results, the data sets, and the lessons learned, in order to raise the awareness of the importance of protecting identity information and mitigating psychological cyber-attacks.",1220026,30,private information
Radu Sion,,sion@privatemachines.com,Private Machines Inc.,Secure &Trustworthy Cyberspace,"TTP: Medium: A Campus Pilot For A Privacy-Enabled Cloud Storage, Search, and Collaboration Portal for Education","As higher education institutions consider moving services to the cloud to save costs and improve collaboration, significant challenges to successful large-scale adoption still exist. Institutions are unwilling to risk cloud deployment because provable technological defenses have thus far been lacking. Control over sensitive data is relinquished without the institution's knowledge, liability is shifted and data breach risks are significantly increased. Further, regulatory-sensitive data has become an increasingly attractive target. Recent history shows one major breach every few weeks, with an alarming increase in frequency. This project directly addresses this significant challenge by transitioning to practice -- through an educational campus pilot -- searchable cloud storage and collaboration tools with full in-cloud privacy, at-runtime security and no shift in liability for educators and students. The pilot provides secure cloud storage to 24,500 students and 2,500 educators. It constitutes a testbed for deploying secure protocols in a live environment via the participation of researchers and students in its operation. Extensive data dissemination for academic research purposes is an integral part of the pilot. Collected data points serve as significant knowledge repositories not only in the security community but also in the storage and cloud research communities. The project will result in the creation of a significant number of trust and security-related information technology jobs which must remain stateside to maintain the economic and political security of the United States. The project trains students and contributes to the creation of a skilled cyber-security domestic workforce available to fulfill our nation's needs. The project provides new course material and involves underrepresented groups, including at least one female PhD student.\n\nThe technical underpinnings of the work rely on new mechanisms that ensure data is strongly encrypted before leaving trusted client premises while allowing users to still securely collaborate, query, share, synchronize, search, backup etc. Contrary to conventional wisdom, the project constructs a secure design that is also truly practical, but would not have been feasible with a server-centric approach in which search operations are performed server-side on encrypted data and are computationally expensive and necessarily limited in expressiveness. Instead, the pilot is based on an efficient design in which operations are distributed to client-side logic in a scalable, efficient manner, operating orders of magnitude faster than the equivalent server code on encrypted data. To further optimize processing, the system enables clients to leverage each other's work (such as indexing) in a distributed secure manner, through cloud-mediated conduits and mechanisms. Overall, the system ensures the cloud cannot ever access user data or see user search queries. Ultimately, clients receive all cloud benefits while ensuring full regulatory compliance. Even if the cloud provider is breached, data is fully protected.",1562376,3,user data
Hoda Mehrpouyan,,hodamehrpouyan@boisestate.edu,Boise State University,CRII CISE Research Initiation,CRII: SaTC: A System for Privacy Management in Ubiquitous Environments,"As mobile and network technologies proliferate, so does society's awareness of the vulnerability of private data within cyberspace. Protecting private information becomes specially important, since researchers estimate that 87% of Americans can be identified by name and address, if their zip code, gender, and birthday are known to intruders. The goal of this proposal will be to develop a new set of verification tools, algorithms, and interfaces that enable secure, effective and unobtrusive management of users' private information. The proposed approach leverages formal verification techniques to ensure that the intended privacy properties and goals are met. Because of the modular and robust design of the proposed platform, it can be re-aligned and tuned to accommodate the needs of specific use cases and applications (e.g., health-care, connected automotive systems, and smart cities).\n \nThe proposed system will: (1) use model-checking to ensure that updated rules and boundaries correctly enforce users' privacy intents, given that users' privacy boundaries and rules constantly evolve (e.g., due to aging, social pressure, and changes in health and personal relationships), (2) automatically translate control policies to privacy-preserving protocol executions, which provably enforce privacy intents. To achieve this, new approaches for activating privacy-preserving functionalities based on the knowledge of privacy rules and boundaries, as well as novel cryptographic tools will be used.\n\nFor further information see the project web site at: http://hodamehrpouyan.com/privacy.html .",1657774,25,private information
Jean Yang,,jyang2@cs.cmu.edu,Carnegie-Mellon University,"CRII CISE Research Initiation, SPECIAL PROJECTS - CISE",CRII: SaTC: Repairing Code from Inferred Specifications of Information Flow Security,"As more software computes using sensitive user data, it increasingly important to ensure that data flows only where it is permitted. Protecting sensitive data often involves reasoning about how sensitive values and policies are interacting with functionality across the program. The need to reason across the code base makes it difficult not only for programmers to implement computations using sensitive data, but also to make existing code adhere to new policies. This project investigates an approach for (1) inferring information flow policies from potentially buggy code and (2) performing program repair to ensure programs adhere to the specified policies. Not only does this approach help identify potential misuses of sensitive information, but it also helps prevent leaks in code from well-intentioned programmers, and provides a mechanism for modifying arbitrary code to satisfy a given policy.\n\nEnabling this approach is a policy-agnostic semantic model, developed to disentangle information flow concerns from other functionality. Using policy-agnostic programming, the programmer may implement information flow policies by specifying them alongside sensitive data values, rather than implementing them as conditional access checks across the program. Prior work on Lifty supports policy-agnostic programming using type-driven repair, based on program synthesis for liquid types, which are value-dependent refinement types. Previously, programs could only benefit from policy-agnostic programming if the policies are known, but this is not always the case with existing code. We propose an approach for inferring liquid types from potentially buggy code. The inferred types now make it possible to perform sound repair. Representing the inferred policies as liquid types also facilitates modification of the policies. The proposal describes both a strategy for inferring a set of possible policies, and a solution for choosing which policies to use for repair.",1657530,25,"sensitive information, user data"
Hoeteck Wee,,hoeteck@alum.mit.edu,George Washington University,TRUSTWORTHY COMPUTING,CAREER: Secure Public-Key Cryptography,"As the Internet emerges as the platform for computation, we have become increasingly reliant on cryptography to provide privacy and security in many of our day-to-day activities. We rely on cryptographic protocols to protect our credit card numbers from hackers in electronic transactions and our personal information from unauthorized access on online social networks. However, the design of many cryptosystems do not adequately account for new computational and cryptographic attacks made possible by advances in quantum computing and complex protocol interactions on the Internet. The focus of this project lies in the design and analysis of new cryptographic protocols that address these new attacks.\n\nThe research is centered around two goals: (1) to develop cryptosystems from large classes of intractability assumptions as viable alternatives to the widely-used factorization-based cryptosystems; (2) to obtain new techniques and efficient protocols secure against coordinated attacks amidst concurrent protocol executions.\n\nThis research is expected to develop ideas and techniques which hold the potential to bridge the gap between theory and practice in cryptography, and to fundamentally change the way we communicate, compute and collaborate. To ensure broader impact of this research, this project also encompasses a program of educational and outreach activities, including curriculum development (with an emphasis on new pedagogical approaches) as well as collaboration and regular exchanges with research institutions in the New York area and abroad.",1237429,30,personal information
Wu He,,whe@odu.edu,Old Dominion University Research Foundation,Secure &Trustworthy Cyberspace,SBE: Small: Collaborative: Improving Security Behavior of Employees in Cyberspace through Evidence-based Malware Reports and E-Learning Materials,"As the use of Web applications has increased, malicious content and cyber attacks are rapidly increasing in both their frequency and their sophistication. For unwary users and their organizations, social media sites such as Tumblr, Facebook, MySpace, Twitter, and LinkedIn pose a variety of serious security risks and threats. Recent studies show that social media sites are more in use for delivering malware than were previously popular methods of email delivery. Because of this, many organizations are looking for ways to implement effective security policies. Getting employees to comply with a security policy, however, can be a significant challenge. \n\nStudies show that the weakest link in a security chain is users. Thus, organizations need to find innovative methods to increase their employees' security awareness and their capabilities to engage in online security behavior. This project aims to study the factors that affect cyber security behavior, and use customized and evidence-based malware reports and e-learning materials to help employees more deeply understand their security risks and to improve their security behavior. The first stage of this project will be a large-scale survey study to identify key factors that affect organizational employees' security behavior. These data will be used to develop a psychological decision-making model for cyber security compliance. The second stage of this project will be a controlled experimental study to compare the effectiveness of different evidence-based interventions designed to increase employees' cyber security compliance. \n\nThis project will contribute to the understanding of cyber security behaviors of employees and the development of more effective cyber security policies. The developed model will explain and predict how various factors affect employees' cyber security behaviors. Project results will contribute to the psychological, behavioral, and educational theories relating to the basic processes by which people assess vulnerabilities and threats, choose adaptive or maladaptive coping strategies, and adopt new coping strategies. Through educational workshops, a project portal, journal publications, and conference presentations, the results of the project will reach a broad audience that includes corporate IT directors, managers, employees, researchers and practitioners in various industries such as real estate, financial services, logistics and supply chain, insurance and education.",1318470,3,linkedin
Neil Heffernan,,nth@wpi.edu,Worcester Polytechnic Institute,"PROGRAM EVALUATION, Software Institutes",SI2-SSE: Adding Research Accounts to the ASSISTments' Platform: Helping Researchers Do Randomized Controlled Studies with Thousands of Students,"ASSISTments is a free, university-based platform created to perform controlled experiments with the potential to help increase the quality, speed, and reliability of results related to K12 education. ASSISTments' mission is ""to improve education through scientific research while not compromising student learning time."" Each day, teachers assign problems to thousands of students (currently 50,000 students) in ASSISTments. These problem sets often contain controlled experiments. ASSISTments has used this platform to do controlled experiments that have resulted in 17 peer-reviewed publications. For a typical education researcher, developing relationships with schools is costly. ASSISTments has built relationships with teachers and researchers to run experiments to improve education without disrupting classrooms. This project will add researcher accounts to ASSISTments to better facilitate the research process. Researchers will create their own experiments, get approval from WPI for release to teachers, and get anonymized data. ASSISTments will reach out to its community of teachers who trust ASSISTments, to invite them to run the study in their classrooms. The intellectual merit of this work will be the contribution of the studies that this system would support. ASSISTments' ten-year goal is to have a community of hundreds of scientists that use this tool to do their studies. \n\nPsychologists tend to study human learning in lab studies; researchers in education and learning sciences point out that it's not clear if those studies generalize to K12. These communities need to work together, but are lacking common ground. Thousands of researchers in psychology, mathematics education, and learning sciences care about using science to better understand human learning. Some researchers study how to help students with motivational messages, spaced retesting, or comparing feedback. Many researchers have used thousands of psychology undergraduates as subjects, but want their ideas tested and validated in authentic K12 settings. Everyone understands physicists need a shared scientific instrument to do their work, but so do educational psychologists. The broader impact of this work will be as a demonstration, showing how a tool could be built that helps many researchers conduct controlled experiments. This will include showing how the project can increase the efficiency of the scientists? work.",1440753,8,anonymized data
Bing Wang,,bing@uconn.edu,University of Connecticut,Smart and Connected Health,SCH: EXP: LifeRhythm: A Framework for Automatic and Pervasive Depression Screening Using Smartphones,"Because of its high prevalence and significant health and economic impacts, depression is a profound public health problem. Currently, screening for depression is based on physician-administered interview tools or patient self-report. While physician-administered tools are more authoritative, availability is constrained both by cost and lack of access to trained mental health professionals. Patient self-reporting, on the other hand, suffers from recall bias and inconsistent patient participation. In particular, neither approach satisfactorily addresses the chronic and recurring nature of depression that requires frequent assessment for monitoring onset and progress. To address depression as a public health problem, there is urgent need for an objective, accurate, easily accessible and scalable depression screening tool. The ubiquitous adoption of smartphones around the world creates new opportunities in automatic and pervasive screening of depression across large populations. The education plan of this proposal includes developing and enhancing various undergraduate and graduate-level courses, as well as disseminating the results to medical students through clinical supervision and increasing the participation from under-represented groups in research and outreach activities. \n\nThe goal of this project is to develop LifeRhythm, an automated system for automatic and pervasive depression screening using smartphone data. LifeRhythm continuously monitors the behavioral rhythms of individuals through their smartphones, extracts normalized features from the raw data, and applies multiple machine-learning models for real-time diagnosis. The project applies LifeRhythm to two settings that have complementary strengths. The first setting uses ""high-resolution"" sensing data collected from smartphones, which provides extremely rich and descriptive behavioral data, allowing the best leverage for machine learning models. The second setting uses ""low-resolution"" wireless association meta-data collected passively from large-scale WiFi networks, which eliminates the need of data collection on smartphones and can be especially valuable for a large organization, where it could automatically provide depression screening of tens of thousands of people simultaneously at very little cost. Development of LifeRhythm will be coupled with several tightly related machine-learning research efforts, including novel techniques for collaborative prediction, integrative learning, modeling of temporal dynamics, and model refinement using multiplicative-weights-based techniques. Though this proposal is primarily focused on development of screening tools, future work could naturally develop an associated intervention program. In addition, this research may lead to methodologies that are applicable to other mood disorders such as bipolar illness. The broader impacts will include dissemination of research results (and the annotated dataset) to the technical communities. The project web site (http://nlab.engr.uconn.edu/sch.html) provides access to additional information on research and results.",1407205,7,smartphone data
Paul Pavlou,,pavlou@temple.edu,Temple University,Secure &Trustworthy Cyberspace,"WORKSHOP: PRIVACY IN AN ERA OF BIG DATA-Temple University; May 20-21, 2015","Big Data has sparked great interest in practice. Public and private sector organizations of all kinds are making huge investments in Big Data in the well-justified belief that innovations in data analytics can bring enormous benefits in such areas as public health and safety, economic competitiveness and consumer welfare. However, a key aspect of the debate on Big Data is the potential for privacy breach by corporations, malicious individuals, and governments. The workshop's overall goal is to develop a forward-looking research agenda on how to overcome the tradeoff between Big Data and privacy to effectively pursue the potential of big data without sacrificing people's individual privacy rights. \n\nThe workshop examines how privacy may be impacted in the Big Data era with emphasis on implications for individuals, firms, as well as government and non-profit organizations. By inviting prominent participants from several diverse perspectives, including law, social, behavioral, economic and computer sciences, and business, the goal is to foster a cross-disciplinary approach into understanding and mitigating the tradeoff between big data and privacy. The workshop aims to enable big data to transform organizations, markets, government, the private and public sector, and society in general, while respecting people's rights to their private information. The workshop seeks to advance a multi-disciplinary scientific inquiry on big data and privacy and aims to serve as a bridge for inter-disciplinary collaboration among academia, industry, and government with emphasis on academic rigor and real-world relevance.",1449619,3,private information
Xiaojin Zhu,,jerryzhu@cs.wisc.edu,University of Wisconsin-Madison,INFO INTEGRATION & INFORMATICS,III: Small: Advancing the Scientific Understanding of Bullying Through the Lens of Social Media,"Bullying has been recognized as a serious national health issue. Traditional approaches to the scientific study of bullying are hindered by data acquisition. For example, the standard approach has been to conduct personal surveys in schools. Due to its relatively small sample size and low temporal resolution, neither the true frequency of bullying over the population nor the evolution of bullying roles can be satisfactorily studied. The traditional approaches are also very labor intensive. \n\nSocial media has developed to the point where it contains enough signal about bullying. This project develops novel machine learning models that automatically monitor and analyze publicly available social media data to understand bullying. These machine learning models reconstruct hidden bullying episodes from a sequence of social media posts. They automatically determine who participated in which bullying episode as what role. In addition, this project conducts human studies on bullying in school and in social media in parallel, by collecting self-report surveys by school-aged children and their social media posts simultaneously. Such studies correlate the traditional psychological approach and social media data on bullying. Taken together, the project will provide significant new scientific data toward understanding, intervention, and helping policy-making regarding bullying.",1216758,30,social media data
Sanjay Shakkottai,,shakkott@austin.utexas.edu,University of Texas at Austin,RES IN NETWORKING TECH & SYS,"NeTS: Small: Inverse Problems from Cascades: Structure, Causation and Opinions","Cascades, also known as epidemic processes, are network phenomena where the activation of one node increases the likelihood of activation of its neighbors; this results in an event starting at one node eventually affecting a much larger part of the network via successive spread. Cascade processes serve as flexible yet coherent models for several phenomena: spread of viruses and malware in mobile phones, diseases in human society, opinions and actions in online social networks. This project focuses on using cascades as an inference and learning tool; the aim is to ascertain important network structure and properties, from partial and very noisy observations of cascade progressions on it. This runs counter to the vast majority of work on cascades, which is focused on the ""forward"" problem (of predicting how a cascade will spread given network properties). \n\nThe project will develop an analytical and algorithmic framework that achieves the following three aims: \n\n1. Inferring Graph Structure: What graph best explains observed cascades? From noisy samples of multiple cascade progressions, the project will formulate graph-learning as non-parametric statistical inference, and propose an algorithmic approach that leverages recent break-throughs in regularized convex optimization and iterative (forward-backward) methods. Conversely, the project will develop lower-bounds on sample complexity using statistical minimax theory. Applications abound - for instance, learning the true Twitter interest graph from observation of cascades over the follower network. \n 2. Detecting and Identifying the Causative Network: Is it possible to detect if a cascade is progressing; if so which network is it evolving on? Interactions occur over multiple possible networks in many different domains (mobile forensics, epidemiology, online social networks), pointing to the broad applicability of this thrust. \n3. Learning Node Opinions: Users often need to be active participate for cascades to progress (e.g., retweet on social media). By correlating user decisions with user actions, is it possible to learn individual user opinions? \n\nFor validation the project will test the algorithms both on both synthetic data and real data. Public data sets to be leveraged include Texas hospital records along with online blog and search records (Spinn3r, Twitter, Google Flu Trends, infochimps).\n\nCascade processes are widely prevalent in modern networks. The project's algorithms and understanding of inverse problems will further the state of the art in diverse fields including biological and human disease networks, societal networks of self-interested agents, and mobile and malware networks. In addition, this project will continue and broaden the PI's emphasis on recruiting and mentoring students from under-represented communities. The industrial affiliates program of the Wireless Networking and Communications Group at The University of Texas at Austin will facilitate technology transfer to industry.",1320175,28,public data
Michael Zentner,,mzentner@purdue.edu,Purdue University,DATANET,CIF21 DIBBs: Conceptualization of the Social and Innovation Opportunities of Data Analysis,"CIF21 DIBBs: Conceptualization of the Social and Innovation Opportunities of Data Analysis\nThis proposal presents an opportunity to work on the problem that scientists have access to continuously growing data repositories across economic and geographic boundaries. However, both individual innovation and the formation of rich collaborations still rely on traditional research and social mechanisms. While virtual organizations help with access to data environments among groups, members must still proactively seek to collaborate. The difficulty of sharing analysis tools, and the lack of understanding of how such tools are used, create friction that impedes extracting the greatest benefit from data and its usage. If the scientific community can formalize collection of User Data Interaction (UDI) data and develop actionable characteristic behavior patterns from it, the friction can be relieved and scientists can be connected in behaviorally meaningful ways that are not currently imagined. In this proposal is discussed the opportunity for working on the problem. Data is the lifeblood of science. Recent funding opportunities have fueled support for uploading, archiving, and managing data in more formal and standard ways. However, the actual use of data through data exploration tools is still a highly variable process. Interactive data exploration tools provide the opportunity to record researcher interactions during the exploration process. The pattern of interactions such users undertake while searching, exploring, and using data is a largely unexploited opportunity for new connections and new learning that could help researchers identify useful exploration modes or gaps, and even new collaborative partners that could increase interactions and innovation. Such data about how users explore data are here termed, ?User-Data Interaction (UDI) Data.? Creating cyberinfrastructure building blocks to support a standard for collecting UDI Data, community development of data exploration tools, and the exploration of UDI data could fundamentally change the practice of science and engineering. Having such data and analysis tools hosted within a shared cyberinfrastructure could also allow for unprecedented study of their use and effectiveness.\nThe goal of this conceptualization research will be to define an implementation project for the DIBBs program. To achieve this goal, the approach will be to understand the kinds of data analysis tools that various user communities currently use, those that they would like to create and share, and to explore the ensuing UDI data that could be collected and leveraged. A data source will be characterized as any service into which a user can specify a query and receive a semi-structured result. By way of example, this may include an online database with which users interact through forms, a graphical interface to a data cube, or even an online simulation tool. The proposing team has access to three such toolkits in use by thousands of users today (Rappture Toolkit, iKNEER, and DataView) to study as sources of analysis tools and UDI data. Specifically, access to the developers of these systems will provide information about how such systems could generate UDI data and what its important features may be. Having built an understanding from active communities and small group discussions, the final step of information gathering will be two larger discussions held in conjunction with two events: HUBbub 2013 and an NSF S2I2 conceptualization project meeting. The Intellectual Merit: This research will identify the social and technological roadblocks to sharing data analysis tools, and the transformational potential of UDI data. The intellectual merit of this activity will be an evidence-based blueprint for a cyberinfrastructure environment that will automatically gather UDI data, develop patterns from those data, and facilitate amplified discovery and collaboration based on those patterns in a way that acceptably balances efficacy and privacy. Collaborations will increase and will be of greater substance. Broader Impacts: This work will pave the way for new scientific connections among researchers, educators, and students that will accelerate research and innovation. The difficulties that underrepresented groups inherently face in traditional methods of establishing scientific collaborations will be bridged by an implementation of the proposed work, allowing everyone to connect to tools and other researchers?not solely by established reputation, but based on their interactions with data. Because the work is not specific to one virtual organization or data tool, it will have a broad reach across diverse scientific communities that use data and data analysis tools.",1255781,9,user data
Thomas Carsey,,carsey@unc.edu,University of North Carolina at Chapel Hill,DATANET,CIF21 DIBBs: Designing the Roadmap for Social Network Data Management,"CIF21 DIBBs: Designing the Roadmap for Social Network Data Management \nScholarly interest in network analysis has increased dramatically in the social sciences and beyond. The explosion of social media tools such as Facebook, Flickr, and Twitter, along with new developments in machine learning and data mining have produced new types of behavioral data for scholars to analyze (Mislove et al., 2007). Significant advances in mathematics, statistics, and computer science have also produced unprecedented opportunities to analyze ?Big? social network data. Social network analysis sits at the cutting edge of social science and also links the social, natural, and computational sciences. Understanding the multi-faceted nature of social networks and their effects on human behavior is one of the grand challenges faced as this project seeks to maximize our investments in scientific research (NSF-ACCI, 2011). However, the community has not seen comparable advances in the management, archiving, and sharing of social network data. This presents a fundamental obstacle to advancing network science across the social, natural, and computational sciences. This proposal seeks to begin to remedy this problem. The data management needs social network scholars are complex. Network data often come from unstructured environments that require researchers to define and describe a set of units or actors (called nodes) and the connections (called edges) between them. Networks might be static or dynamic, include one type of node or multiple node types, and include edges that are uni-directional or bi-directional and weighted or unweighted. In addition, as relationships spread within the network and/or a network grows, the associated data management, data storage, and analytical memory requirements can grow exponentially. This proposal brings together the social network analysis, information science, computer science, and data archive communities to develop a data infrastructure to support advanced analysis and research on social networks as well as to facilitate data sharing and archiving within this community. The group will address key questions concerning data storage architecture and lifecycle requirements, develop design specifications for creating a sustainable data infrastructure that will be discoverable, searchable, accessible, and usable to the entire research and education community, and initialize a prototype solution based on that plan. Intellectual Merit: The proposed project will bring together the social network analysis community to work with information technology professionals to design a robust data management infrastructure to promote the sharing and interoperability of social network data. Effective data management for social network data amplifies the impact of research by revealing data quality issues early in the data collection process, ensuring that required data is retained and usable throughout the life of a research project. It also facilitates data sharing and reuse. A key to responsible data stewardship is the application and auditing of quality data management policies ? something included in this proposal. Providing a robust infrastructure to store, analyze, curate, share, and manage important social network data will increase researchers? production and provide an unprecedented view of the social world only visible through social network data. Broader Impacts: The project will facilitate data sharing and increase social network data availability while assuring researchers that data management policies are followed. It will help formalize a community of network data experts that will begin developing best practices for the community. Availability of data particularly benefits early-stage researchers, and researchers at diverse institutions. Widespread availability of data facilitates citizen science and the integration of science and teaching at all levels of education. Managed data sharing is critical to the multi disciplinary research to answer the critical challenges facing society today. It would be difficult to overstate the importance of social network analysis to better understand human networks and social behavior.",1255826,9,"flickr, social network data"
Calton Pu,,calton@cc.gatech.edu,Georgia Tech Research Corporation,"INFORMATION TECHNOLOGY RESEARC, SPECIAL PROJECTS - CISE, COMPUTER SYSTEMS, CYBER-PHYSICAL SYSTEMS (CPS), Science Across Virtual Instits",RCN: SAVI: Adaptive Management and Use of Resilient Infrastructures in Smart Cities: Support for Global Collaborative Research on Real-Time Analytics of Heterogeneous Big Data,"Cities provide ready and efficient access to facilities and amenities through shared civil infrastructures such as transportation and healthcare. Making such critical infrastructures resilient to sudden changes, e.g., caused by large-scale disasters, requires careful management of limited and varying resources. The rapidly growing big data from both physical sensors and social media in real-time suggest an unprecedented opportunity for information technology to enable increasing efficiency and effectiveness of adaptive resource management techniques in response to sharp changes in supply and/or demand on critical infrastructures. Within the general areas of resilient infrastructures and big data, this project will focus on the integration of heterogeneous Big Data and real-time analytics that will improve the adaptive management of resources when critical infrastructures are under stress. The integration of heterogeneous data sources is essential because many kinds of physical sensors and social media provide useful information on various critical infrastructures, particularly when they are under stress. \n\nThis Research Coordination Network (RCN) will promote meetings and activities that stimulate and enable new research on integration of heterogeneous physical sensor data and social media for real-time big data analytics in support of resilient critical infrastructures such as transportation and healthcare in smart cities. As first example, the RCN will support participation from young faculty attending the Early Career Investigators' Workshop on Cyber-Physical Systems in Smart Cities (ECI-CPS) at CPSweek (April of each year) and young faculty attending the Workshop on Big Data Analytics for Cyber-physical Systems (BDACPS). As a second example, the RCN will support contributions to a Special Track on Big Data Analytics for Resilient Infrastructures at the IEEE Big Data Congress. As a third example, the RCN will support participation in International meetings organized by other countries, e.g., Japan's Big Data program by Japan Science and Technology Agency (JST). The project will also maintain a repository of research resources. Concretely, the RCN will actively collect and make readily available public data sets (e.g., physical and social sensor data) and software tools (e.g., to support real-time big data analytics). The technologies and tools that arise from RCN-enabled research will be applied to socially and economically impactful areas such as reducing congestion and personalized healthcare in smart cities.",1550379,29,public data
Mohit Tiwari,,tiwari@austin.utexas.edu,University of Texas at Austin,"SPECIAL PROJECTS - CISE, Secure &Trustworthy Cyberspace",TWC: Medium: Collaborative: DIORE: Digital Insertion and Observation Resistant Execution,"Cloud computing allows users to delegate data and computation to cloud providers, at the cost of giving up physical control of their computing infrastructure. An attacker with physical access to the computing platform can perform various physical attacks, referred to as digital insertion and observation attacks, which include probing memory buses, tampering with memory, and cold-boot style attacks. While memory encryption can prevent direct leakage of data under digital observation, memory access patterns to even encrypted data may leak sensitive information. This work will allow organizations to securely outsource their computing infrastructure to an untrusted cloud provider, while preserving a similar level of security as if hosting the infrastructure in house\n\nThis project will develop DIORE (Digital Insertion and Observation Resistant Execution) which is a combined hardware software platform immune to digital insertion and observation attacks. DIORE provides memory-trace oblivious execution, relying on efficient hardware implementations of Oblivious RAM, and novel compiler techniques for partitioning programs such that Oblivious RAM accesses are minimized. This ensures that an adversary with access to a program execution's memory trace learns nothing about the code or data other than what is revealed intentionally. DIORE opens up possibilities for new cloud applications involving sensitive information such as genomic, medical, or financial data -- domains that are considered too privacy sensitive for today's cloud.",1314709,3,sensitive information
Alexandra Boldyreva,,aboldyre@cc.gatech.edu,Georgia Tech Research Corporation,Secure &Trustworthy Cyberspace,EAGER: Collaborative: Quantifying Information Leakage in Searchable Encryption,"Cloud storage is currently experiencing explosive growth as more and more businesses and organizations store large amounts of data on cloud servers. Encrypting such data provides security against untrusted servers or malicious intrusions. However, standard encryption has the drawback of compromising functionality and efficiency and it is so strong that its ciphertexts are not searchable. For this reason, searchable encryption (SE) has become an important research area, aimed at providing weaker forms of encryption that balance security, efficiency, and functionality goals. SE schemes have already been deployed in practical systems like CryptDB and there is strong demand for more such solutions. However, recently published high-profile attacks call into question whether such systems can in fact be used safely. More generally speaking, it has proven very difficult to understand the security implications of using SE in practice. This project aims to bring clarity to the current rather challenging situation seeking to analyze and quantify the amount of leakage of sensitive information that can occur under various SE schemes in practice, thereby offering guidance to both cryptographers and practitioners about when and how such schemes can be used safely.\n\nThe approach of this project is interdisciplinary, coupling the provable security analysis of cryptographic schemes with quantitative information flow (QIF) theory, building on the expertise of the principal investigators in their respective areas. A technical challenge is that the adversaries considered in provable security are computationally bounded, while those considered in QIF are information theoretic. Yet, the security of an SE scheme is often formulated as computational indistinguishability from an ""ideal object"" which can be modeled as an information-theoretic channel, and whose leakage can then be analyzed using QIF techniques. In particular, notions of channel capacity allow worst-case bounds on information leakage to be established, and since refinement is a partial order it is possible to show that one SE scheme never leaks more than another, regardless of the adversary's prior knowledge or goals. The project seeks to carry out such analyses on a number of pertinent SE schemes, including deterministic encryption and order-preserving encryption. The goal is to establish new cryptographic foundations and metrics for measuring and comparing the security of different searchable encryption schemes and their usage.",1749069,3,sensitive information
Wensheng Zhang,,wzhang@iastate.edu,Iowa State University,Secure &Trustworthy Cyberspace,TWC: Small: Building Efficient and Accountable Multi-User ORAM Systems for Protecting Data Access Patterns,"Cloud-based data sharing is increasing in popularity, but there is also an increasing privacy consciousness. Continued growth in cloud computing will require reconciling these issues. While data encryption can provide some protection, it has been shown that even access patterns to encrypted data can reveal private information (e.g., knowing encrypted data was uploaded by the Centers for Disease Control and later accessed by a particular user suggests that the user is concerned about some disease, even if the specifics are not known.) There is a lack of study and understanding of how to protect data access pattern in practical scenarios, where there are multiple users and cloud servers and the users and server may not trust each other. The first objective of the research project is to deepen the understanding of the challenges in protecting data access patterns as well as the collateral security risks.\n\nSpecifically, this project will develop formal models for protecting data access patterns and initial approaches to that protection, laying a solid foundation for developing novel solutions to protect data access patterns and strictly evaluating the security properties of the solutions. The project also plans to deliver security-provable, oblivious, efficient and accountable multi-user access protocols to outsourced data, for the sake of preserving each individual user's access pattern under a selected set of representative adversary models.",1422402,3,private information
Paolo Gasti,,pgasti@nyit.edu,New York Institute of Technology,"SPECIAL PROJECTS - CISE, Secure &Trustworthy Cyberspace",TWC: Small: Collaborative: RUI: Towards Energy-Efficient Privacy-Preserving Active Authentication of Smartphone Users,"Common smartphone authentication mechanisms such as PINs, graphical passwords, and fingerprint scans offer limited security. They are relatively easy to guess or spoof, and are ineffective when the smartphone is captured after the user has logged in. Multi-modal active authentication addresses these challenges by frequently and unobtrusively authenticating the user via behavioral biometric signals, such as touchscreen interaction, hand movements, gait, voice, and phone location. However, these techniques raise significant privacy and security concerns because the behavioral signals used for authentication represents personal identifiable data, and often expose private information such as user activity, health, and location. Because smartphones can be easily lost or stolen, it is paramount to protect all sensitive behavioral information collected and processed on these devices. One approach for securing behavioral data is to perform off-device authentication via privacy-preserving protocols. However, our experiments show that the energy required to execute these protocols, implemented using state-of-the-art techniques, is unsustainably high, and leads to very quick depletion of the smartphone's battery. \n\nThis research advances the state of the art of privacy-preserving active authentication by devising new techniques that significantly reduce the energy cost of cryptographic authentication protocols on smartphones. Further, this research takes into account signals that indicate that the user has lost possession of the smartphone, in order to trigger user authentication only when necessary. The focus of this project is in sharp contrast with existing techniques and protocols, which have been largely agnostic to energy consumption patterns and to the user1s possession of the smartphone post-authentication. The outcome of this project is a suite of new cryptographic techniques and possession-aware protocols that enable secure energy-efficient active authentication of smartphone users. These cryptographic techniques advance the state of the art of privacy-preserving active authentication by re-shaping individual protocol components to take into account complex energy tradeoffs and network heterogeneity, integral to modern smartphones. Finally, this project will focus on novel techniques to securely offload computation related to active authentication from the smartphone to a (possibly untrusted) cloud, further reducing the energy footprint of authentication. The proposed research will thus make privacy-preserving active authentication practical on smartphones, from both an energy and performance perspective.",1619023,3,private information
Rohit Chadha,,chadhar@missouri.edu,University of Missouri-Columbia,Secure &Trustworthy Cyberspace,CAREER: Automated Analysis of Security Hyperproperties,"Computer programs and cryptographic protocols are increasingly being used to access confidential and private information on the Internet. Due to their complex nature, they often have subtle errors that can be exploited by malicious entities. As security flaws can have serious consequences, it is important to ensure that computer programs and cryptographic protocols achieve their security objectives. As such systems have a large (potentially infinite) number of states due to presence of malicious adversaries and the concurrent nature of Internet, `pen and paper' reasoning about their correctness is challenging. In addition to the state explosion, reasoning about correctness is also challenging within the context of security because standard security objectives such as confidentiality and privacy turn out to be hyperproperties. The challenge lies in the fact that when reasoning about hyperproperties, one has to reason about correctness of the set of all executions of a system as a whole instead of correctness of individual executions. Therefore, the development of techniques to automate this reasoning is of vital importance, and is the research focus of this project. \n\nFormally, hyperproperties generalize properties that are used to express safety and liveness guarantees in classical automated verification. A property is a set of allowable executions. A system violates a property if it exhibits an execution that is not allowed. In contrast, security objectives such as confidentiality, non-interference, privacy, and anonymity are hyperproperties. A hyperproperty is a collection of allowable sets of executions. A system violates a hyperproperty if the set of its executions is not in the collection specified by the hyperproperty. Current state-of-the art automated tools for verifying security guarantees do not scale very well as they are often aimed at certain security guarantees and often make restrictive assumptions on the systems. This project aims to develop new scalable state-of-the-art techniques in automated verification of hyperproperties by undertaking primarily three research tasks. First, we will develop and implement new symbolic algorithms for verifying finite-state systems against an expressive set of hyperproperties. The second task shall be devoted to scaling the analysis by a novel combination of automated analysis and automated counterexample generation designed specifically for hyperproperties. Finally, we shall establish theoretical results that shall reduce the problem of verifying cryptographic protocols in the presence of unbounded message sizes and nonces to the finite case. The research aims of the proposal will be paired with curriculum development at the University of Missouri where a new concentration in security will be introduced in the undergraduate curriculum that will integrate security design with software development. The results of this project will be integrated in the courses, and the project will support both undergraduate and graduate student research.",1553548,3,private information
Aravind Prakash,,aprakash@binghamton.edu,SUNY at Binghamton,"CRII CISE Research Initiation, SPECIAL PROJECTS - CISE",CRII: SaTC: Robust and Platform Independent Recovery of Design Features from C++ Binaries,"Computer software play a ubiquitous role in the modern way of life. Attacks against vulnerable software lead to compromise and loss of financial and personal information. While the application stores and the software manufacturers may strive to provide vulnerability-free software, the onus to defend against attacks and ensure integrity of one?s personal information and resources is on the end-user. However, due to the lack of source code, (1) end users are unable to identify and fix vulnerabilities in the software they run, and (2) the open source community is unable to detect violation of software licensing terms by closed source software. This project aims to recover design information from binaries in a platform-neutral and obfuscation resilient manner. As a direct consequence, this project promises advancement in end-user-level security, and for the first time, facilitates detection of design-level plagiarism in software. \n\nThis project aims to reconstruct a design profile of a C++ binary by leveraging the unavoidable information leakage that occurs due to adherence to Application Binary Interface (ABI) specification. It takes advantage of the platform-independent nature of ABI specification to offer both platform neutrality and obfuscation resilience in design recovery. First, traditional static and dynamic binary analysis approaches are employed in order to extract design elements and design pertinent features. Then, theorem proving is utilized to establish relationship between various design-level program entities. This project views adherence to ABI as a source of design leakage, exploits the leakage to recover design information, and investigates ways to minimize the leakage while maintaining interoperability through adherence.\n\nThe results from this research will be disseminated through peer-reviewed publications and software release. Based on the research, new course materials and professional training tutorials will be developed, to help future security engineers and researchers gain in-depth knowledge about design recovery.",1566532,25,personal information
Ruby Lee,,rblee@princeton.edu,Princeton University,Secure &Trustworthy Cyberspace,STARSS: Small: Collaborative: Practical and Scalable Security Verification of Security-Aware Hardware Architectures,"Computers form the backbone of any modern society, and often process large amounts of sensitive and private information. To help secure the software, and the sensitive data, a number of secure hardware-software and processor architectures have been proposed. These architectures incorporate novel protection and defense mechanisms directly in the hardware where they cannot be modified or bypassed, unlike software protections. However, due to lack of practical and scalable security verification tools and methodologies, very few of the proposed hardware security architectures have been commercially deployed. This project develops a security verification methodology that is applicable to different hardware-software security architectures. \n\nThis project develops security invariants and methodology that hardware architects can deploy to check the security properties of their architectures in a scalable and semi-automated manner. The methodology is applied to verify hardware-enhanced isolation architectures and architectures that minimize the attack surface in cloud computing. Verification of a secure cache's resistance to cache side channel attacks is also investigated. Researchers and designers will have a new method to systematically check their designs, and show to others the conditions under which they work. Hardware manufacturers will gain assurance to actually implement these security architectures in real products. In turn, customers will gain assurance about the secure hardware that protects their computations running on their devices or virtual machines running on remote cloud servers. Security architectures are important to customers and hardware manufacturers, however, security verification is needed to make them a reality.",1526493,3,private information
Ryan Kastner,,kastner@ucsd.edu,University of California-San Diego,Secure &Trustworthy Cyberspace,TWC: Small: Employing Information Theoretic Metrics to Quantify and Enhance the Security of Hardware Designs,"Computing devices control much of the world around us. They power smart phones, kitchen appliances, cars, power grids, medical devices, and many of the other objects that we rely upon in our everyday lives. The foundation of these systems is the hardware, which are complex multi-billion transistor chips. Gaining control of the hardware provides unfettered access to every part of the system. This makes it a highly attractive target for attackers. Compromised hardware allows unauthorized users to obtain personal information, and can be used to force the device into unsafe and potentially life threatening scenarios. Thus, it is paramount to develop techniques to enable secure hardware design.\n\nCreating secure hardware requires the designer to assess potential vulnerabilities. Currently it is hard to concretely say anything about the security (or lack thereof) of the hardware. This project is developing quantitative hardware security metrics that enable designers to precisely evaluate the security of the system. The team is attempting this by employing statistical measures on the amount of uncertainty and information flow that is present across different portions of the hardware. These metrics are oblivious to the types of variables under consideration. Thus, the team can assess both functional security properties related to confidentiality and integrity as well as covert channels. These metrics enable the characterization of portions of the system that are potentially vulnerable to attacks. And they determine the effectiveness of mitigation techniques on the overall security of the system. The end result is more secure hardware, which leads to safer and more secure computing devices.",1527631,3,personal information
Jules Polonetsky,,julespol@fpf.org,FPF Education and Innovation Foundation,"Secure &Trustworthy Cyberspace, Big Data Science &Engineering",Privacy Research and Data Responsibility Research Coordination Network,"Concerns about privacy continue to pervade large-scale deployments of technologies including big data, cloud services and the Internet of Things. The main purpose of the proposed work is to build a community of academic researchers and industry practitioners to address research priorities identified in the newly released National Privacy Research Strategy. By engaging industry stakeholders with researchers and advancing new thinking, research, best practices and practical tools, this work will prompt industry action and advocate privacy-aware approaches to collecting and processing personal information in a manner that respects individual privacy, equality and fairness. This project aims to enhance opportunities for innovation, provide meaningful protections for personal information across industry sectors and institutions, inform the public debate on privacy, and contribute to the development of systems and products used to help society realize the benefits of networked information technology without sacrificing privacy and individual rights.\n\nThe project will undertake major activities such as an inaugural launch conference and a series of academic workshops and networking opportunities. Issues for discussion will include mechanisms to encourage and support multi-disciplinary research along a continuum of privacy challenges, including how to increase transparency of data collection and use; to ensure data flows are consistent with privacy rules; and to reduce privacy risks of analytical algorithms. Its emphasis on engagement of industry practitioners with academic researchers furthers the development of a community of privacy researchers working across disciplines and sectors and dedicated to the promotion of new knowledge, techniques and practices to better protect individual privacy.",1654085,2,personal information
Robin Sommer,,robin@icsi.berkeley.edu,International Computer Science Institute,CYBERCORPS: SCHLAR FOR SER,TC: Medium: Understanding and Managing the Impact of Global Inference on Online Privacy,"Correlation of seemingly innocuous information can create inference chains that tell much more about individuals than they are aware of revealing. However, while media coverage occasionally draws attention to privacy leaks on individual web sites, there is still no comprehensive analysis of the fundamental risks that users face in their online worlds. This project pursues such a study, focusing in particular on the threat of personal, yet publicly available information that can be correlated with modern multimedia retrieval and content analysis technologies. One thrust of the work is informing users about potential risks by exposing the broader possibilities that arise with more sophisticated privacy attacks. A second thrust concerns understanding the control that users can exercise over their privacy in the light of such potential. When analyzing the impact of ""global inference"" on privacy, the primary conceptual challenge concerns understanding the trade-off between the benefits that providing personal information to web services offers, versus the risks that doing so entails. By combining expertise from two traditionally separate communities---network security and multimedia---this work advances the start of privacy protection in an area that is poised to raise in importance as more information moves into public spaces. The project enables users to better understand the threats they are facing by developing scenarios that intuitively demonstrate the relevant effects; and it develops novel tools supporting them in better protecting their privacy. The research results will benefit the user community by promoting risk awareness and empowering control. The project also includes outreach activities including working with Berkeley Foundation for Opportunities in Information Technology (BFOIT) to recruit minority students in this research, organizing summer schools, and raising public risk awareness by working with Identity Theft 911.",1065240,30,personal information
Rebecca Wright,,rebecca.wright@rutgers.edu,Rutgers University New Brunswick,Secure &Trustworthy Cyberspace,RCN: DIMACS/Simons Collaboration in Cryptography,"Cryptography is one of the most important tools in securing data, communication, and cyberinfrastructure. Driven by ever-increasing amounts of data and the associated computational demands, organizations and individuals are outsourcing storage and computation to ""the cloud."" As our e-mail, medical, financial, and other personal information increasingly reside in systems outside of our direct control and are of increasing value to attackers, the need to simultaneously guarantee privacy, availability of data, and correctness of computations is paramount. This digital reality poses complex challenges to cryptography and requires a paradigm shift in our goals and mode of thinking. This research coordination network, led by DIMACS and the Simons Institute for the Theory of Computing, will bring together cryptographers and others to depart from the traditional goals of cryptography, namely a relatively narrow focus on secure and authenticated communication, and significantly advance the state of the art toward systems that are simultaneously highly efficient, highly secure, and highly functional. \n\nSpecific goals for this RCN project include bringing researchers together to facilitate and catalyze our understanding of: what primitives and performance can be obtained from specific intractability assumptions; fundamental tradeoffs and impossibility results; and how best to drive adoption by system designers and implementers of more secure technologies and practices. The project will start with an intensive Summer 2015 program at the Simons Institute to launch the collaboration and build momentum, followed by a two-year ""special focus"" led by DIMACS that will sustain the project through Summer 2017 and expand it to include more people and more topics. The project will enable both foundational advances in cryptography and practical advances in its usability, providing improved security, flexibility, and efficiency. These advances have the ability to positively impact society by improving: the robustness of our national cyber infrastructure and cyber-connected physical infrastructure; the security of commercial applications in banking, health care, manufacturing, media, and more; and the extent to which individuals can have control over and confidence in protection of their personal data. Participants will be diverse across a variety of dimensions, including women and other under-represented groups; a mix of junior and senior participants; people from other disciplines and other areas of computer science beyond cryptography; and both industry and academic participants.",1523467,3,personal information
Mihir Bellare,,mihir@cs.ucsd.edu,University of California-San Diego,Secure &Trustworthy Cyberspace,SaTC: CORE: Small: Foundations of Applied Cryptography,"Currently, on an almost weekly basis there are reports on security breaches which expose private information such as passwords or credit card numbers to cyber criminals. In order to address this problem, this project develops theoretical foundations and cryptographic approaches, and analyzes these new mechanisms. This project focuses on three areas of great practical interest: (1) the recovery of a cryptographic key by an adversary; (2) delegatable format-preserving encryption which, for example, allows the in-place encryption of credit-card numbers while limiting the damage in case of a compromise, and (3) user authentication on the Internet. The broader impact of this work is in that the team will develop a software library PlayCrypt with the goal to make cryptography more accessible to a larger population.\n \nKey-recovery attacks discussed in the literature lack precise models and rigorous estimates of success probability. To address this shortcoming, this project defines metrics for key-recovery security in order to enable rigorous claims about attacks and to be able to prove hedging theorems that complement moderately good security bounds under strong metrics with better bounds under these newly-developed key-recovery metrics. This is of great interest and value as key-recovery attacks are considered much more damaging in practice than violations of semantic security. Furthermore, this project defines security metrics for delegatable format-preserving encryption (FPE). FPE is widely used for in-place encryption of credit-card numbers, and delegation allows the damage from physical compromise of terminals to remain local. The project is also geared to inform current standardization efforts. As a third direction, this project considers client-to-server authentication over the Internet. Today's paradigm of passing the password to the server over a server-authenticated TLS (Transport Layer Security) channel exhibits many vulnerabilities. This project formalizes the notion of piggy-backed authentication which allows the client to convince the server of its identity without handing it its password yet designing this mechanism in such a fashion that allows its implementation within the current TLS-based web-security architecture in order to maximize the impact of this work in practice.",1717640,3,private information
A. Selcuk Uluagac,,suluagac@fiu.edu,Florida International University,"SPECIAL PROJECTS - CISE, CYBER-PHYSICAL SYSTEMS (CPS), Secure &Trustworthy Cyberspace",CAREER: Securing Sensory Side-Channels in Cyber-Physical Systems,"Cyber-Physical Systems (CPS) integrate devices that can interact with each other and the physical world around them. With CPS applications, engineers monitor the structural health of highways and bridges, farmers check the health of their crops, and ecologists observe wildlife in their natural habitat. Using sensory side-channels (e.g., light, temperature, infrared, acoustic), an adversary can successfully attack CPS devices and applications by (1) triggering existing malware, (2) transferring malware, (3) combining multiple side-channels to increase the impact of a threat, or (4) leaking sensitive information. This project develops novel security tools and techniques to protect CPS devices and applications against sensory side-channel threats. The project results are released as an open source project, so interested software developers can extend and reuse them in other CPS research. Broader impacts include educational training and tools for the CPS field, and a collaboration with the Miami-Dade County Public Schools (M-DCPS), to expose underrepresented middle school students to state-of-the art technology topics to pique students' interests in cyber-security and cyber-physical systems. \n\nThe project investigates the sensory side-channel (e.g., acoustic, seismic, light, temperature) threats to CPS devices and applications and evaluates the feasibility and practicality of the attacks on real CPS equipment. The result is novel sensory side-channel-aware security tools and techniques for the CPS devices. Specifically, the principal investigator (1) analyzes the physical characteristics of the sensory CPS side-channels to understand how the physical world impacts the cyber world of CPS devices; (2) investigates the information leakage through the sensory side-channels on the CPS devices; (3) develops a novel IDS particularly designed to be aware of the sensory CPS side-channels; (4) designs and develops a CPS security testbed for test and experiments on real equipment and simulation tools.",1453647,3,sensitive information
Augustin Chaintreau,,ac3318@columbia.edu,Columbia University,"SPECIAL PROJECTS - CISE, RES IN NETWORKING TECH & SYS",CAREER: Banalytics: Behavioral Network Analytics with Data Transparency,"Data from customers become central to many companies' business, and tomorrow's society's progress critically depends on scientists and public decision makers accessing citizens' data. But how to reconcile progress with privacy? This dilemma, everyday, is getting worse, because we lack an abstraction where analytics -- the science of identifying and exploiting individual types and trends -- are transparent to users and they can effectively own and trade their data. Today's analytics on proprietary data, and its highly debated need for regulation, seems a necessary evil for a lack of credible alternative. This project aims at unlocking this tussle by (1) building behavioral analytics that are compatible with distributed systems of personal data, (2) showing how context and social influence can be better leveraged, and (3) enabling incremental deployment along mutual benefits to make privacy economically efficient. \n\nThe principal investigator will explore a new abstraction -- Behavioral Networks -- and show its tractability using recent results in matrix factorization, algorithms exploiting social influence, and analysis of networked incentives. Banalytics will demonstrate its applicability to various analytics tasks, including data collection, behavioral targeting, content rating and recommendation. \n\nBroader Impact:\n\nThe need to regulate proprietary access to personal data is heavily discussed towards a new Privacy Bill. Unfortunately, controlling personal data used behind closed doors is highly complex, making enforcement of top-down regulation either ineffective or potentially disastrous for the free and thriving life of the web. The Banalytics project explores and makes available alternative designs empowering users, to inform this important societal debate towards better self-regulation.\n\nNot only are sound solutions to reconcile privacy and progress essential, but they need to be disseminated and understood broadly. Banalytics aims to impact the education of students at large -- not only future engineers but also the future journalists informing our citizens -- on the management of personal information and its technical, economic and societal aspects. Through close collaboration with the Columbia School of Journalism, the principal investigator will promote a unique interdisciplinary education program to allow all students and citizens, especially those who might not otherwise express interest in computer science, to be engaged in this effort.",1254035,28,personal information
Ting Wang,,tiw315@lehigh.edu,Lehigh University,CRII CISE Research Initiation,CRII: SaTC: Re-Envisioning Contextual Services and Mobile Privacy in the Era of Deep Learning,"Deep Learning (DL)-powered personalization holds great promise to fundamentally transform the way people live, work and travel, but poses high risk to people's individual privacy. This project will address the privacy risks arising in DL-powered contextual mobile services by developing solutions that facilitate the use of personal information while maintaining explicit user control over use of the information. The developed learning methods will enable learning from mobile devices in a manner flexible enough to enable current and future DL-powered contextual services, while maintaining explicit user control over how that information is used by third-party service providers.\n\nThis research will design and implement PADLOCK, a Privacy-Aware Deep Learning Of Contextual Knowledge engine. PADLOCK executes DL computation over users' personal data in a sandbox environment, while performing lightweight static and runtime analysis to ensure that mobile apps comply with users' privacy policies. The design of PADLOCK explores the tradeoff among privacy protection, communication cost, system overhead and service quality, providing solutions with different provable privacy and efficiency features for a wide range of contextual mobile services. For further information see the project web site at: http://x-machine.github.io/project/padlock",1566526,25,personal information
Narseo Vallina-Rodriguez,,narseo@icsi.berkeley.edu,International Computer Science Institute,"RES IN NETWORKING TECH & SYS, Secure &Trustworthy Cyberspace","NeTS: Medium: HayStack: Fine-grained Visibility and Control of Mobile Traffic for Enhanced Performance, Privacy and Security","Despite our growing reliance on mobile phones for a wide range of daily tasks, their operation remains largely opaque even for experts. Mobile users have little insight into how their mobile apps operate and perform in the network, into how (or whether) they protect the information that users entrust to them, and with whom they share user's personal information. A number of previous studies have addressed elements of this problem in a partial fashion, trading off analytic comprehensiveness and deployment scale. This project seeks to overcome the limitations of previous approaches by building a handset-, traffic-, and user-centric mobile measurement platform: the ICSI Haystack. Haystack offers a novel and flexible mobile vantage point capable of correlating real-world mobile traffic with user input and high-fidelity device activity at scale while also enabling mechanisms to aid mobile users to stay in control of their mobile traffic and personal data. The research community, operators and regulatory bodies will also benefit from the novel measurement mechanisms and from the data collected in order to safeguard mobile users and to increase the operational transparency of mobile apps and trackers.\n\nTo achieve this vision, this project develops novel techniques to perform high-fidelity mobile measurements by capturing user traffic in user-space on the device using native platform support. As a result, Haystack will be available for anyone to install from traditional app stores such as Google Play, thereby enhancing user reach. In order to gain a truly in-depth and broad understanding of the mobile ecosystem, Haystack takes advantage of its local operation to correlate network traffic with user input and local context, such as which app generated a particular network flow and device location, obtained from the operating system itself with real network and user stimuli. Critically, Haystack's system design must be flexible and extensible in order to enable researchers to conduct a wide range of mobile measurements, to cope with new mobile technologies, and to reach a broad cross-section of mobile users. The ability to combine all these features together in user devices makes Haystack an ideal vantage point to conduct a wide range of mobile measurements such as mobile traffic characterization in the wild, privacy leak detection, identifying online tracking services, auditing app security, and network performance measurements.",1564329,3,personal information
Parvathinathan Venkitasubramaniam,,parv.v@lehigh.edu,Lehigh University,COMM & INFORMATION FOUNDATIONS,CIF: Small: Combining Information Theoretic Security and Stochastic Control to Study Advanced Persistent Threats,"Despite tremendous advances in cryptography and communication security, information attacks -- both passive such as eavesdropping, and active such as unauthorized data injection --, can severely impair the functioning of modern infrastructural systems that combine cyber communication systems and networked physical components. The ability of adversaries to monitor transmitted data or introduce false information for sustained periods of time whilst staying undetected can result in leakage of sensitive information or cause critical damages to underlying systems with consequences ranging from airline collisions, power blackouts to malfunctioning nuclear reactors. In this research, rigorous frameworks are developed to study vulnerabilities of cyber physical systems to such persistent security threats with the goal of designing novel and resilient system controllers. The systematic approach to study adversarial behavior will not only enable effective cyber policing, but also lay a platform for developing technologies to prevent the next generation of cyber terrorism that aims to cripple basic infrastructural systems in energy, healthcare, transportation etc. The education and outreach components will facilitate an enhanced awareness in society of potential vulnerabilities of the burgeoning Internet of Things and the path towards cyber physical security.\n \nThis research will study two key challenges in securing cyber physical systems: preventing retrieval of physical system information through continually monitored cyber flows, and limiting disruption to system operations through continually hacked cyber flows. Incorporating the strengths of information theoretic security and statistical inference methodologies into a dynamic programming framework which models cyber physical system evolution as a function of external information and internal control, this research will study quantitatively the trade-offs between information security and the system operational performance, and through the process, develop attack detection and mitigation methodologies.",1617889,26,sensitive information
Ranjitha Kumar,,ranjitha@illinois.edu,University of Illinois at Urbana-Champaign,Cyber-Human Systems (CHS),CAREER: Tying Design to Outcomes: Open-sourced Analytics for Mobile App Testing,"Developing methods to do effective, usable design for mobile applications is an important problem, affecting domains ranging from healthcare to finance. Current methods for app design are largely based on either individual designer expertise and intuition, leading to variable results, or else require a large number of active users and engineering resources that are typically only available to large companies. This project's goal is to solve the research challenges involved in automatically capturing and aggregating design features and user interactions across the large number of existing mobile applications already available for download and develop a platform allowing designers to learn from the choices and experiences other designers have made. The platform will allow designers to find relevant third-party apps that have relevant design requirements and features, define experiments involving those design choices, and run the experiments by leveraging the existing user base of those apps. This will help individual designers explore options more cheaply than developing their own prototypes while providing data-driven arguments to support design decisions and communicate with other members of their project teams. The platform could also lead to tools to accumulate collective design knowledge, identifying emerging trends as well as best practices, useful for both practicing designers and as an educational resource for existing courses on web and app design. \n\nDeveloping the platform will require a number of technical advances. The first involves developing scalable systems for capturing design features and interaction data from large numbers of mobile apps and combining them into representations useful for data mining. To do this, the team will expand the existing ERICA platform for black-box design capture in a single application, developing it into a background monitoring app that captures interaction and design data for any app the user gives permission for, while detecting and obscuring personally identifying information. The monitoring app will be deployed primarily via long-term crowdworkers who are paid to install it and participate in designer-defined experiments. The second main advance is creating functional semantic embeddings for interface components based on the collected data that define common features, app states, and functions. To do this the team will capture visual, textual, structural, and interactive information about each interface element, then use multimodal embeddings of the data to first classify individual interface elements, then use those labels to identify semantics of app screens and interaction flows between elements and screens. The third advance involves developing a series of tools to use the data and semantic embeddings. This includes working with designers to develop query techniques and views over sets of applications to find relevant applications and flows; creating visualizations of user behavior based on Sankey flow diagrams that allow designers to make sense of user behavior across applications; and designing app analytics tools to support meta-analysis across applications and experiments.\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",1750563,20,interaction data
Daniel Kifer,,duk17@psu.edu,Pennsylvania State Univ University Park,Secure &Trustworthy Cyberspace,SaTC: CORE: Medium: Developing for Differential Privacy with Formal Methods and Counterexamples,"Differential privacy is an elegant formulation of conditions an algorithm must meet in order to avoid leaking personal information contained in its inputs. It is becoming mainstream in many research communities and has been deployed in practice in the private sector and some government agencies. Accompanying this growth is an unfortunate, but expected, difficulty: many algorithms that are claimed to be differentially private are incorrect. This phenomenon affects both new-comers and seasoned veterans of the differential privacy field because of the difficulty and subtlety of developing new differentially private algorithms.\n\nThis proposal outlines a research plan for the development of a system called DevDP, whose purpose is to enable novice and expert users to develop prototypes and explore differentially private algorithms. In particular, the project will develop program analysis tools and theory that leverage both programming language and machine learning technology to aid the development of correct differentially private programs by automating much of the verification and reasoning about errors. As part of broader impacts, DevDP also has the potential to help educate students and less-technical members of the scientific community by providing interactive software tools. A solid understanding of differential privacy will become crucial as it makes its way into public policy.",1702760,3,personal information
Krishna Kant,,kkant@temple.edu,Temple University,"SPECIAL PROJECTS - CISE, Big Data Science &Engineering",BDD: Dynamic Evolution of Smart-Phone Based Emergency Communications Network,"Emergency communications networks are crucial for reaching out and helping affected people during long-persisting disasters such as the Tohoku earthquake in Japan or Hurricane Katrina in the US. Because of the ubiquitous availability of smartphones, it is important to integrate them into the emergency communications network consisting of both surviving cellular network and deployed emergency communications infrastructure. All these components vary in their ability to provide necessary communications as the disaster unfolds. It is therefore essential to continuously monitor and ``tune'' the network to provide the best possible coverage and communications capability. \n\nThis project explores how to interconnect clusters of smartphones via WiFi tethering technology and integrate them with the deployed emergency network. Since the densities of people and hence of smartphones can vary substantially over the affected area, the project will explore mechanisms to conserve battery and other smartphone resources by exploiting overlap/redundancy in the sensed data. Another key issue addressed by the project is the evolution of the emergency network as the disaster situation itself evolves. The project analyzes data obtained from twitter during the disaster in order to get insights into how the network should evolve. Furthermore, the project examines how to deploy the mobile access points initially and later move them to tune the network to the needs of the evolving disaster. The analysis of twitter data involves several bigdata challenges including classification, prioritization, and ordering of tweets so the interpretation and decision making -- which is still expected to be done by humans -- is considerably simplified.\n \nThe rapidly increasing penetration of smart phones that carry a wide array of sensors makes them ideal for disaster communications. In addition, twitter has established itself as the premier human communication mechanism during disasters owing to its modest technological requirements and ease of use. This project takes these phenomena and stitches them together to create an agile disaster communications network that can provide significant additional value in quickly reaching affected people and collecting crucial data for rushing the required assistance to them. As a part of US-Japan joint program, with research teams from Temple University in the United States and Aizu University in Japan, the project will develop technologies that will be of direct relevance to both countries.",1461932,2,twitter data
Mihaela van der Schaar,,mihaela@ee.ucla.edu,University of California-Los Angeles,"Big Data Science &Engineering,",EAGER-DynamicData: Real-time Discovery and Timely Event Detection from Dynamic and Multi-Modal Data Streams,"Emergency responders (police, fire, ambulance services) have more and more access to more and more data stream: sensor readings, security cameras, personal reports (via cellphone, texts, tweets), GPS data etc. The availability of these data streams presents enormous opportunities - but also poses fundamental challenges:\n* Data streams arrive from a wide variety of sources and contain many diverse features; this makes it difficult to extract information from the streams, and especially, to integrate information from different streams. \n* Knowledge learned from past events must be transferred to knowledge about present (and future) events. Because no two events are ever identical, the knowledge learned from past events must be transferred to knowledge about present events that are not identical but only ""similar"" - and in ways that may not be known in advance and so must be discovered. \n* Learning and detection - and the actions that follow learning and detection ? must take place in a timely fashion: it is of little use to learn how to respond to an emergency only long after the emergency has passed. \nTo accomplish this, the proposed work relies on new methods to discover what is relevant both in each individual data stream and across data streams, and to learn and exploit the similarities between the past and the present. This work is transformative and success in this project has the potential to lead to enormously enhanced, even life-saving, responses to emergencies of many sorts. \n\nExisting approaches treat individual data streams by exploiting particular physical characteristics of the signal, and treat multiple data streams in an ad-hoc fashion. These approaches miss the fact that it is not the physical characteristics of the signal that are important but rather the (semantic) information in the signal, and that there are connections between the information in different data streams. This project transforms the problem of learning from multiple (multi-modal) data streams by focusing on the relevance of information in each data stream, across data streams, and through time. The relevant information will generally be different for different events and different purposes and will not be known in advance, so relevance must be learned. To do this, this project organizes the information available at each moment in time in terms of contexts which encode exogenous metadata (e.g., when, where and by whom data was gathered) and endogenous metadata (e.g., features and statistics extracted from the data). In general, there are an enormous number and variety of contexts, but the most relevant information is embedded in only a few contexts. Because these most relevant contexts will not generally be known in advance and will be different in different scenarios, this project will develop a new class of methods and algorithms to discover the relevant contexts from multiple dynamic, multi-modal and high-dimensional data streams, and to use what is discovered to learn, detect and respond in a timely fashion. Because no two events are exactly the same, this project will develop of a new class of methods and algorithms for the discovery of relevant semantic similarities and their application, making it possible to transfer knowledge learned from past events to knowledge about present events. This work requires the development of highly innovative methodology and techniques that go far beyond existing work (high risk) and are potentially transformative for a wide variety of applications ranging from event detection to actionable intelligence.",1462245,2,gps data
Sara Riggs,,sriggs@clemson.edu,Clemson University,Cyber-Human Systems (CHS),"CAREER: Collaboratively Perceiving, Comprehending, and Projecting into the Future: Supporting Team Situational Awareness with Adaptive Multimodal Displays","Especially in data-rich and rapidly changing environments, effective teams need to give members the information needed to develop awareness of their own, their teammates', and the overall team's current situation. However, attentional demands are high on such teams, raising questions of how to both monitor those attentional demands and develop systems that adaptively provide needed information not just through visual displays that are often overloaded, but through other senses including touch and sound. Most existing work on adaptive multimodal interfaces for situational awareness focuses on individuals; this project will address how to do this work for teams, using unmanned aerial vehicle (UAV) search and rescue as its primary domain. This includes developing conceptual models that connect individual and team-level situational awareness, algorithms that use eye gaze data to assess both situational awareness and workload in real-time, and multimodal display guidelines that adaptively present information to the most appropriate team members through the most effective modes. This work will fundamentally advance research on understanding and designing to support team interaction, leading to practical improvements in a variety of safety-critical domains. The project also has a significant educational component, providing research opportunities for both graduate and undergraduate students and conducting design activities aimed at outreach and broadening participation in STEM disciplines, including workstation design to support teams of people with disabilities in manufacturing contexts.\n\nThe research work has two main thrusts. The first involves collecting baseline data through a study where pairs of novices are trained to carry out simulated UAV search and rescue tasks using a standard visually-focused interface; the team will collect situational awareness (SA) assessments using existing validated surveys, eye gaze data, and team interaction data and member characteristics. This data will be used to build two main models. The first is a model that relates team dynamics and individual member characteristics with levels of SA and performance, using qualitative analysis of recorded observational and audio data, along with focus group interviews with participants. The second is a quantitative model that attempts to predict SA using eye gaze data, using both a factor analysis of eye gaze data and Markovian models of how teams and their members transition their visual attention between interface elements and tasks to predict levels of SA. These models will support unobtrusive assessments of SA that avoid the interruptions imposed by existing surveys and are necessary for developing the adaptive multimodal interfaces that are the other main thrust of the project. This second thrust will use the models of attention and problematic tasks and contexts identified in the first study to iteratively develop a pilot suite of multimodal interfaces that combine visual, audio, and tactile information channels. These multimodal interfaces will be evaluated using a series of studies similar to the first set, with the goal of developing cost-benefit models for presentation modes and types of information that minimally interfere with teams' existing visual workload while still providing information that raises individual and team SA.\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",1750850,20,interaction data
Kent Seamons,,seamons@cs.byu.edu,Brigham Young University,"SPECIAL PROJECTS - CISE, Secure &Trustworthy Cyberspace",TWC: Small: Middleware for Certificate-Based Authentication,"Every time someone uses a phone or computer to connect to an Internet site, software determines whether the connection is safe or being intercepted by attackers. Unfortunately, this software is error-prone, leaving users vulnerable to having their privacy violated or their personal information stolen due to phishing attacks, identity theft, and unauthorized inspection of their encrypted traffic. A number of solutions are being proposed, but the software is fragmented across many platforms and redundantly or incorrectly implemented. The goal of this research is to develop a trust platform that consolidates the decision-making process into a single location to provide a correct, consistent, and usable service for all existing and future applications. The platform is designed to make it easier for researchers to test and deploy alternative methods for detecting trustworthy Internet sites. The research emphasizes usability, so that users are not faced with error messages that are difficult for them to understand or act on.\n\nThe project designs and analyzes middleware that consolidates certificate-based authentication in the operating system. It is designed for both Linux and Android, and automatically overrides the broken decision-making process currently found in many existing applications. Developers are no longer required to provide their own certificate validation function when building applications. The platform supports a variety of plugins for certificate authentication, such as dynamic certificate pinning, crowd-sourced notary services, and a curated root store. Policy configuration enables users or organizations to define how responses from multiple plugins are combined to make a final trust decision. Usability studies are focused on all aspects that require user interaction with the system: installing the system, configuring security and privacy preferences, choosing authentication providers, and receiving actionable notifications and warnings. This research benefits national security by addressing numerous weaknesses in the current trust system with new authentication services. The project contributes to the security community by providing an open source platform for easily extending the current trust system.",1528022,3,personal information
Evimaria Terzi,,evimaria@bu.edu,Trustees of Boston University,INFO INTEGRATION & INFORMATICS,III: Small: Entity Selection and Ranking for Data-Mining Applications,"Expert-management portals like linkedin.com, odesk.com and guru.com are indicative sites that allow people to advertise their work or set of skills to the broader public. For example, linkedin features more than 120 million members which allows potential employers, collaborators, etc. to discover individuals or groups of individuals with the desired expertise. Similarly, review-management sites like Amazon or Yelp collect large number of reviews about products or services. For example, kindle has more than 30,000 reviews on Amazon. Naturally, users cannot go over all these reviews and are helped significantly by the identification of a small subset of reviews that is sufficiently informative. Finally, as online social and media networks grow in importance as sources of news and other information, there is an urgent need for tools that automatically identify and recommend important nodes of the network, that specific users may need to follow to fully exploit the power of online social media. In each of these scenarios, given a collection of entities (e.g., reviews about a product, experts that declare certain skills, network nodes or edges), the goal is to identify a subset of important entities (e.g., useful reviews, competent experts, influential nodes respectively). \n\nExisting work on recommender systems attempts to identify important entities either by entity ranking or by entity selection. Entity-ranking methods associate a a score with each entity; They ignore the redundancy between the highly-scored entities. Entity-selection methods try to overcome this drawback by evaluating the desirability of a group of entities taken together; They attempt to identify the best subset of entities, while ignoring other subsets of entities that may be equally-good or almost as good as the best subset. Against this background, this project aims to overcome the drawbacks of existing entity selection and entity ranking methods through a synergistic integration of both into a common framework that allows entity-ranking based on entity selection and entity-selection that based on entity ranking. In the resulting framework, the scores of individual entities are determined in part by the number of good groups of entities they can be part of; and good group of entities consist of entities with high scores. \n\nThe main challenge addressed by this work is how to explore the solution space of combinatorial problems in order to identify subsets of entities that participate in many good solutions. The resulting new practical methods for exploring the solution space of combinatorial problems find applications related to expert management systems, management of online product reviews, and network analysis (including physical and social networks). The project also offers enhanced opportunities for research-based training of graduate and undergraduate students at Boston University. All of the research results including publications, software, and data will be freely disseminated to the broader research and educational community through the project website at: http://www.cs.bu.edu/~evimaria/sel-and-ranking.html.",1218437,30,linkedin
William Harris,,wharris@cc.gatech.edu,Georgia Tech Research Corporation,Secure &Trustworthy Cyberspace,STARSS: Small: Self-reliant Field-Programmable Gate Arrays,"Field-programmable gate arrays (FPGAs) are hardware circuits that can be reconfigured by a system user after being deployed. FPGAs are a compelling alternative architecture that may allow hardware performance to continue to improve at a dramatic rate. Unfortunately, systems that incorporate an FPGA may allow a potentially untrusted user to reprogram hardware after it has been deployed. Such a scenario enables novel security attacks that can leak a user's private information or corrupt critical information stored on a system, but are performed entirely in hardware. This project develops an approach for ensuring that FPGAs satisfy strong security policies even when programmed by an untrusted user that will incur no overheard for runtime performance. \n\nThis research investigates techniques that automatically infer proofs of information-flow properties of circuits. This project designs and implements policy languages, proof languages, checking-circuit synthesizers, and proof generators, which in combination will dramatically improve the security of FPGA-based systems. These techniques will enable devices and data-centers to use FPGAs in novel circuit designs that satisfy strong, precise security guarantees and can be updated dynamically.",1526211,3,private information
Adam Smith,,ads22@bu.edu,Pennsylvania State Univ University Park,"Secure &Trustworthy Cyberspace, Big Data Science &Engineering","BIGDATA: F: DKA: Scalable, Private Algorithms for Continual Data Analysis","For the very same reasons that big data is transforming modern life, it also presents a profound threat to privacy and the control of personal information. A major challenge associated with big data is to enable statistical analysis of complex data sets, without compromising the privacy of the individuals whose data they contain. Addressing this challenge is both necessary, since access to many data sources is restricted due to privacy concerns, and difficult, as numerous attacks on supposedly anonymized data demonstrate. This project will investigate the design and limitations of algorithms for the private, continual analysis of time-varying data sets. That is, it will study algorithms that release information about a data set as it is collected (say, in the form of a data stream from the web, or a long-term sociological study). The research will advance the state of the art in the private analysis of ""big"" -- massive, complex, time-varying -- data. If successful, the project will provide enabling technologies that facilitate research in areas where access to sensitive data is limited by confidentiality concerns.\n\nThe project will focus on the design of algorithms that satisfy differential privacy -- a rigorous notion of privacy that is widely studied in computer science and related fields. The privacy implications of sequential releases are still poorly understood, and relatively few of the algorithms developed in the extensive recent literature on private data analysis allow for sequential releases with high accuracy. The two major thrusts of the project are (1) algorithms for the ""continual release"" model, and (2) algorithms for the ""local"" model, which offers even stronger privacy guarantees. The work will provide novel algorithmic design techniques and understanding of complexity-theoretic limitations of algorithms for these models. The research will entail advances in related areas such as learning theory, statistical inference and streaming algorithms. The project will also include educational, outreach and work-force training activities designed to broaden the impact of the research.\n\nFor further information see the project web site at: http://www.cse.psu.edu/~asmith/projects/continual/",1447700,2,"anonymized data, personal information"
Natalya Bazarova,,nnb8@cornell.edu,Cornell University,Cyber-Human Systems (CHS),CHS: Medium: Understanding and designing for online disclosure and its effects on well-being,"From mundane details of daily life to tragic warnings of planned suicide, people exchange a massive amount of personal information in social media that defies traditional models of self-disclosure. This project will advance understanding of personal information disclosure in social media contexts and will use this information to develop well-being interventions designed to enhance self-reflection and capacity to productively seek and offer support. Such interventions benefit individuals and society overall by helping people know how and from where to seek support in informal and formal social networks. Knowledge about key drivers of self-disclosure will also be useful in improving the design of other systems that use personal data and will inform public discussions about the use and risks of personal information online. The PIs will work to raise awareness of disclosure benefits and risks through users' participation in experiments, use of systems, and publicizing research results beyond the research community, e.g., by working to move research results into practical applications through the Bronfenbrenner Center for Translational Research at Cornell.\n\nMore specifically, this research will yield a multi-theoretical and multi-level model of how individual attributes such as personality and mental health status, technological affordances such as reviewability and audience visibility, and social network properties such as size, density, diversity, and tie strength, individually and in combination, shape anticipated rewards and risks of disclosure and disclosure strategies. These models will incorporate responses by network members into the production cycle of disclosure, and examine how these combined characteristics determine both anticipated and actual outcomes. The researchers will collect examples of actual disclosure behaviors in both text and photos, annotated with information about people's disclosure goals and perceptions, individual characteristics, and social networks, and will use these to validate predictive models of the presence of and responses to disclosure in social media data. These models will help identify when people create meaningful content, which in turn can be used in systems and interventions that support the well-being of social media users, both in the general population and those at risk. These systems and interventions will operate by using disclosed information to facilitate reflection, enrich existing positive psychology interventions, and promote awareness of and effective responses to disclosure and mental health needs.",1405634,20,"social media data, personal information"
Berk Sunar,,sunar@wpi.edu,Worcester Polytechnic Institute,Secure &Trustworthy Cyberspace,TWC: Medium: Collaborative: Development and Evaluation of Next Generation Homomorphic Encryption Schemes,"Fully homomorphic encryption (FHE) is a promising new technology that enables an untrusted party to efficiently compute directly on ciphertexts. For instance, with FHE a cloud server without access to the user's encrypted content can still provide text search services. An efficient FHE scheme would significantly improve the security of sensitive user data stored and processed on cloud servers. Significant progress has been made in bringing FHE proposals closer to practice. The PIs conduct a multi-pronged investigation of next generation homomorphic encryption schemes that do not require heavy computation or storage.\n\nThe investigation is structured in two research modules. The first module focuses on the theoretical foundation of FF-Encrypt - a new scheme by the PIs - including a thorough security analysis, selection of parameters for secure and efficient construction, and investigation of noise mitigation techniques. The second module compares the new scheme to existing ones with similar properties to establish its security and efficiency. Further techniques are studied to improve the scalability of the scheme. The performance of the developed theoretical homomorphic encryption schemes is scrutinized under the lens of optimized software and hardware implementations.\n\nThe investigation develops strategies for creating and optimizing next generation FHE schemes. In particular, the proposed new FF-Encrypt scheme has the potential to reduce parameter sizes while also eliminating evaluation keys, pushing FHE a step closer to the realm of real-world applications. The challenges in this project include the analysis of a new homomorphic scheme based on a new assumed computationally hard problem.",1561536,3,user data
Eytan Modiano,,modiano@mit.edu,Massachusetts Institute of Technology,RES IN NETWORKING TECH & SYS,NeTS: Small: Optimizing Information Freshness in Wireless Networks,"Future Internet-of-Things (IoT) applications will increasingly rely on the exchange of delay sensitive information. Application domains such as autonomous vehicles, industrial control systems, virtual reality, and sensor networks, heavily rely upon the distribution of time-critical information. Age of Information (AoI) is a recently proposed performance metric that measures the freshness of the information from the perspective of the application. AoI measures the time that elapsed from the moment that the most recently received data was generated to the present time. This project develops an AoI framework for characterizing latency in wireless networks, and mechanisms for minimizing latency; thus enabling a wide range of societal benefits including the control of critical infrastructure, mobile health care, and automated manufacturing. \n\nThe goal of this project is to extend the Age-of-Information framework to the wireless network setting. In particular, this project advances the theory of AoI from the simple and abstract queueing models to realistic wireless network models, taking into account the effects of wireless interference, transmission scheduling, and multi-hop routing. The project will consider single-hop networks, such as LTE or WiFi; multi-hop networks such as sensor networks; and mobile ad-hoc networks. The goal is to understand the impact of network topology, node mobility, interference and channel reliability on AoI, and to develop network control mechanisms that minimize AoI under different wireless network settings. The project will progressively extend the AoI framework to the wireless setting: Starting with simple single-hop networks and extending to mobile multi-hop networks. The research agenda includes the following tasks: 1)Single-Hop Wireless Networks: Develop methods to analyze AoI in single-hop wireless networks, and devise transmission scheduling schemes for minimizing AoI; 2) Multi-Hop Networks: develop routing and scheduling schemes for minimizing AoI in multi-hop wireless networks; and 3)Mobile Wireless Networks: develop fundamental limits on AoI in mobile networks, and develop packet relaying schemes for minimizing AoI.",1713725,28,sensitive information
Srini Devadas,,devadas@mit.edu,Massachusetts Institute of Technology,Secure &Trustworthy Cyberspace,STARSS: Small: Trapdoor Computational Fuzzy Extractors,"Fuzzy extractors convert biometric data into reproducible uniform random strings, and make it possible to apply cryptographic techniques for biometric security. They are used to encrypt and authenticate user data with keys derived from biometric inputs. This research investigates how hardware security primitives can have provable cryptographic properties, a connection which is largely lacking in currently available hardware primitives. The development of such computational fuzzy extractors could result in substantially more efficient and reliable key extractors which may be better received by industry and other stakeholders, due to their improved efficiency and well-established security properties. \n\nComputational fuzzy extractors derive keys from biometric sources including silicon biometric sources, and their security is based on the difficulty of problems such as Learning Parity With Noise (LPN). Existing computational fuzzy extractors require exponential time to extract keys when the bits generated by the biometric source contain a constant fraction of errors. The project explores the concept of a noise-avoiding trapdoor that results in a computational fuzzy extractor that can correct errors in polynomial time in a constant fraction of the bits generated by the biometric source. The security assumption is exactly the assumption of computational hardness of LPN. This approach remains secure under weaker assumptions about biometric data than previous schemes which assumed uniform distributions of biometric data. The project introduces high-school students to research in applied cryptography and security through the MIT PRIMES high-school outreach program.",1523572,3,user data
Chinmay Hegde,,chinmay@iastate.edu,Iowa State University,COMM & INFORMATION FOUNDATIONS,CAREER: Advances in Graph Learning and Inference,"Graph-based data processing algorithms impact a variety of application domains ranging from transportation networks, artificial intelligence systems, cellphone networks, social networks, and the Web. Nevertheless, the emergent big-data era poses key conceptual challenges: several existing graph-based methods used in practice exhibit unreasonably high running time; several other methods operate in the absence of correctness guarantees. These challenges severely imperil the safety and reliability of higher-level decision-making systems of which they are a part. This research introduces an innovative new computational framework for graph learning and inference that addresses these challenges. Specific applications studied in this project include: better approaches for monitoring roadway congestion and identify traffic incidents in a timely manner; root-cause analysis of complex events in social networks; and design of better personalized learning systems, lowering educational costs and increasing quality nationwide. Activities include integrated programs to increase participation of women and under-represented minorities in the computational sciences. \n\nFrom a technical standpoint, the investigator pursues three research themes: (i) designing scalable non-convex algorithms for learning the edges (and weights) of an unknown graph given a sequence of independent static and/or time-varying local measurements; (ii) designing new approximation algorithms for utilizing the structure of a given graph to enable scalable post-hoc decision making in complex systems; (iii) developing provable algorithms for training special families of artificial neural networks, and filling gaps between rigorous theory and practice of neural network learning. Progress in each of the above themes will be extensively evaluated using real-world data from engineering applications including social network data, highway monitoring data, and fluid-flow simulation data. Collaborations with domain experts in each of these application areas will ensure that the new theory, tools, and software emerging from this project will lead to meaningful societal benefits.",1750920,26,social network data
L Sethumadhavan,,simha@cs.columbia.edu,Columbia University,TRUSTWORTHY COMPUTING,CAREER: Trustworthy Hardware from Untrustworthy Components,"Hardware components can contain malicious, illegal modifications that can siphon sensitive information to transmit to adversaries or shutdown critical operations. Such modifications to the hardware - the root of trust in computing - can compromise trustworthiness of systems. A malicious modification (backdoor) can find its way into a design through a core component implemented by a malicious insider on the team, or through a third-party intellectual property (IP). This research investigates techniques to build trustworthy hardware systems even with such untrustworthy, malicious hardware components.\n\nA key insight used in this study is that while a large number of digital backdoor implementations are feasible they can be all classified into four simple categories with unique behavioral characteristics that can be leveraged to thwart backdoors. Digital backdoors can only be triggered in two ways: they must turn-on after certain amount of time or by specially crafted inputs. This classification is complete because time and data are the only two ways a digital system can change. After a backdoor is woken up it can impact the infested hardware component in only two ways: it can cause the hardware component to produce extraneous outputs or corrupt existing outputs. Backdoor detection techniques monitor the outputs of hardware units for anomalous behavior while protection techniques scramble inputs making it infeasible for the backdoor to be triggered at runtime. Hardening both the inputs and outputs of a hardware module against backdoors provides comprehensive protection and provides a strong basis, for the first time, to certify digital hardware to be free of backdoors. Such certification can significantly enhance the state of trustworthy computing",1054844,30,sensitive information
Ben Zhao,,ravenben@cs.uchicago.edu,University of California-Santa Barbara,Secure &Trustworthy Cyberspace,TWC: Small: User Behavior Modeling and Prediction in Anonymous Social Networks,"Human beings are diverse, and their online behavior is often unpredictable. In today's data-driven world, providers of online services are collecting detailed and comprehensive server-side traces of user activity. These records or logs include detailed, timestamped logs of actions taken by users, often called clickstreams. Given their scale and level of detail, clickstreams present an enormous opportunity for research into user behavioral analysis and modeling. Understanding, modeling and predicting user behavior can dramatically improve the security of today's online systems, while significantly advancing understanding of user behavior. This project develops a general platform for user behavioral modeling using clickstreams, with the goal of providing general tools for modeling user behavior in any application context. If successful, this approach will produce a generalized platform for identifying similar types of user behavior. Prior work using a similar approach already produced significant results in the context of automatically detecting fake accounts and identities in online social networks.\n\nThe PIs will explore the use of clickstream similarity graphs, graphs designed to capture and model the similarity (or differences) between behavior logs of different users. By applying existing graph analysis techniques, these similarity graphs can identify general user behavioral patterns using semi-supervised learning techniques, and can be used to identify abnormal or unknown user behavior patterns. The researchers will use real detailed clickstreams from two online social networks (Renren and Whisper). The goal of the project is to make clickstream similarity graphs a general and practical user modeling tool. The project will address 3 key challenges. First, it will explore and address challenges of scale in users and trace length, so that the techniques can be applied to large user populations of hundreds of millions. Second, the project will quantify the level of dynamics in user behavior over time, developing techniques to incrementally modify or update user behavior models. Finally, the PIs will study issues in application specificity, i.e., how we can tune the tool for different dimensions of user behavior.",1527939,3,clickstream
Ye Zhao,,zhao@cs.kent.edu,Kent State University,S&CC: Smart & Connected Commun,S&CC: Support Community-Scale Study by Visual Analytics of Human Mobility and Opinion Data from Social Media Data,"Improvements to the urban physical landscape, such as adding new greenspaces and healthy activity destinations, or removing problem areas like ""blight"" are decided upon using ideas, plans and existing theory. Yet little is ever done to evaluate the effectiveness of such improvements. The project develops a visual analytics platform of anonymized human mobility and human opinion data retrieved from social media, so that community-focused stakeholders can interactively study human activity usage and associate insights around multiple location types. In this way questions can be asked such as has this change actually done any good on this/my street? Or, where can I tell my patient to exercise that is safe, culturally acceptable, and appropriate to who he/she is? This visual analytics system can become part of a best practices approach when re-designing urban environments, where re-designing means changing aspects of the built, social and physical environment. The Broader Impacts of this work include potential usage beyond academia; a doctor interactively investigating the neighborhood around a patient, or a community group considering the likely impact of adding a community garden by investigating the changes such an initiative has had elsewhere. The project provides a non-specialist software interface that makes that type of analysis ubiquitous. This tool can be applied irrespective of cultural, racial or economic barriers. The geographic area of the proposed study comprises largely of minority neighborhoods allowing us to show the utility of this approach to attack issues of racial disparity. Every effort will be made to attract and employ minority students on the project in order to reflect the social makeup of these study neighborhoods.\n\nThis project is the first near real time intervention tool designed for community use that utilizes the most dynamic data available: social media trajectory data and associated contextual meaning. The technique takes social media and leverages it in an interactive visual system to answer day-to-day questions by non-specialized users. It captures behaviors/activities related to urban communities by investigating and understanding social media trajectory data. These points of investigation will be gleaned from a rich array of sources including community narratives. Important negative and positive spaces, and changes to those spaces, will be investigated to justify the utilization of big social media data and evaluate our visual analytics system. A database, NeighborBase, manages these heterogeneous data extracted from tweets and support various real-time queries. A community-scale visual analytics tool, NeighborVis, further provides intuitive interactions for users to perform efficient knowledge discovery over NeighborBase. The software platform seamless integrates these data with computational and visualization algorithms for the design and improvement in physical communities.",1637242,14,social media data
Pramod Viswanath,,pramodv@uiuc.edu,University of Illinois at Urbana-Champaign,COMM & INFORMATION FOUNDATIONS,CIF: Medium: Anonymous Broadcasting over Networks: Fundamental Limits and Algorithms,"In a free society, people have the right to live without being unduly monitored or surveilled. When that right is violated, anonymity-preserving technologies can provide a safety net. Anonymity-preserving systems are specifically designed to prevent adversaries from linking users to their actions within the system; most online services today are not designed to protect anonymity.\n\nThis project studies the anonymity properties of networked systems in which users broadcast sensitive information over a network. It considers two representative applications: anonymous microblogging and cryptocurrencies. Anonymous microblogging refers to social networks where users broadcast messages over a social graph; examples include Yik Yak and Whisper. Cryptocurrencies are distributed, digital currencies that allow a community to collectively, cryptographically verify the legitimacy of transactions; Bitcoin is the most successful example. Both anonymous microblogging platforms and cryptocurrencies broadcast sensitive messages over a network and are generally perceived as systems that protect user anonymity. This project aims to understand, from first principles, the vulnerabilities of existing broadcasting protocols against statistical deanonymization attacks, and whether such attacks can be prevented by carefully designing new protocols. Additionally, the project will develop open-source implementations to demonstrate the key theory including a distributed anonymous messaging application and a lightweight upgrade to Bitcoin's networking stack. The project also includes training of graduate and undergraduate students in this important research area.\n\nThe project is conducted in three steps. The first step is model-building which extracts simple, canonical mathematical models that capture the essence of these complex systems. The second step is source detection, where an adversary performing statistical inference to deanonymize the source of a broadcast is considered, and the mathematical models from step 1 are used to analyze the vulnerability of current protocols. The final step focuses on source hiding where the aim is to design novel broadcasting and network-management protocols that give provably-optimal anonymity guarantees while maintaining high utility for the target applications.",1705007,26,"sensitive information, social graph"
S. Shyam Sundar,,SSS12@psu.edu,Pennsylvania State Univ University Park,"SPECIAL PROJECTS - CISE, Secure &Trustworthy Cyberspace",EAGER: Exploring Heuristics and Designing Interface Cues to Understand Revealing or Withholding of Private Information,"In individual pursuits of personalized service and other functionalities, people disclose personal and private information by trusting certain online sites and services. Scholars often assume that such trust is based on a careful assessment of the benefits and risks of disclosing information online. This project departs from such an assumption and investigates the possibility that decision-making about online information disclosure is not systematic, but rather based on cognitive heuristics (or mental shortcuts) triggered by cues in the interaction context. The objective of this project is to identify positive and negative heuristics triggered by online interface cues that predict why users instinctively trust or distrust certain online systems. \n\nPhase 1 involves exploratory interviews and a survey pertaining to privacy and security mishaps. Based on findings from these studies, the PIs design cues and related interface functionalities in Phase 2. Phase 3 features controlled experiments that empirically assess how interface cues trigger heuristics and what role they play in user trust and information disclosure.\n\nThe findings of this research can be transformational in advancing knowledge about the many paradoxes that confront researchers, such as the privacy paradox (users reveal more than they admit) and control paradox (provision of control to users makes them worry more, not less, about security). Design insights from this project can be propagated to a variety of contexts to trigger desired heuristics and promote secure and trustworthy computing. The enumeration of heuristics serves a media-literacy function by alerting users to common psychological biases that compromise the security of their information.",1450500,3,private information
Evimaria Terzi,,evimaria@bu.edu,Trustees of Boston University,INFO INTEGRATION & INFORMATICS,CAREER: On the identification of collections with complex objectives,"In many domains, there is an increasing reliance on Recommender Systems for helping identify products, services and people that meet some user-specified criteria. Given a pool of entities (e.g., movies, books, experts) and an objective function such systems have to identify a collection (i.e., a subset) of entities from the pool that optimizes the objective function. For example, in movie-recommendation systems (e.g., Netflix) the goal is to identify subsets of movies to recommend to registered users. Analogous problems arise in social networks and social media (e.g., Twitter, Facebook), where advertisers need to identify a small set of targets for their advertisements. Finally, project management teams in large organizations often use expertise management systems to identify the subset of experts needed to complete a specific project.\n\nCurrent Recommender Systems suffer from severe limitations in settings where (i) the users multiple interactions with the system over time and the recommendations provided to a specific user at any given time need to take into account the past recommendations given to the same user or (ii) The entities that make up the recommended collections are rational entities, e.g., participants in a social network, or members of a project team, that have their own goals and preferences that influence their behavior as members of the collection. This project aims to address these two shortcomings of current Recommender Systems by designing, implementing, and evaluating combinatorial algorithms for identifying (a) sequences of collections, rather than a single collection and (b) collections of rational entities with individual goals, preferences, or objectives.\n\nIn addition to developing a suite of novel combinatorial algorithms and heuristics for recommending sequences of collections and collections of rational entities, the project aims to develop and deploy two application-specific testbeds: (i) a personalized meal planner provides to its users weekly meal recommendations to guide them towards healthy eating choices; and (ii) A crowdsourcing platform with support for virtual team formation to allow students registered in some of the courses at Boston University, to form teams online to collaborate on class projects (when appropriate). \n\nBroader impacts of this research include: new models and methods that signficantly advance the current state of the art in Recommender Systems, with broad applications in a number of domains including social networks (e.g., LinkedIn, Facebook, etc.), online recommendation systems (e.g., Amazon, Netflix, etc.), and daily-deal sites (e.g., Groupon, LivingSocial, etc.). The project contributes to the education and advanced research-based training of graduate and undergraduate students in Computer Science at Boston University. Wide dissemination of software implementations of the algorithms can be expected to benefit the larger research community.\n\nAdditional information about the project, including links to project personnel, publications, and software can be found at: http://www.cs.bu.edu/~evimaria/recommendations.html",1253393,19,linkedin
Xiuzhen Cheng,,cheng@gwu.edu,George Washington University,Secure &Trustworthy Cyberspace,SaTC: CORE: Medium: Collaborative: Privacy Attacks and Defense Mechanisms in Online Social Networks,"In online social networks, people and their connections often share personal information, such as demographics, interests, and opinions, and leave traces of their interaction with others and content in the network. Not everyone wants to share personal information; however, people's attributes are correlated with each other among themselves, with attributes of nearby people in the network, and between a person's accounts on different networks. These correlations create risks around inferring attributes people would rather keep private. This project will try to identify and quantify the risks by developing new ways to infer attributes by leveraging these correlations, then develop defense mechanisms in two common social networking tasks. For querying social network datasets, which is commonly used in advertising and research, the researchers will develop new differential privacy techniques for networks to ensure that query results do not inadvertently identify individual users or their attributes. For matching social network profiles, which is often used in recommender systems, the team will develop novel similarity matching methods that work on encrypted personal data. Overall, the research will provide a deeper understanding of the risks of inadvertent leakage of personal information and possible technical and policy approaches for addressing those risks. The project will also provide research opportunities for both graduate and undergraduate students at three institutions, and the research team will emphasize recruiting students from historically underrepresented groups in computing both at the college and high school level. \n\nThe project is organized around three main thrusts. The first thrust is to develop inference attacks on users' attributes and identity in social networks. To do this the team will first compute the relative discriminatory power of different attributes based on their distributions in the network, then use this and network structural information to perform attribute inference through affinity propagation. The second thrust focuses on improving differential privacy protection for graph queries. For this, the team will define similarity metrics that account for the non-independence of edges in social networks to better protect attribute privacy and develop new query techniques based on subgraph partitioning and consideration of the sensitivity of the query function. They will also develop new variants of differential privacy based on k-anonymity that hide a user's attributes relative to those of similar users. The third thrust explores how to do profile matching without revealing sensitive personal information, inspired by ideas from secure multiparty computation. Here, the team will develop efficient and accurate methods to do dot-product computation on data protected by chaos-based encryption and keyword search on data protected by attribute-set-based encryption, as well as hashing-based approaches to compute image similarity without sharing the image data itself. The team will release its code, suitably protected datasets, and tutorials and educational materials through a dedicated project website, and do outreach to members of underrepresented groups through the McNair programs, Women in Computer Science, the Society of Hispanic Professional Engineers, and the National Society of Black Engineers.",1704397,3,personal information
Mahadevan Gomathisankaran,,mgomathi@unt.edu,University of North Texas,"INFORMATION TECHNOLOGY RESEARC, Smart and Connected Health",RAPID: SCH: A Framework for Epidemic Contact Tracing Using Multi-contextual Information,"In order to stop the Ebola Epidemic it is essential to do effective ""contact tracing."" The problem is complex as the contact tracing has to be done retroactively after a patient is diagnosed with the disease. Any lapse in contact tracing could potentially fail to track citizens at risk and spreading of the disease. Ad hoc tracing, relying on the infected carrier?s recollection of places visited and people met may lead to inaccurate tracking. Similarly, relying on the recollections of individuals who may have come into contact with the carrier cannot accurately identify those who are at high risk and those who are at low risk of contracting the disease. This project proposes a framework that uses existing readily available technologies that can do effective, automated contact tracing without compromising the privacy of the users. The framework has two parts: aggregator - which collects information from the users, and analyzer --- which processes the information to do contact tracing. This project will develop advances in algorithms to minimize false positives, storage features for context and social data that is both scalable and will preserve privacy of potential contacts. \n\nThe best current method for managing the Ebola epidemic currently is through contact tracing; that is, retrospectively identifying anyone with whom the affected person has come in contact. The current contact tracing system is time intensive and error-prone. The proposed system will do the tracing automatically, using a scalable, privacy preserving method that draws data from the affected person?s social network and smart phone context. The proposed framework is not only applicable to contact tracing for Ebola, but also can be used for contact tracing of other infectious diseases, as well as emergency preparedness in the case of bio-terrorism attacks. The technologies developed as part of this project, such as privacy preserving storage architecture, can also be applied to other problems.",1513369,7,social data
Rakesh Verma,,rverma@uh.edu,University of Houston,Secure &Trustworthy Cyberspace,TWC: Small: Unsupervised and Statistical Natural Language Processing Techniques for Automatic Phishing and Opinion Spam Detection,"In phishing, an attacker tries to steal sensitive information, e.g., bank/credit card account numbers, login information, etc., from Internet users. The US society and economy are increasingly dependent on the Internet and the web, which is plagued by phishing. One popular phishing method is to create a site that mimics a good site and then attract users to it via email, which is by far the most popular medium to entice unsuspecting users to the phishing site. Because of this modus operandi and the damages caused by phishing, it is important to design efficient and effective classifiers for emails and web sites. \n\nIn this project, new techniques, inspired from natural language processing methods, are being designed for phishing email and web site detection. They are then implemented and validated rigorously on realistic data sets. They are also applied to automatic detection of opinion spam. \n\nProposed research is expected to: (i) be useful in pushing the envelope of natural language processing techniques, and (ii) yield new applications of these techniques in cyber security. In the past, the PI has been very successful in involving women and minorities including underrepresented minorities in his research and this effort will be continued in this project. University of Houston has been recognized as a Hispanic-serving institution and the PI will continue his past successful efforts to involve underrepresented minorities including African Americans and Hispanics in this research. This research will be moved into the classroom and broadly disseminated through publications and software on the web.",1319212,3,sensitive information
Chunsheng Xin,,cxin@odu.edu,Old Dominion University Research Foundation,"SPECIAL PROJECTS - CISE, RES IN NETWORKING TECH & SYS",EAGER: A Beamforming Optimization Framework for Large Scale mmWave Networks,"In recent years we have witnessed remarkable proliferation of intelligent wireless devices. According to leading market research firms, there will be 50 to 100 billion wireless devices by 2025. At the same time, mobile-broadband services will continue driving the demand for higher consumer data rates. As a result, the wireless industry is anticipated to meet a 1000x growth challenge. The vast majority of today's wireless communications systems operate in the microwave spectrum below 3 GHz, which is experiencing severe spectrum shortage and has become a crowded and precious resource. The millimeter wave (mmWave) band, operating at frequencies between 20 and 300 GHz, is a promising candidate for next-generation wireless communications systems. The massive underutilized spectrum at the mmWave band provides great potential to support multiple gigabit-per-second user data rates and thousand-fold increase in total mobile broadband data rate. Beamforming is a fundamental enabling technology for wireless communications at the mmWave band. This project explores a framework for beamforming optimization in large scale mmWave networks. If successfully completed, this exploratory research is expected to advance and affect the design, deployment, and operation of future mmWave communication systems, and result in significant scientific, economical, and societal impacts. The project will broaden the participation of underrepresented groups, such as underrepresented minority and female students, in the state-of-the-art research, as well as promote STEM education through outreach activities such as summer camps.\n\nTo compensate the high path loss at the mmWave band, the beamforming technology concentrates energy in a sharp beam toward the receiver to achieve a high channel gain. However, the existing beamforming approaches are prohibitively costly, both technically and economically, and cannot be scaled to beamforming for large scale mmWave networks. This project explores a framework for beamforming optimization in large scale mmWave networks, with multiple base stations, a large number of mobile stations or users, and a vast, multi-dimensional solution space for various beam formation, user association, and beam scheduling configurations. The proposed research is exploratory and consists of four tasks: 1) Clustering and Resource Allocation, 2) Power Control, 3) Multi-base station Oriented Clustering, and 4) Beamforming and Interference. The four tasks are intertwined and multidisciplinary. The innovative optimization formulations, various heuristic algorithms and schemes based on multiple techniques such as set covering and bin packing will be explored, to form a comprehensive framework for user clustering, beam resource allocation, power control and beam scheduling in large scale mmWave networks. The optimization models, algorithms and schemes developed for the framework make it practical for beamforming optimization in large scale mmWave networks. This will significantly help to pave the roads for large scale mmWave networks. The techniques developed in this project will have significant impacts on a broad spectrum of applications, including 5G, mmWave WLAN, and public safety communication networks.",1745632,28,user data
Carleen Maitland,,cmaitland@ist.psu.edu,Pennsylvania State Univ University Park,"INFORMATION TECHNOLOGY RESEARC, Cyber-Human Systems (CHS)",RAPID: Socio-technical systems and Big Data Analytics in the Ebola Response,"In response to the 2014 Ebola virus outbreak in western Africa, dozens of humanitarian relief organizations as well as the CDC and U.S. military are providing medical assistance or logistics support to the relief effort. At the same time, a diverse range of volunteer technical communities (VTCs) and academics, as well as the humanitarian relief organizations themselves, are attempting to make use of ""big data"" to improve the response. These big data analyses are based on diverse data from diverse sources, including call records from mobile phone companies, health worker inventories from ministries, and daily case-data reports aggregated from multiple organizations. The analyses have generated outputs such as visually appealing maps and predictions of outbreak trajectories. However, precisely how, when and where these analyses can be used effectively by response organizations are still open questions. The project will develop knowledge to guide response organizations interested in leveraging existing and emerging big data from a variety of sources (response organizations, firms, government, individuals), which in turn may improve the speed, quality, and efficiency of crisis response. \n\nThe research team of computer and social scientists will partner with a consultant with expertise in crisis information management deployed in the Ebola response. They will examine both organizational and technical dimensions of the use of big data analytics in the Ebola response organizations, carrying out a series of interviews to investigate how and where data is used (field, headquarters, or both) and the work involved to make big data analyses usable in the decision making of response organizations. The results will inform the development of a socio-technical systems framework to explain what makes big data analyses useable. The social dimensions of the framework will include the response context as well as decision making processes. The technical dimensions will include data availability, data analyses and output formats. In the process of developing this socio-technical framework, this research will identify mechanisms for matching organizational needs with big data analyses. More importantly, however, by identifying these mechanisms, the research will shed light on the fundamental roles of multi-level governance and articulation work in making effective use of big data analyses. The multi-level approach helps explain and predict the location in an organization?s hierarchy of technical and decision making expertise. Within these levels, a focus on articulation work helps specify the necessary tasks for using data in a highly dynamic environment. The project will extend the scholarship in the crisis informatics sub-field of big data beyond its current focus on social media data and provide clarification of ""last mile"" issues in big data aimed at ensuring the usefulness of output.",1519023,20,social media data
Dieter Pfoser,,dpfoser@gmu.edu,George Mason University,"Algorithms in the Field, ALGORITHMIC FOUNDATIONS",AitF: Collaborative Research: Modeling movement on transportation networks using uncertain data,"In the current data-centered era, there are many highly diverse data sources that provide information about movement on transportation networks. Examples include GPS trajectories, social media data, and traffic flow measurements. Much of this movement data is challenging to utilize due to the inherent uncertainty caused by infrequent sampling and sparse coverage. The goal of this project is to develop a unified framework that uses as many available data sources as possible to extract meaningful traffic and movement information automatically from the data. Probabilistic network movement models will be developed that capture movement probabilities and traffic volume on a network over time. The results will impact a range of applications that rely on capturing population movements, such as urban planning, geomarketing, traffic management, and emergency management. Educational activities will be integrated throughout the project. Students will be closely involved in research and practical implementations, and will be trained in spatio-temporal data management, algorithms development, and (trajectory) data analysis. The combination of such skills is increasingly important in spatial data science. Topics involved in this project will enrich the course material and curriculum development at both institutions. \n\n The objective of this project is to create a unified framework for aggregating and analyzing diverse and uncertain movement data on road networks, with the aim to provide tools for querying and predicting traffic volume and movement. Probabilistic movement models on the network will be developed that can handle heterogeneous data sources, including GPS trajectories, geo-tagged social media data, bike-share data, public transport data, and traffic volume data. The diversity and spatio-temporal uncertainty of this data will be addressed with a Bayesian traffic pattern learning approach that first trains the movement models with the more certain data, which in turn will be used to fill gaps in the more uncertain data. The project will advance the state-of-the-art in theoretical communities (computational geometry, data mining) as well as in applied communities (spatial databases, location science). The results of the research will available on the project website (movementanalytics.org), and will be disseminated in prestigious venues through presentations and demonstrations at conferences, and through publications in journals.",1637541,1,social media data
Ranjit Jhala,,jhala@cs.ucsd.edu,University of California-San Diego,Secure &Trustworthy Cyberspace,TWC: Medium: Detection and Prevention of Data Timing Channels,"In the era of ""big data"" and ""cloud computing"", it is critically important that we are able to protect sensitive data, and prevent it from being leaked. The goal of this project is to explore a new kind of attack, called data timing attacks, which circumvents known defenses, and allows adversaries to exfiltrate private information from web browsers and privacy preserving databases.\n\nThis project investigates not only how prevalent data timing attacks are, but also how to prevent them. By developing techniques for understanding and mitigating data timing attacks, this project will provide better guarantees about the security of widely used software like web browsers, web servers, and databases, yielding safer and more secure systems for end-users. Finally, the project will elucidate what best programming practices should be taught to the next generation of programmers, so they can develop safe and secure software.",1514435,3,private information
Kathleen McKeown,,Kathy@cs.columbia.edu,Columbia University,Secure &Trustworthy Cyberspace,Cybersecurity Risk Conference,"In the past several years, cyber-crime and cyber-breaches have exploded, due to the increase in commerce conducted on the Internet, as well as increasing quantities of sensitive information being stored in databases that can be accessed by outside parties. In addition, cyber-criminals have advanced, not only employing technical acumen but also preying on human error by using sophisticated techniques like phishing. To address these challenges, the Northeast Big Data Innovation Hub (NEBDIH) will convene two workshops to bring together experts from academia, government, and industry in the fields of insurance, cybersecurity, data science, and policy. The ultimate goal is to develop a collaborative model (e.g., a cross sector consortium) that will help to define the frequency and economic/social severity related to successful attacks, and to begin to develop baselines for measuring and managing cyber risk more effectively. \n \nNEBDIH will seek to gather leading academic, government, risk management, general industry and healthcare participants, whose mission will be to identify, quantify, and to help mitigate risk associated with cyber-related criminal activity. The workshops will serve as a forum for these experts to collaborate on: 1) better understanding and defining the interrelated cybersecurity landscape across sectors, and 2) planning for the resources necessary to develop baselines for measuring and managing cybersecurity risk. Expected outcomes from these activities include developing a framework for filling the gaps in quantifying and managing cybersecurity risk across organizations and sectors. This may take the form of an initiative that captures the best of breed technologies, vendors, and best practices; captures a substantial portion of the data that is required, in aggregate, to make a cybersecurity risk assessment at multiple levels (by industry, by region, by company); provides the tools to make the data discoverable and interoperable; and provides a basic suite of tools to allow for high level analysis of the data within the context of cybersecurity risk.",1748395,3,sensitive information
Yaron Singer,,yaron@seas.harvard.edu,Harvard University,ALGORITHMIC FOUNDATIONS,CAREER: Algorithmic Foundations for Social Data,"In the past several years, there has been a great deal of exposure to the opportunities and promise that lie in large-scale data. Although massive data sets have been collected and analyzed in well over a decade, the excitement is largely due to the relatively recent availability of social data: massive digital records of human interactions. This provides a unique system-wide perspective of collective human behavior which poses fundamental challenges and opportunities. Despite the tremendous progress made in recent years, very few algorithmic frameworks to-date have been purposefully developed for analyzing social data sets. The goal of this project is to develop frameworks that enable analysis of large-scale social data. This project seeks novel models that are rich in problems, raise deep questions about computation, and can lead to long-lasting impact on sociology and data science.\n \nFrom a technical perspective, the goal of the project is to develop appropriate algorithmic machinery with strong theoretical guarantees that translate to results in practice. The project consists of three main lines of research. The first line of research seeks to develop a theory to optimize events in the future given a distribution on the consequences of actions we take in the present. The second line of research considers learnability and scalability of social data, and its interpretation for optimization. The third line of research considers design of robust optimization algorithms for noisy data. The methodology includes experimentation on real data sets to develop appropriate algorithmic machinery with strong theoretical guarantees that translate to results in practice. Both undergraduate and graduate curriculum will benefit from the development of courses in this interdisciplinary area.",1452961,27,social data
Rogers Hall,,rogers.hall@vanderbilt.edu,Vanderbilt University,Cyberlearn & Future Learn Tech,EAGER: Building Learning in Urban Extended Spaces,"In this Cyberlearning: Transforming Education-funded EAGER Project, the PIs are carrying out first steps in understanding how to use the archives of cities to draw inner-city youth into inquiry and expressive activities in the context of documenting the interesting history, geography, civics, and so on, of the places they live. The work is being carried out in Nashville, Tennessee. It is a collaborative effort of Vanderbilt University and city of Nashville, represented by The Country Music Hall of Fame and The Nashville Library System. The project uses what is known about how people learn to inform the design of technology and pedagogy in support of ""place-based education"" -- education that takes advantage of the place where learners live to promote learning and sustain their engagement. In this approach to place-based education, learners are taking on the curatorial practices of museum and library curators and using those practices to learn both the history of their city and also how to synthesize across information sources and express themselves in ways that are engaging and educational for others. New knowledge is being created about youth authoring and its potential for promoting learning and also about bringing the wealth of interesting city and cultural archives to the people who live in a city and its visitors.\n\nSocial media and ""curation"" applications have drawn the interest of many youth. The PIs in this project recognize potential in these technologies and in the interests of young people in curating collections for promoting learning of history, civics, geography, and other content and at the same time learning to synthesize across information sources and express themselves well. They envision a technical and socio-technical infrastructure for a new kind of informal after-school learning environment in which technology provides structure and aid as learners explore the archives of their city and compose compelling narratives to teach others, and social and interaction structures (the pedagogy) empower learners to engage in telling the stories of their city. The work is timely, as the tools that bring location-aware media together with mobile, personal information devices are becoming more capable and sophisticated at a rapid rate. The tool designs and pedagogy that are created through this project and follow-on projects will be applicable to supporting youth initiatives, both formal and informal, throughout our country and internationally.",1341882,5,personal information
Jeff Huang,,Jeff_Huang@brown.edu,Brown University,Cyber-Human Systems (CHS),CAREER: Modeling User Touch and Motion Behaviors for Adaptive Interfaces in Mobile Devices,"In this project the PI will investigate how to apply mobile interaction data to automatically improve the usability and accessibility of mobile user interfaces. The research will identify user abilities based on their behaviors, leading to mobile user interfaces that are more accessible to diverse user communities (e.g., veterans), in a variety of environments (including cases of situation-induced impairments). In particular, the PI will explore the challenges of data-driven adaptive interface layouts based on user behavior and visual attention in mobile computing when the user is actually mobile. The work will involve student researchers from under-represented groups currently advised by the PI, and will be evaluated by different populations engaged in realistic but varied activities. This will allow the PI to release software and research findings that enable people to design interfaces that properly adapt to the abilities of members of the target communities. The research ties directly into an educational plan to develop a student response tool for lecture-style user interface courses, that allows students to create wireframe interfaces, to design typefaces, and to draw visualizations during class on touchscreen and mobile devices, which can be displayed on the room screen for discussion and peer feedback. The tool will be iteratively refined in a user interface course with a diverse student population, and deployed in a course at the PI's institution outside of computer science as well as in user interface courses at other universities.\n\n User interfaces on mobile devices are not one-size-fits-all, nor should they be. Users' abilities may differ, or the situational context of the environment can introduce new challenges. By their very nature, mobile devices are used in many different environments and with different postures. For example, users may hold their tablet in both hands with the screen in landscape orientation to read in bed, swiping to different pages occasionally; at other times, they may be pushing a stroller while gripping their phone with one hand to navigate a map application. Because manufacturers know this, smartphones and tablets, unlike desktop computers, can accept touch input and sense both motion and orientation, and data from these interactions can be captured by websites and apps to identify specific user abilities and context. Over time, user interaction data collected at scale will enable personalization of the interface, say by reshaping touch targets to compensate for a user's habit of typically tapping to the right of a target, by relocating important buttons to more accessible locations on the screen, or by determining ideal text size by noting the zoom level a user often applies. Thus, the work will comprise three research objectives. The first objective is to investigate how to passively capture touch and motion data from mobile devices, to compute metrics representing user habits and mistakes as they perform touch interactions, and to determine the environmental context of the user from motion and touch behaviors. The second objective is to incorporate orientation and touch sensors to train an eye tracking model using the front-facing camera to detect the user's attention. The third objective, informed by findings from the first two, is to improve the usability and accessibility of existing interfaces, e.g., by adjusting the hittable area of targets, the text size, and interface layout.",1552663,20,"user interaction data, interaction data"
Rachel Greenstadt,,rachel.a.greenstadt@drexel.edu,Drexel University,"SPECIAL PROJECTS - CISE, Secure &Trustworthy Cyberspace",CAREER: Privacy Analytics for Users in a Big Data World,"Increasing amounts of data are being collected about users, and increasingly sophisticated analytics are being applied to this data for various purposes. Privacy analytics are machine learning and data mining algorithms applied by end-users to their data for the purpose of helping them manage both private information and their self-presentation. This research develops privacy analytics that help users answer three interconnected questions about their online persona (1) What data does the user consider sensitive, and in what contexts should one share it?; (2) What does the data say about the user; and (3) Who knows what? These privacy analytics introduce a novel, inverse data mining problem where users analyze their data to estimate the conclusions the data will produce when incorporated into larger data sets. This project designs new algorithms for quantitative and automated methods to detect privacy-related phenomena that have been observed qualitatively. These algorithms support the development of usable privacy enhancing technologies and will give users tools to cope with and manage their data in a complicated data environment. These tools will provide awareness to users about how their data is being used. These analytics will also help answer questions critical to the development of privacy law and policy.\n\nThis work involves approximately twenty-five undergraduates in research activities, exposing them to research methods and privacy issues. This project also develops novel educational materials including course offerings for an interdisciplinary master's program in security and educational tools for use by the general public to bridge the digital divide.\n",1253418,3,private information
Marco Gaboardi,,gaboardi@buffalo.edu,SUNY at Buffalo,Secure &Trustworthy Cyberspace,TWC: Large: Collaborative: Computing Over Distributed Sensitive Data,"Information about individuals is collected by a variety of organizations including government agencies, banks, hospitals, research institutions, and private companies. In many cases, sharing this data among organizations can bring benefits in social, scientific, business, and security domains, as the collected information is of similar nature, of about similar populations. However, much of this collected data is sensitive as it contains personal information, or information that could damage an organization's reputation or competitiveness. Sharing of data is hence often curbed for ethical, legal, or business reasons. \n\nThis project develops a collection of tools that will enable the benefits of data sharing without having the data owners share the data. The techniques developed respect principles of data ownership and privacy requirement, and draw on recent scientific developments in privacy, cryptography, machine learning, computational statistics, program verification, and system security. The tools developed in this project will contribute to the existing research and business infrastructure, and hence enable new ways to create value in information whose use would have been otherwise restricted. The project supports the development of new curricula material and train a new generation of researchers and citizens with the multidisciplinary perspectives required to address the complex issues surrounding data privacy.",1565365,3,personal information
Guru Prasadh Venkataramani,,guruv@gwu.edu,George Washington University,Secure &Trustworthy Cyberspace,NSF workshop on side and covert channels in computing systems,"Information leakage to malicious adversaries is a fast growing concern affecting computer users. Among the various forms of information leakage, side- and covert channels have particularly gained notorious attention. Side channels are information leakage channels where an adversary silently observes the activities of a victim process to infer its secrets. Covert channels are even more dangerous because a malicious insider trojan process intentionally colludes with the attacker to exfiltrate secrets. Such information leakage channels can exploit the abundance of shared hardware resources in current multi-core hardware, and result in compromising sensitive user information. A thorough understanding of the impact of such information leakage channels on computer system security and their consequences can immensely benefit our national cyber-security. \n\nThis project organizes a workshop to gather leading researchers and experts who have spent considerable effort and time in studying side and covert channels, as well as security researchers with strong experience in computer systems and their security needs. The workshop narrowly focuses on hardware side and covert channels, in order to create a unique forum for studying a cross-disciplinary challenge in a focused manner, namely the threat of side-channel and covert-channel leakage in computing, with the participation of different communities: computer architects, hardware designers, and cryptographers. The workshop participants identify potential improvements to existing computer design methodologies and strategies to minimizing and potentially neutralizing information leakage. A comprehensive workshop report then summarizes the findings and discussion on the current state of the art and projection in terms of detecting, mitigating and defending against side- and covert channels.",1747723,3,user information
Brian Levine,,brian@cs.umass.edu,University of Massachusetts Amherst,Secure &Trustworthy Cyberspace,EAGER: Privacy-Preserving Approaches to Proactive Forensics,"Insider attacks are a critical issue for companies and governments in scenarios involving trade secrets, sensitive information, intellectual property, personally identifiable information, classified documents, and more. Too many existing approaches for responding to these attacks rely on mechanisms that assume the recovery of locally stored, unencrypted data. These techniques fail on the growing number of devices that employ file system encryption and cloud storage. This project advances novel methods of offering to an attacker's system covert evidence of their attack that may remain after primary data and documents are encrypted or securely wiped. The data has precise meaning to investigators that is demonstrable in court and to other third parties. The data is obfuscated from interpretation by third parties without investigator assistance, and thus is privacy preserving. The long-range outcome of this project will be the enabling of research including: generalized methods of attack response when the computers involved are outside or partially outside the administrator's control, automated methods of discovering channels for offering evidence, and defenses against these techniques. Our research is an important stepping stone towards the broader topic of privacy-preserving, proactive investigation of attacks committed using networked computer systems.",1442069,3,sensitive information
Jingguo Wang,,jwang@uta.edu,University of Texas at Arlington,Secure &Trustworthy Cyberspace,SBE: Small: Collaborative: Modeling Insider Threat Behavior in Financial Institutions: Large Scale Data Analysis,"Insiders pose substantial threats to an organization, regardless of whether they act intentionally or accidentally. Because they usually possess elevated privileges and have skills, knowledge, resources, access and motives regarding internal systems and data, insiders can easily circumvent security countermeasures, steal valuable data, and cause damage. Perimeter and host-based countermeasures like firewalls, intrusion detection systems, and antivirus software are ineffective in preventing and detecting insider threats. Despite the availability of abundant anecdotal information regarding insider threats, research relying on field data to advance understanding of such threats is still lacking. This proposal presents a theoretically driven approach to investigate the risk of insider threat within financial institutions. It will utilize large scale field data from two financial institutions to provide comparison and improve the generalizability of results. \n\nIntellectual Merit: The proposed research will use criminology theories and extend them to the domain of insider threat. It will use both objective log data from the enterprise single sign-on (eSSO) systems and subjective data through surveys and focus groups to understand perceptual characteristics of applications as well as perceptions of employees regarding attractiveness of targets. Thus, this research will be among the first that takes both the technical and human aspects into consideration in investigating victimization risk and attack proneness associated with information assets within financial institutions. In essence, the proposed study will utilize multi methods and multi-source data to establish how information resources can be better protected from misuse and abuse of access privileges. The study will initiate a new perspective for analyzing existing behavioral log data to improve the practice of risk management, which may have a transformative impact in terms of mitigating risks from different user groups and informing interventions to deal with the insider threat problem. \n\nBroader Impact: This multi-disciplinary collaborative project will deepen understanding of insider threat behavior in the context of financial institutions. A PhD student will be funded at each university and the research will result in a few Masters' independent studies in this area as well. The findings of this proposal will be disseminated among the law enforcement task forces, as well as banking organizations. The channels to be employed include workshops with the local InfraGuard program in collaboration with the regional FBI office. The outcomes of the proposal will not only provide an applied understanding of insider threat, but also important implications for risk management applications. It is important to note that the President's Critical Infrastructure Protection Board identified the banking and finance sector as one of the critical infrastructures to be secured. This proposal will help in this regard by having an impact on public policy with respect to regulations for financial institutions. The potential reduction in financial crime as a result would have significant societal benefits.",1420758,3,log data
Keith Ross,,keithwross@nyu.edu,New York University,Secure &Trustworthy Cyberspace,TWC SBE: Small: Protecting the Online Privacy of Users of Social Networks,"It is generally recognized that protecting online privacy is important, with modern society manifesting this concern in many ways. Preliminary research indicates that third parties, with modest crawling and computational resources, and employing simple data mining heuristics, can potentially combine online services and publicly available information to create detailed profiles of the users living in any targeted geographical area.\n\nThis research investigates measures that can significantly improve privacy protection of users, while not degrading their overall Internet experience. The focus is on less-trustworthy third parties (e.g., data brokers, advertisers, spammers, malware distributors, and pedophiles), who can scrape, aggregate and infer information from many different online and offline sources. This research has two interrelated research thrusts. First, it explores to what extent third parties can collect, aggregate, and statistically process information from OSNs and other online and offline sources to create profiles. This thrust is developing rigorous statistical methodologies and probabilistic models for estimating the degree of potential privacy leakage. Second, this research investigates a variety of privacy policies that governments can establish, and a wide range of measures OSNs can take, to reduce the privacy risk. For promising combinations of policies and measures, this research quantifies the trade-off between privacy protection and usability.",1318659,3,scrape
Sergey Levine,,sergey.levine@gmail.com,University of California-Berkeley,ROBUST INTELLIGENCE,CAREER: Deep Robotic Learning with Large Datasets: Toward Simple and Reliable Lifelong Learning Frameworks,"Learning robot behaviors from large data sets is an important way to make robots more capable and reliable. This project will develop algorithms for autonomous robotic skill learning that can easily be used by novice hobbyists with low-cost robots. If deployed widely, such an approach could be used to gather a large number of robotic motions, which can be combined to improve the robot's skills. Availability of large datasets has proven critical in machine learning application areas, from computer vision to speech recognition, and the ability to collect a large amount of robotic interaction data would substantially increase the capabilities of learning-based robotic systems. Since the approach will be designed for untrained users, it also doubles as an effective tool for robotics education.\n\nDeep learning has emerged as a powerful technique for taming the complexity of the real world. The success of deep learning depends on the availability of large datasets, which traditionally have been difficult to obtain for robotic learning. This project will focus on deep learning algorithms that can be used for effective and reliable robotic skill learning, generating intelligent actions directly from raw sensory input, with an eye towards enabling widespread deployment for large-scale data collection. To that end, the proposed research will aim to: (1) devise reliable and robust real-world robotic learning algorithms that can collect experience without human oversight or intervention; (2) build algorithms centered around transfer learning, whereby experience from prior tasks can be used to inform dramatically faster learning of new skills with potentially different robotic platforms; and (3) devise algorithms that can effectively control heterogeneous, low-cost, imprecise robots, so as to facilitate widespread deployment and the project's educational mission.",1651843,18,interaction data
Douglas Turnbull,,dturnbull@ithaca.edu,Ithaca College,ROBUST INTELLIGENCE,RI: Small: Collaborative Research: RUI: Batch Learning from Logged Bandit Feedback,"Log data is one of the most ubiquitous forms of data available, as it can be recorded from a variety of systems (e.g., search engines, recommender systems, ad placement platforms) at little cost. Making huge amounts of log data accessible to learning algorithms provides the potential to acquire knowledge at unprecedented scale. Furthermore, the ability to learn from log data can enable effective machine learning even in systems where manual labeling of training data is not economically viable. Log data, however, provides only partial information -- ""contextual-bandit feedback"" -- limited to the particular actions taken by the system. The feedback for all the other actions the system could have taken is typically not known. This makes learning from log data fundamentally different from traditional supervised learning, where ""correct"" predictions together with a loss function provide full-information feedback.\n\nThis project tackles the problem of Batch Learning from Bandit Feedback (BLBF) by developing principled learning methods and algorithms that can be trained with logs containing contextual-bandit feedback. First, the project develops the learning theory of BLBF, especially with respect to understanding the use and design of counterfactual risk estimators for BLBF. Second, the project derives new learning methods for BLBF. Past work has already demonstrated that Conditional Random Fields can be trained in the BLBF setting, and the project derives BLBF analogs of other learning methods as well. Third, the project derives scalable training algorithms for these BLBF methods to enable large-scale applications. And, finally, the project validates the methods with real-world data from operational systems.",1615679,18,log data
Nihar Shah,,nihars@andrew.cmu.edu,Carnegie-Mellon University,COMM & INFORMATION FOUNDATIONS,CRII: CIF: Crowdsourcing-aware Learning,"Machine learning has significantly advanced the state of the art in a variety of applications. These successes have required massive labeled datasets for training machine learning algorithms. The collection of these labeled datasets usually involves human annotation. For instance, the training labels for supervised learning algorithms are often obtained through ""crowdsourcing"" where people label the data over the Internet in exchange for monetary incentives. Most learning algorithms, however, are agnostic of this human-labeling process. This project designs improved learning algorithms by incorporating the ""human"" aspect of the data collection process in the machine learning objective.\n\nIn more detail, this project considers supervised binary classification tasks where the labels for the training data are obtained from people. The research involves design of learning algorithms that jointly consider the human collection process -- including the interfaces and incentives available to the human labelers -- and the overall learning objective. Theoretical guarantees of optimality are derived and compared with guarantees for algorithms which are agnostic of the human component. The algorithms and guarantees are based on models of human behavior from psychology, such as permutation-based models, that allow for maximal accuracy while making minimal assumptions on how the human labelers behave. The theoretical results are corroborated with practical implementations (open sourced) and real-world experiments (data freely available online).\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",1755656,26,freely available online
Thorsten Joachims,,tj@cs.cornell.edu,Cornell University,INFO INTEGRATION & INFORMATICS,III: Medium: Machine Learning with Humans in the Loop,"Machine learning is increasingly used in systems common in daily life -- from search engines to online education to smart homes. Through interactions with their users, these systems learn about the world to improve efficiency and performance. Building effective and robust Human-Interactive Learning (HIL) systems, however, requires a framework that simultaneously integrates models of human behavior with the design of machine learning algorithms, because user data is only an indirect mapping, mediated through the human decision-making process, of the knowledge the system aims to elicit. This project takes an interdisciplinary approach to developing effective techniques to design human-interactive learning systems.\n\nThis project explores how humans provide data and how learning algorithms use this data in an integrated framework that encompasses three aspects. First, the project develops models of the user decision process, formalizing how observable user actions map to the underlying knowledge the system aims to acquire. Second, these models inform the design of the interface that connects the user and the learning algorithm to suitably trade off the quantity and quality of the data acquired. Third, the user model and interface motivate new machine learning settings and algorithms to maximize learning efficiency. By developing an integrated framework for the three interconnected components for building Human Interactive Learning Systems -- human decision models, information-elicitation interfaces, and learning algorithms -- this project will impact future designs of widely-used systems such as non-web information search, recommendation, and online education.",1513692,19,user data
Zeev Dvir,,zdvir@cs.princeton.edu,Princeton University,"ALGORITHMIC FOUNDATIONS, COMM & INFORMATION FOUNDATIONS, Secure &Trustworthy Cyberspace",AF: Small: New Techniques for Private Information Retrieval and Locally Decodable Codes,"Maintaining user privacy in a computerized world is a difficult challenge. Private Information Retrieval (PIR) schemes allow a user to retrieve a piece of information replicated among several servers without disclosing the precise nature of that information to each individual server. The aim of this project is to develop new techniques for constructing such protocols with improved efficiency. In a recent breakthrough, the PI introduced a new technique that dramatically reduced the cost of the best PIR protocols. The main goals of the projects are to develop this technique further and to understand better the underlying mathematics that make it work. Pursuing the research goals of the project will require integration of research and education and training of gradate and undergraduate students. The PI will also contribute to education of high school students, including those from under-represented groups, by participating in a computer science summer school program. \n\nPIR protocols guarantee information theoretic privacy in the setting where the same database is replicated among several non-communicating servers. The most interesting case is that of two servers (the smallest possible). The current state of the art (obtained in a recent work by the PI) gives sub-polynomial communication cost, improving the two-decades long record requiring communication proportional to the cube root of the database size. The improvement comes from leveraging the connection between PIR and Locally Decodable Codes (LDCs) which are error correcting codes that allow for quick correction of a single codeword position by querying the code in only a few places. The main goals of this project are to further study these two objects (PIR and LDCs) and to improve their known constructions. On the other hand, the project will aim to understand the limitation of PIR and LDCs in the form of provable lower bounds on their efficiency.",1523816,3,private information
Guofei Gu,,guofei@cse.tamu.edu,Texas A&M Engineering Experiment Station,Secure &Trustworthy Cyberspace,NETS: Small: Exploiting Social Communication Channels Against Cyber Criminals,"Malware, especially botnets, have become the main source of most attacks and malicious activities on Internet. Bots communicate with each other and Command & Control servers to coordinate their malicious activities. This project is developing new techniques and tools to detect malicious activities and botnets through analyzing their communication channels. This project plans to investigate mechanisms for detecting these communication channels through several novel mechanisms: (i) through a graph analysis of social contacts, (ii) analyzing graph properties of communication to decipher Peer to peer communication properties of bots and (iii) machine learning based approaches to analyzing network traffic. The automation required to propagate malicious contents and malware will result in different behavior than human behavior in these communication channels. We expect the generated names, content, the time of contacts and communication, the social graph structures to be sufficiently different to enable us to develop techniques for detection of malicious entities. Our work focuses on developing robust techniques that are hard to evade or limit the botnet functions when they try to evade the detection mechanisms. Developed analysis tools will help in detecting botnets in networks and malicious entities in social networks. Educational impact will include training graduate and undergraduate students with valuable research skills while advancing the state of the art in network security and traffic analysis, contributing to the technology workforce. We will publish our results and enable technology transfer to industry.",1218929,30,social graph
Neelesh Patankar,,n-patankar@northwestern.edu,Northwestern University,"OFFICE OF MULTIDISCIPLINARY AC, Software Institutes, CDS&E-MSS, CDS&E","SI2-SSI: Collaborative Research: Scalable Infrastructure for Enabling Multiscale and Multiphysics Applications in Fluid Dynamics, Solid Mechanics, and Fluid-Structure Interaction","Many biological and biomedical systems involve the interaction of a flexible structure and a fluid. These systems range from the writhing and coiling of DNA, to the beating and pumping of cilia and flagella, to the flow of blood in the body, to the locomotion of fish, insects, and birds. This project aims to develop advanced software infrastructure for performing dynamic computer simulations of such biological and biomedical systems. To facilitate the deployment of this software in a range of scientific and engineering applications, this project will develop new software capabilities in concert with new computer models that use the software. Specific application domains to be advanced in this project include models of aquatic locomotion that can be used to understand the neural control of movement and ultimately to develop new treatments for neurological pathologies such as spinal cord injuries, and models that simulate the interaction between the electrophysiology of the heart and the contractions of the heart that pump blood throughout the body, which could lead to improved approaches to treating heart disease. The software to be developed within the project is freely available online and is used by a number of independent research groups in a variety of scientific and engineering domains. It is being actively used in projects that model different aspects of cardiovascular dynamics, such as platelet aggregation and the dynamics of natural and prosthetic heart valves, and in projects that study other biological problems, including cancer dynamics, insect flight, aquatic locomotion, and the dynamics of phytoplankton. The software is also being applied to non-biological problems, including nanoscale models of colloidal suspensions and models of active particles. The improved methods and software to be developed in this project will thereby have a broad and sustained impact on a large number of ongoing research efforts in the biological and biomedical sciences and other scientific and engineering disciplines.\n\nThe immersed boundary (IB) method is a broadly applicable framework for modeling and simulating fluid-structure interaction (FSI). The IB method was introduced to model the fluid dynamics of heart valves, and subsequent development initially focused on simulating cardiac fluid dynamics. This methodology is broadly useful, however, and has been applied to a variety of problems in which a fluid flow interacts with immersed structures, including elastic bodies, bodies with known or prescribed deformational kinematics, and rigid bodies. Extensions of the IB method have also been developed to model electrophysiological systems and systems with chemically active structures. To improve the efficiency of the IB method, the PI has developed adaptive versions of the IB method that employ structured adaptive mesh refinement (AMR) to deploy high spatial resolution only where needed. These methods have been implemented within the IBAMR software framework, which provides parallel implementations of the IB method and its extensions that leverage high-quality computational libraries including SAMRAI, PETSc, and libMesh. This project will further extend the IBAMR software by implementing modeling and discretization technologies required by the research applications of current and prospective users of the software, by developing improved solver infrastructure facilitated by the implementation of native support for structured AMR discretizations in the PETSc library, and by integrating with existing high-quality software tools for model development, deployment, and analysis. IBAMR is freely distributed online and is used within a number of independent research groups both to the further development of the IB method and also to its application to simulate diverse problems in fluid dynamics and FSI. By enhancing IBAMR, this project will also enhance the ability of these and other researchers to construct detailed models without requiring those researchers to develop the significant software infrastructure needed to perform such simulations. This project will also develop general-purpose support for AMR discretizations in PETSc, a software library with thousands of active users, ~400 downloads per month, and numerous applications. The work of this project will help to grow the IBAMR user community of students and researchers by developing UI tools for building models, running simulations, and analyzing results. Students will be actively engaged in all aspects of the project, including code, method, and model development.",1450374,8,freely available online
Kevin Xu,,Kevin.Xu@utoledo.edu,University of Toledo,INFO INTEGRATION & INFORMATICS,CRII: III: Generative Models for Robust Real-Time Analysis of Complex Dynamic Networks,"Many complex systems in the computer, information, biological, and social sciences can be represented as networks with nodes denoting objects and edges denoting relationships between the objects. Such complex network structures often change continuously over time through the observation of events at irregular times, such as networks of social interactions between people via messages, networks of transactions between organizations, and networks of face-to-face interactions between people. This project formulates a range of models of varying complexity for continuously evolving networks to enable robust real-time analysis of these networks in a variety of application settings. Such dynamic network models could be used in many scientific disciplines and in public health applications, including modeling the spread of airborne viruses between people. The project trains new graduate and undergraduate students, including female students from the University of Toledo's ACM-W chapter, in practical data science research involving a variety of data types and sources. The project also results in the development of an open-source Python software package, DyNetworkX, for analyzing dynamic networks along with educational materials on dynamic networks through a series of lectures and hands-on tutorials using the DyNetworkX package.\n\nThis project aims to create a range of probabilistic generative models for continuous-time event-based networks that are flexible enough to account for the types of complex structures seen in real network data, including node popularity, community structure, reciprocity, and transitivity. The project also seeks to develop efficient incremental inference algorithms and discrete-time approximations that allow for real-time analysis of extremely large social networks that are rapidly changing over time, such as those seen in online social network data. The proposed range of models allows an analyst to trade off flexibility and scalability depending on the needs of a particular application. Two main applications are targeted: prediction of the spread of infectious disease over networks of physical proximity and real-time summarization and prediction of online social network activity. Deliverable assets of the project include new probabilistic models and inference algorithms, the DyNetworkX open-source software package, and educational materials on dynamic networks. These are intended to benefit researchers and educators in the computer and information sciences as well as researchers in other fields such as the social and economic sciences, software engineers, and hobbyists who work with dynamic network data.\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",1755824,19,social network data
Peter Pirolli,,ppirolli@ihmc.us,"Florida Institute for Human and Machine Cognition, Inc.",Smart and Connected Health,SCH: INT: Collaborative Research: FITTLE+: Theory and Models for Smartphone Ecological Momentary Intervention,"Many health conditions are caused by unhealthy lifestyles and can be improved by behavior change. Traditional behavior-change methods (e.g., weight-loss clinics; personal trainers) have bottlenecks in providing expert personalized day-to-day support to large populations for long periods. There is a pressing need to extend the reach and intensity of existing successful health behavior change approaches in areas such as diet and fitness. Smartphone platforms provide an excellent opportunity for projecting maximally effective interventions for behavior change into everyday life at great economies of scale. Smartphones also provide an excellent opportunity for collecting rich, fine-grained data necessary for understanding and predicting behavior-change dynamics in people going about their everyday lives. The challenge posed by these opportunities for detailed measurement and intervention is that current theory is not equally fine-grained and predictive. \n\nThis interdisciplinary project investigates theory and methods to support fine-grained behavior-change modeling and intervention integrated via smartphone into the daily lives of individuals and groups. Fittle+ develops a new and transformative form of smartphone-delivered Ecological Momentary Intervention (EMI) for improving diet and physical activity. This approach will provide social support and autonomously planned and personalized coaching that builds on methods from mobile sensing, cognitive tutoring, and evidence-based social design. The foundation for this new approach will require new predictive computational theories of health behavior change. Current coarse-grained conceptual theories of individual health behavior change will be refined into fine-grained predictive computational models. These computational models will be capable of tracking moment-by-moment human context, activity, and social patterns based on mobile sensing and interaction data. Using these monitoring capabilities, Fittle+'s computational models will support assessment of, and predictions about, individual users and groups based on underlying motivational, cognitive, and social mechanisms. These predictive models will also be used to plan and optimize coaching actions including detailed diagnostics, individualized goals, and contextually and personally adapted interventions. \n\nThe collaborative team of researchers works with weight-loss interventionists at one of nation's largest health organization's facility in Hawaii. The team includes expertise in mobile sensing, artificial intelligence, computational cognition, social psychology, human computer interaction, computer tutoring, and measurement theory.",1757520,7,interaction data
Gabriel Ghinita,,gabriel.ghinita@umb.edu,University of Massachusetts Boston,Secure &Trustworthy Cyberspace,"SaTC:EDU: Capacity Building in Security, Privacy and Trust for Geospatial Applications","Many mobile devices with GPS-positioning capabilities allow users to retrieve and share their geographical coordinates and such geospatial data is critical in many areas including traffic optimization, emergency response, disaster rescue missions or military intelligence. At the same time, there are serious concerns related to the security, privacy and trustworthiness (SPT) of the data since users disclose their coordinates and an adversary could derive sensitive information violating users' privacy, or could change the data leading, for example, to wrong conclusions in a crime investigation. SPT for geospatial data has been a subject of intensive research and its results have been gradually transferred into market products. However, there has been no transition to education and this project intends to build capacity for geospatial SPT education by developing six course modules. They will cover geospatial SPT from several different perspectives, with focus on data management, networking, data mining, and social issues. The modules can be integrated into upper-level high-school, community college, or university courses.\n\nThe dissemination plan includes workshops, seminars and other outreach activities at high schools, community colleges and universities to increase awareness on geospatial SPT issues and to disseminate project's resources to a broad audience. It is expected that the proposed course modules will be also used in other fields such as law, economics, sociology and other disciplines where mobile technology introduces new opportunities but also considerable challenges. The project intends to reach students from underrepresented groups through collaboration with institutions in the Pathways to Prosperity network.",1523101,3,sensitive information
Lin Zhong,,LZhong@rice.edu,William Marsh Rice University,Secure &Trustworthy Cyberspace,SaTC: CORE: Medium: Collaborative: Defending against Compromise and Manipulation of Mobile Communities,"Many of today's mobile services build mobile communities of users who share their valuable experiences and data. Examples include traffic incidents (Waze), restaurant reviews (Yelp, FourSquare), anonymous social networks (Whisper, Yik Yak), and even dating (Tinder, Bumble). Unfortunately, new threats can compromise and manipulate these communities, using lightweight software to mimic mobile devices. The researchers have shown how attackers can eavesdrop on mobile network traffic, learn their patterns, and write software to emulate mobile devices running the application. This amplifies existing attacks by multiple orders of magnitude, and allows attackers with limited resources to overwhelm mobile communities using millions of emulated devices under their control. These devices are difficult to detect, and completely cripple entire mobile systems, or manipulate them for the attacker's personal gain. Preliminary work showed that such an attack on Waze, Google's crowdsourced traffic navigation application, enabled fine grained GPS-level tracking of large user populations. Similar vulnerabilities apply to Yelp, Tinder, Uber, and others, with consequences ranging from manipulating/censoring content, theft of monetary incentives, to completely crippling the service.\n\nThe work described in this proposal seeks a better understanding of the threat of software-emulated devices to mobile communities, and explores systematic defenses against them. The researchers will develop defenses that detect large-scale attacks and limit their impact to that of single misbehaving devices, using three orthogonal approaches: a) a centralized infrastructure-based solution, that uses hidden patterns in aggregate user data to authenticate mobile devices; b) hardware solutions that extracts device-specific sensor data using platform APIs, and compares them to known models of hardware data models; and c) application-level solutions that use unsupervised learning to automatically detect similarity clusters in devices, based on analysis of user behavior (clickstreams) and physical mobility trace.",1701374,3,user data
Feifei Li,,lifeifei@cs.utah.edu,University of Utah,Secure &Trustworthy Cyberspace,TWC: Medium: Collaborative: Seal: Secure Engine for AnaLytics - From Secure Similarity Search to Secure Data Analytics,"Many organizations and individuals rely on the cloud to store their data and process their analytical queries. But such data may contain sensitive information. Not only do users want to conceal their data on a cloud, they may also want to hide analytical queries over their data, results of such queries, and data access patterns from a cloud service provider (that may be compromised either from within or by a third party).\n\nThis research designs and implements SEAL, a Secure Engine for AnaLytics over large data on a cloud. SEAL encrypts data using secure encryption schemes, but supports analytical operations through a new approach of building a meta-answer database. SEAL is developed within a security framework that allows specifying different levels of desired security. The design of SEAL in particular explores the tradeoff between security and efficiency, providing solutions with different provable security and efficiency features for a wide variety of analytical operations. Users are able to continue to enjoy the benefits a cloud has to offer, but now without the worry of losing sensitive information and with control over the security and efficiency tradeoff.",1514520,3,sensitive information
John Owens,,jowens@ece.ucdavis.edu,University of California-Davis,Software Institutes,SI2-SSE: Gunrock: High-Performance GPU Graph Analytics,"Many sets of data can be represented as ""graphs"". Graphs express relationships between entities, and those entities and relationships can be used to solve problems of interest in many fields. For instance, a social graph (like Facebook's) links people (entities) by friendships (relationships), and with that graph, Facebook can suggest people to you who might be your friends. Amazon might use a graph made of people and items for sale (entities) connected by who bought those items (relationships) to suggest items you might want to buy. A credit card company might look at your pattern of purchases and detect possible fraud even before you know your credit card was stolen. Graphs are also useful in many fields of science, such as genomics, epidemiology, and economics. This project uses an emerging programmable processor, the graphics processor (GPU), to solve graph problems. GPUs are rapidly moving into our nation's largest data centers and supercomputers. The project team is building a system for computation on graphs that will significantly improve performance on these problems. In this project, the team will work with the computing community and the scientific community, both of whom have numerous interesting, challenging graph computation problems that this system will target. The system is open-source software and can be used freely by researchers and industry all over the world.\n\n\nThis project, supported by the Office of Advanced Cyberinfrastructure seeks to develop the ""Gunrock"" programmable, high-performance, open-source graph analytics library for graphics processors (GPUs) from a working prototype to a robust, sustainable, open-source component of the GPU computing ecosystem. Gunrock's strengths are its programming model and highly optimized implementation. With this work the project team hopes to address Gunrock's usability in the computing and scientific communities by improving Gunrock's scalability, capabilities, core operators, and supported graph computations. In this work the team will collaborate with the GPU Open Analytics Initiative and the NSF-sponsored CINET project for network science to ensure that our work has the broadest possible impact.\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",1740333,8,social graph
Jugal Garg,,jugal@illinois.edu,University of Illinois at Urbana-Champaign,ALGORITHMIC FOUNDATIONS,CRII: AF: Strongly Polynomial Algorithms for Market Equilibria with Applications to Network Flows and Nash Social Welfare,"Market equilibrium is a central solution concept in economics with applications in a variety of domains. It has found several surprising applications even in non-market settings which do not involve any exchange of money but only require the remarkable fairness and efficiency properties of equilibria. These applications include scheduling, mechanism design, fair division, and others. Designing fast algorithms for computing equilibria in markets is vital for their many applications.\n\nFor a number of classical market settings with divisible goods, computing an equilibrium reduces to maximizing the geometric mean of agents' utilities, called the Nash social welfare. Nash social welfare is also considered an important fairness notion in allocating scarce resources in a non-market setting. Recently there has been a surge of interest in finding such an allocation with indivisible goods. However, the problem is NP-hard even when there are only two agents with additive valuations, and hence designing fast and near-optimal approximation algorithms is a crucial problem in this domain.\n\nThe PI aims to obtain fast algorithms for computing equilibria in fundamental market models and their extensions, and for the Nash welfare problem. Many of these reduce to network flow problems, which are central optimization problems that arise in numerous applications such as transportation and communication. Despite the extensive work on equilibrium computation and network flows, designing a strongly polynomial time algorithm for many of these market models has been long open. The first main thrust of this project is to make progress towards settling these open questions. New algorithmic techniques and tools, as well as structural insights will need to be developed to solve them. Recent work on the Nash welfare problem crucially uses the market model extensions and their connections with the flow problems. The second main thrust of this project is to obtain improved approximation algorithms for the Nash welfare problem.\n\nThe project will support and engage two PhD students, and an undergraduate student to implement the designed algorithms and test them for their practical efficiency. The new algorithmic techniques developed in this project will be incorporated into a graduate course on Games and Markets that the PI teaches. All the educational and implementation material will be made freely available online.",1755619,27,freely available online
Aron Culotta,,culotta@cs.iit.edu,Illinois Institute of Technology,INFO INTEGRATION & INFORMATICS,III: Small: Quantifying Multifaceted Perception Dynamics in Online Social Networks,"Measuring public perceptions and how they change over time is a central problem in marketing, public health, and politics. Traditional measurement methods rely on surveys and focus groups, which can be costly and time-consuming. Online social networks offer an attractive alternative: real-time perceptions can be estimated from public, online activity and compared with an entity's communications to quantify how public messaging affects perception. While prior algorithmic approaches rely purely on text-based sentiment analysis, this project will develop novel methods based on the insight that an entity's online social connections are indicative of how they are perceived (e.g., ""birds of a feather flock together""). Thus, rather than typical one-dimensional measures of sentiment, the project will instead investigate public perception with respect to multiple characteristics of an entity (e.g., is it seen as pro-environment, pro-health, etc.). A multi-faceted evaluation will be performed to study the phenomenon of ""greenwashing,"" a deceptive marketing practice in which firms market their products or policies as more environmentally friendly than they truly are. This project has the potential to enhance consumer protection by exposing deceptive marketing practices.\n\nThe project will develop social network analysis algorithms to assess perception of an entity and also language processing algorithms to quantify the communications of an entity with respect to a perceptual attribute. The approaches to both problems rely on innovative algorithms to measure the strengths of the social and linguistic relations between public entities and exemplar accounts that typify the perceptual attribute of interest. A key advantage of the approach is its minimal requirement of human input, e.g., given only a single keyword like ""environment,"" the approach identifies suitable exemplars and fits linguistic and perceptual models. The project will develop novel machine learning methods for domain adaptation, positive-unlabeled learning, and learning from label proportions in order to fit such models and ensure they are robust to omitted variable bias. The models will be evaluated using public Twitter and Facebook data to quantify the relationship between the perceptions and online communications of brands and other public entities, with a particular focus on identifying cases of greenwashing.",1618244,19,facebook data
Donald Truxillo,,truxillod@pdx.edu,Portland State University,Secure &Trustworthy Cyberspace,EAGER: Exploring Job Applicant Privacy Concerns,"Millions of people in the U.S. and worldwide apply for jobs online, and the use of online job application systems is growing. Thus, online job applicants are an important population to study. However, few studies have examined job applicants' concerns about their privacy and how to protect it. Further, job applicants' privacy concerns may affect how willing they are to apply for jobs and even whether job applicants pursue legal action against employers for privacy violations. This research focuses on job applicants' concerns about the privacy of information they provide during the job application process. Specifically, this research examines which privacy issues applicants are most concerned with and how to ease those concerns. The researchers also examine how well job applicants understand explanations surrounding the technologies that employers use to maintain the privacy of applicants' personal information. By understanding how security issues can be explained effectively to users, the research will provide recommendations for the development of security practices in the online job application process and how to explain them to applicants. Insights resulting from this research will also be applicable in the broader (and constantly expanding) arenas where online privacy is a factor.\n\nIn Phase 1, the researchers focus on the intersection between computer science and the social sciences, utilizing an archival dataset of 77,443 online job applicants for entry-level retail jobs. The data include the attitudes of these job applicants, such as concerns about the privacy of their data and attitudes toward the employer, as the applicants move through the stages of an online job application from the initial application to its resolution, i.e., when job applicants have received the hiring decision. In Phase 2, the researchers conduct an online job application simulation using an experimental design to examine conclusions from Phase 1. This entails an online study of approximately 4,000 Internet users who will be asked questions typically asked on a job application. Participants are randomly assigned to conditions that provide different explanations for the technical mechanisms-such as encryption-that are used to secure their information and how the organization will use applicants' data. Participants then complete measures such as fairness of the process, employer attractiveness, self-perceptions, how much they want the job, and privacy concerns. The two studies will guide future research on how to explain privacy issues to job applicants so that applicants and employers may better protect privacy.",1544535,3,personal information
Xusheng Xiao,,xusheng.xiao@case.edu,Case Western Reserve University,CRII CISE Research Initiation,CRII: SaTC: Enhancing Mobile App Security by Detecting Icon-Behavior Contradiction,"Mobile applications (i.e., apps) are becoming critical parts in our daily life. While these apps provide better customized services using users' personal data, certain behavior of the apps is less than desirable or harmful. For example, if an app's user interface (UI) has no texts or images to indicate that it will access users' personal data (e.g., GPS data), but the app discloses users' personal data when an action is performed (e.g., pressing a button), then red flags should be raised. Thus, it is crucial to understand the intents of the app to determine whether the app will perform within the user's expectation. Various research efforts have been dedicated to understand apps' intents via analyzing the semantics of texts in UI. However, images, especially icons, remain unexplored. In apps' UIs, icons are often used in interactive widgets (e.g., buttons) to express the intents to use sensitive data. It is often difficult to analyze the semantics of icons due to the varieties in image styles and the lack of descriptive texts.\n\nThe proposed research will build a knowledge base of icons' semantics via collecting icons from apps in major smartphone markets, and develop a framework to infer the semantics of icons based on the collected icons. More specifically, the PI proposes to adapt computer vision techniques to develop icon recognition techniques that identify similar icons based on the collected icons, and leverage program analysis techniques to check the compatibility between the icons and the program behaviors. Furthermore, this research will combine the semantics of both texts and icons to better detect undesired behavior in apps. The proposed research in understanding apps' intents improves mobile app security, which will have tremendous economical impact on society due to our increasing reliance on mobile apps. The proposed techniques will also benefit the security analysis of other event-driven GUI software applications, such as desktop applications, wearable apps, and web apps.\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",1755772,25,gps data
Rui Zhang,,ruizhang@udel.edu,University of Delaware,"Cyber-Human Systems (CHS), Secure &Trustworthy Cyberspace",TWC: Small: Collaborative: Secure and Usable Mobile Authentication for People with Visual Impairment,"Mobile authentication is necessary for preventing unauthorized access to mobile devices with increasingly more private information. Despite significant progress in mobile authentication for sighted people, secure and usable mobile authentication for people with visual impairment remains largely under-explored. This project is to develop, prototype and evaluate novel secure and usable mobile authentication techniques for people with visual impairment. \n\nThere are three research thrusts. The first thrust is to develop CurveAuth, which authenticates a user based on his/her finger-drawing curves on the device screen. The second thrust is to develop TapAuth, which authenticates a user based on his/her sequence of rhythmic finger taps or slides on the device screen. The third thrust is to develop ShakeAuth, which authenticates a user based on his/her sequence of rhythmic shakes of the device. This research also includes a plan to prototype and evaluates the proposed techniques through comprehensive user experiments. \n\nSuccessful development of the proposed techniques will have profound impact on allowing the visually impaired to fully embrace the power of modern mobile devices for improving their quality of life. Project outcomes will be made publicly available online in the forms of talks, publications, datasets, and Android/iOS apps.",1700032,3,private information
Katherine Shilton,,kshilton@umd.edu,University of Maryland College Park,Secure &Trustworthy Cyberspace,CAREER: Finding Levers for Privacy and Security by Design in Mobile Development,"Mobile data are one of the fastest emerging forms of personal data. Ensuring the privacy and security of these data are critical challenges for the mobile device ecosystem. Mobile applications are easy to build and distribute, and can collect a large variety of sensitive personal data. Current approaches to protecting this data rely on security and privacy by design: encouraging developers to proactively implement security and privacy features to protect sensitive data. Although there are many technical innovations available to help developers protect user data, adoption of these innovations is low. Reasons for low adoption range from a lack of training in privacy or security design to the fact that privacy-enhancing features and best-practice data security measures are often expensive to implement, or even counter to business models that require user profiling or monitoring. Low adoption of privacy and security protection mechanisms is a social problem that inhibits a secure and trustworthy mobile ecosystem. It is unknown what factors can motivate developers to implement privacy or data security features when faced with disincentives such as longer development timelines, markets for personal data, and tensions between data protection and data-enabled services. Understanding developer decision-making is central to addressing this problem: the decisions made by developers are fundamental to enabling privacy and security by design to succeed. \n\nThis project 1) studies developers to discover work practices that encourage privacy and data security by design; and 2) builds tools to encourage such work practices. This project uses surveys and field experiments to determine factors that motivate privacy and security by design. It develops and test evidence-based toolkits for mobile developers to improve privacy and data security in the mobile data ecosystem. The project asks the following research questions: \n1. How do mobile application developers define privacy and security by design?\n2. What practices in mobile application development encourage developers to prioritize data protection? \n3. How can development tools encourage developers to prioritize data protection during design?\n\nBy answering these questions, the project illuminates development culture, illustrates how developers understand privacy and security needs, and discovers practices that prioritize privacy and security. The project explains the impact of development practices on privacy and data security outcomes, advancing knowledge in software engineering and secure and trustworthy computing. Findings and products from this project support the rapidly developing mobile technology sector in enabling privacy and data security by design. Finding development practices that encourage privacy and data security by design improves technology transfer of technical innovations from the secure and trustworthy cyberspace research community, and bolsters protections for this sensitive data.",1452854,3,user data
David Wagner,,daw@cs.berkeley.edu,University of California-Berkeley,Secure &Trustworthy Cyberspace,TWC: Small: A Choice Architecture for Mobile Privacy and Security,"Mobile devices (e.g., smartphones and tablets) allow users to execute rich third-party applications that are capable of making extensive use of device hardware and personal data. This poses security risks, as applications may perform undesirable operations such as deleting data, damaging hardware, or even directly incurring charges on the user's phone bill. Mobile devices also pose privacy risks, as they store sensitive personal information that may be accessed and shared inappropriately.\n\nEmpowering users to decide how resources on their mobile devices are accessed (i.e., ""granting permission"") is an important challenge for the future of mobile computing. Our research has shown that existing mechanisms are ineffective: users frequently grant permissions because they either do not understand them, are habituated to them, or feel that they have no other choice. This research project aims to identify and study potential solutions to these problems.\n\nThis project develops a user-centered approach to mobile device permission requests. The project is conducting human-subjects experiments to design and validate more effective mechanisms for regulating privacy- or security-sensitive actions. The research agenda involves minimizing habituation to security warnings by substituting them with protected widgets (i.e., ""trusted UI"") or audit mechanisms, when possible; improving the design of security warnings, because alternative permission-granting mechanisms are sometimes inappropriate; and integrating these mechanisms into a platform that improves system security by taking a user-centered approach to granting permissions. If successful, this project could help develop a secure foundation for future generations of mobile devices.",1318680,3,personal information
Alex Liu,,alexliu@cse.msu.edu,Michigan State University,"COMPUTER SYSTEMS, Secure &Trustworthy Cyberspace",CSR: Small: Behavior Based User Authentication for Mobile Devices,"Mobile devices equipped with touch screens have increasingly rich functionality, enhanced computing power, and greater storage capacity. These devices often contain private information such as personal photos, emails, and even corporate data. Therefore, it is crucial to have secure yet convenient user authentication mechanisms for touch screen devices. However, the widely used password/PIN/pattern based solutions are susceptible to shoulder surfing (as mobile devices are often used in public settings where shoulder surfing often happens either purposely or inadvertently) and smudge attacks (as oily residues left by fingers on touch screens can be recognized by impostors) and are sometimes inconvenient for users to input when they are walking or driving.\n\nThe goal of this project is to develop a behavior based user authentication approach for touch screen devices. Rather than authenticating users solely based on what they input (such as a password/PIN/pattern), Behavioral Authentication is based upon how users provide input input. Specifically, a user is first asked to perform certain actions, such as gestures/signatures, on touch screens and then the behavior feature information (such as velocity magnitude and device acceleration) is extracted from the actions to authenticate the user based on machine learning techniques. The intuition behind the proposed approach is that people have consistent and distinguishing behavior of performing gestures and signatures on touch screens. Compared with current user authentication schemes for touch screen devices, the proposed approach is significantly more difficult to compromise because it is nearly impossible for impostors to reproduce the behavior of others doing gestures/signatures through shoulder surfing or smudge attacks - they can see it, but they cannot do it.\n\nThis project will advance the knowledge and understanding of behavior based user authentication on touch screen devices. This is potentially transformative research with high-impact. If successful, this project will not only yield a theoretical foundation for behavior based user authentication on touch screen devices but also invite future research along this direction.",1421407,3,private information
Athina Markopoulou,,athina@uci.edu,University of California-Irvine,RES IN NETWORKING TECH & SYS,"EAGER: A New Framework for Mobile Network Monitoring, Learning and Control","Mobile devices generate an ever-increasing volume of traffic, are used for a range of applications from communication to financial transactions, and have access to personal information. Since mobile user behavior as well as third-party activities eventually manifest themselves through using the network, passive network monitoring offers a unique opportunity to detect both legitimate and malicious activity patterns on the mobile device. This project proposes AntMonitor - a new framework for real-time, on-device, passive network monitoring and crowd-sourcing. The goal is to understand, learn and control patterns in network activity, for applications related to privacy, security, performance, and behavioral analysis. The research agenda promotes transparency of mobile data, puts the user in control of how her data are shared or monetized, and can inform policy makers. Given today's size and personal nature of mobile data, changing the practices of how mobile devices handle and share our information can have significant societal impact, primarily in terms of privacy and security and secondarily in terms of the economics of personal data. In addition, the project will train students and minorities, and will provide software tools and data sets to the research community.\n \nThis project will build and deploy AntMonitor - a system for collection and analysis of fine-grained, large-scale, passive network measurements from mobile devices. Design challenges that will be addressed include high performance in terms of network throughput and battery consumption, and modular design so as to support different applications including (i) privacy leaks detection and prevention (ii) learning of user and app behavior and anomaly detection and (iii) network performance monitoring. Each of these application domains requires its own module in the AntMonitor framework and faces its own challenges in terms of system design, algorithms and data analysis. Overall, the project will advance the state-of-the-art in mobile network monitoring and will improve our understanding of patterns in mobile network activity. It will produce novel algorithms and data analysis methods that enhance the performance, security and privacy of mobile devices. A unique challenge lies in crowd-sourcing and deploying AntMonitor with real users in the wild. To this end, the project will explore different ways to popularize the technology, including user-facing apps, libraries, open-source software, and data-sets available to the community.",1649372,28,personal information
Linke Guo,,lguo@binghamton.edu,SUNY at Binghamton,"COMMS, CIRCUITS & SENS SYS, Secure &Trustworthy Cyberspace",CCSS: Collaborative Research: Towards Privacy-Preserving Mobile Crowd Sensing: A Multi-Stage Solution,"Mobile devices, including smartphones and tablets, are becoming extremely prevalent nowadays. Equipped with diverse sensors, from GPS to camera, and paired with the inherent mobility of their owners, mobile devices are capable of acquiring rich information of surrounding environment. However, the wide adoption of mobile crowd sensing is largely hindered by its privacy concerns. To facilitate the functionality of each stage of mobile crowd sensing, including sensing task allocation, sensing data collection, and result aggregation, sensing devices report their location information, sensing capabilities, task preferences, and sensing results to servers that will potentially disclose their daily routings, behavior patterns and even identities. With these concerns, the overall goal of this project is to address privacy leakage issues from different stages of mobile crowd sensing. Privacy-enhanced mobile crowd sensing will attract more participants and thus accelerate the maturity of smart health care, environment monitoring, traffic surveillance, social event observation, etc. In addition, this project will also serve as a training ground for educating future decision-makers and workforce on theory and tools. \n\nThe PIs plan to develop effective and efficient privacy preservation schemes for different stages of mobile crowd sensing. It corresponds to three closely intertwined research thrusts. Thrust I explores protecting user's sensitive information, such as locations, sensing capabilities and task preferences, from the server, while still allowing it to optimally or approximately solve task allocation problems. Rather than highly computationally-intensive crypto-based techniques, privacy preservation schemes will be designed based on decomposition methods and distributed computing algorithms. Thrust II aims to provide user's location privacy in the stage of data collection. Since locations of users, who perform sensing over the same event within a certain geographic area, are highly correlated, it deteriorates user's privacy achieved individually. To address this issue, privacy preservation schemes will be developed by exploring collaborations among users. Game theories will be adopted to further analyze users' strategies and interactions. The objective of Thrust III is to protect users' sensing data privacy during the stage of data analysis. The research is featured by jointly considering the data imperfection that is caused by the limited sensing capabilities at mobile devices and even the misbehavior of lazy/malicious users. To achieve data privacy and service accuracy simultaneously, novel schemes will be developed combining efficient matrix completion methods and advanced crypto techniques.",1710996,3,sensitive information
Robert Capra,,rcapra@unc.edu,University of North Carolina at Chapel Hill,INFO INTEGRATION & INFORMATICS,CAREER: Knowledge Representation and Re-Use for Exploratory and Collaborative Search,"Modern search systems work well when looking for specific information, but do little to support exploratory searches that extend across multiple search sessions and have investigation and learning as primary goals. Current search systems provide few ways for users to capture, re-use, and resume their search efforts, little support for discovering structural information about a topic, and few methods for coordination and knowledge sharing among searchers. As a result, users often begin searches isolated from knowledge about the domain and effective search processes that others have already discovered, and groups working collaboratively on a search must invest considerable manual effort to communicate and coordinate their efforts using channels outside the search system. This research will develop and evaluate novel techniques and interfaces to allow users to capture, save, share, and re-use structured information about search tasks, search processes, and domain information. A central innovation in this research will be the integration of template structures (including lists, hierarchies, two-dimensional grids, and concept maps) into the user interface and underlying search system in order help users save and organize the information they discover. This structured information will then be used by the system to help support the future searches of individuals and groups of users working collaboratively.\n\nThis research will answer questions about how search systems can incorporate methods to capture, share, and re-use knowledge developed during searches to help improve the future search activities of individuals, collaborating groups, and other searchers working on similar tasks. The research will explore new search algorithms and interfaces that leverage structured information about the search to help users with task resumption, coordination of collaborative efforts, discovery of topic structures, and knowledge sharing. The results will provide insights about users' needs for exploratory searches and how systems can best support them. Specifically, five objectives will be addressed. (1) User studies will be conducted to understand how information structures are naturally created and used in different types of search tasks. (2) An experimental search system will be designed and implemented to support the creation, capture, and use of structured search information to help users with exploratory search tasks. (3) Using the experimental search system, studies will be conducted to understand the benefits of different types of template structures for the purpose of saving information for different types of search tasks (e.g., learning, planning, decision-making, collecting). (4) Additional studies will be conducted to understand how existing structured search information can be used to help support individual users in task resumption, collaborating groups of users in coordinating their efforts, and as a form of search assistance for users working on similar tasks. (5) The final objective of this work will investigate methods for identifying and ranking searches that are related to the current search based on the structured search information and other search interaction data. This research will significantly improve the tools and methods for exploratory and collaborative search, and will provide empirical results to guide future search user interfaces and system development. The outcomes of this project will have substantial transformative impact in helping users to discover new information and topic structures, make sense of the information they find, and build from the prior search efforts of others. Results and software developed as part of the project will be disseminated through papers published in top-tier conferences and journals, and will be made available on the project website (http://ils.unc.edu/searchstructures/).",1552587,19,interaction data
Alenka Zajic,,alenka.zajic@ece.gatech.edu,Georgia Tech Research Corporation,"SPECIAL PROJECTS - CISE, Secure &Trustworthy Cyberspace",TWC: Small: Quantitative Analysis and Reporting of Electromagnetic Covert and Side Channel Vulnerabilities,"Most traditional approaches to computer security assume that information from the system can only be sent through intended output channels, such as network connection, monitor, portable disk drive, etc. Side-channel and covert-channel attacks circumvent these protections by extracting information that is leaked or deliberately sent from the system through unintended signals, such as electromagnetic emanations, power consumption, timing of computational activity, etc. Methods for reducing such information leakage are usually tied to specific hardware and/or specific algorithms, and are very labor intensive. This allows specific fragments of code, such as cryptographic functions, to be relatively safe, but is not feasible for analysis and protection in large codes, such as entire operating systems or web browsers, that also tend to process sensitive information. This project is investigating the relationship between software activity and the resulting side-channel leakage, with the goal of automating software analyses that can discover which data might be leaked and in which parts of the code. These analyses can reveal both covert-channel (intentional) and side-channel (unintentional) leakage vulnerabilities in software, helping programmers focus their efforts on reducing or eliminating these vulnerabilities. In addition to these research components that will help increase national security, the project also includes specific outreach activities, such as building an interactive demonstrator to help educate the public about cyber-security concepts, and visits to local schools to help improve K-12 education and participation of women and minorities in science and engineering.",1318934,3,sensitive information
Yuhong Guo,,yuhong@temple.edu,Temple University,ROBUST INTELLIGENCE,RI: Small: Improving Multi-label Classifiers by Learning Output Representations,"Multi-label classification refers to automated classification in which multiple target labels are assigned to each instance. For example articles often contain several topics, and images contain multiple types of objects. Multi-label classification is a central problem in big data analysis. Complex data --- such as documents, images and videos --- require automated content annotation to support ranking, retrieval and monitoring operations. Unfortunately, automatic annotation is difficult because multiple labels, exhibiting complex inter-relationships, must be assigned from a large open vocabulary. This research project addresses the three main challenges faced by automated annotation systems that learn multi-label classifiers from data: (1) capturing and exploiting label dependence to overcome data sparsity, (1) exploiting partially labelled data to expand the range of usable resources, and (3)reducing prediction model size to allow practical usability. These three challenges will be tackled from a unified perspective of output representation learning, which has the potential to deliver automated methods for semantic annotation that demonstrate greater autonomy, robustness and accuracy. This research will be integrated into graduate and undergraduate courses, which will allow students to develop analytical and computational skills for big data analysis that are currently in high demand. Being centered at a university with a strong program for minority students, this project will also engage participation from underrepresented groups. Undergraduate and high school students will also be engaged through student project competitions. \n\nThe core technical challenges addressed by this research project arise from the phenomena of complex label spaces and sparse data: annotations in big data exhibit ontological structure and missing labels, while even in massive data collections, like Flickr, most labels do not have sufficient positive examples to allow an accurate classifier to be trained independently for each label. To address these challenges, this research project will pursue three main research aims: First, methods for learning multi-label output kernels will be developed that allow auxiliary label information to be combined with state of the art multi-label training losses. These methods will provide important new approaches for addressing the label dependence challenge. Second, methods for learning distributed label representations will be developed that also incorporate auxiliary label information with effective training losses. These methods will provide new approaches for addressing the label dependence and label dimension challenges that make an alternative computational trade-off to output kernel learning. Third, new methods for learning output representations from incomplete labels will be developed that combine missing label imputation with predictor training under effective multi-label losses. This work will greatly extend the practical applicability of multi-label classification learning methods to the type of partially labeled data that is usually encountered in big data. By using output representation learning to improve the quality of multi-label classifiers, this research will advance ranking and retrieval capabilities in important applications, including document and health record management, image and video management, and semantic web analysis. Moreover, by offering flexible engagement opportunities via diverse application studies, algorithm development, experimentation and analysis, this project is well suited to engaging students in research.",1422127,18,flickr
Ruta Mehta,,rutameht@illinois.edu,University of Illinois at Urbana-Champaign,ALGORITHMIC FOUNDATIONS,CAREER: Equilibrium Computation and Other Total Search Problems,"Nash and market equilibria are two of the most fundamental solution concepts in computational game theory and economics, respectively. These concepts are applicable to many long-standing open questions in diverse fields such as cryptography, topology, and verification. Even though we have a fair understanding of these by now, many fundamental questions still remain unresolved in areas such as efficient approximation algorithms, beyond-worst-case analysis, and the relations between the sub-classes and the problems therein. This project aims to explore these questions by bringing together tools from equilibrium computation, sum-of-squares analysis, robust analysis, and other areas. The project is expected to provide efficient and robust algorithms for a large class of such problems, as well as to develop tools to obtain connections among problems from disparate fields and thereby bring insights into their complexity. The former will have a positive practical impact due to numerous applications in areas such as social network analysis and resource allocation. The project will involve and train graduate and undergraduate students at various levels of the project, integrate the findings with teaching, and make lecture notes and other material freely available online. The project will also reach students from underrepresented groups through mentoring workshops and the opportunities provided by initiatives at the University of Illinois at Urbana-Champaign.\n\nThe three main research goals of this project are: (i) understand the recent exponential time hypothesis for the class PPAD through the complexity of constant-approximate Nash equilibria, (ii) understand the relative complexity of problems in the class CLS coming from topology, verification, cryptography, etc., and (iii) develop beyond-worst-case analysis to explain the existence of simple and empirically fast algorithms for computing equilibria. These problems will involve developing novel tools for approximation and for beyond-worst-case analysis. Furthermore, new reduction techniques will need to be developed to relate the open problems from diverse fields and/or to prove hardness of approximation. The project will approach these by building on recent work on equilibrium computation and complexity, using tools from the sum-of-squares method, recent smoothed analysis techniques, and advances in proving lower bounds. Tools developed in the process will contribute to the burgeoning literature in these domains and open up avenues for further exploration.\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",1750436,27,freely available online
Paul Dourish,,jpd@ics.uci.edu,University of California-Irvine,Secure &Trustworthy Cyberspace,SBE: Small: Security as an Everyday Practical Concern,"News reports are rife with accounts of data breaches, network security problems, and the dangers of keeping personal information online. Modern life, though, makes it impossible to avoid doing just that. It is almost no longer a choice whether we make purchases, communicate with our bank, apply for government services, or file our taxes online - it is the standard expectation. This research explores how people resolve the tension between these two realities and the practices that people have adopted to balance competing demands upon them.\n\nThe goal is to understand how people manage security online, in two ways. The first is in the sense of engaging in technical fixes for potential problems; the second is how they come to terms with the potential risks and develop strategies, accommodations, and justifications for particular ways of working online. Using techniques from anthropology and sociology, this project sets out to understand online security as part of people's everyday lives. The research will have two major outcomes. The first to document the conditions of contemporary digital life, as a contribution to ongoing studies of the impact of digital technology. The second is to provide the basis on which new technologies can more adequately protect people's privacy and security online, and more easily integrate with people's online and offline practices.",1525861,3,personal information
H. Raghav Rao,,hr.rao@utsa.edu,University of Texas at San Antonio,Secure &Trustworthy Cyberspace,RAPID: Collaborative Research: Employees' Response to OPM Data Breaches: Decision Making in the Context of Anxiety and Fatigue,"Nontechnical Description\nAccording to recent reports in the press, the Office of Personnel Management (OPM) was hit hard in two recent cyber-attacks (OPM 2015). In April 2015, OPM discovered that personal data (e.g., Social Security Numbers, full name, and birth date) of 4.2 million current and former Federal government employees had been stolen (referred to as personnel records incident hereafter). Later in June 2015, OPM discovered that around 21.5 million employees - current and former Federal employees and contractors - were affected as their personal information such as Social Security Numbers, fingerprints, and background investigation records were compromised (referred to as background investigation records incident hereafter). Unlike other information, sensitive data such as the background investigation records, which include personal histories, relationships, and biometrics, reveal employees' personal lives are difficult to be re-issued. Typical protection such as a few months of credit monitoring may be insufficient in protecting victims from determined attackers. To date, little is known about how and why people decide and act in the aftermath of breaches involving their personal data. In particular, the role of data breach fatigue, manifested by insensitivity to data breaches and low estimate of fraud loss, in affecting people's decisions and actions is unknown. The existing research has also been silent on employees' decision making and reactions in response to data breaches. To fill this research gap, in this proposal, we plan to conduct a study that reveals the key decision factors, response actions, and the potential effect of data breach fatigue in the context of anxiety over the possible outcomes of the breach. Findings of the study will help in understanding employee reactions towards data breaches. New knowledge will help industry and policy makers develop intervention strategies that avert the effect of breach fatigue\n\nTechnical Description\nThis proposal will explore the crucial issues that influence employees' responses in the context of the recent two OPM data breach incidents. This proposed research will compare these two different incidents and their impacts on different types of victims, employees who receive notification of the personnel records incident (now), employees who receive notification of both incidents (future), and employees who only receive notification of the background investigation records incident (future). We will also survey employees who have not received any notification, as a control group. In addition to self-reported data through surveys, we shall extend this study by capturing organic Twitter messages related to the two breach incidents in the respective time periods in 2015 to study how people coped with breach incidents. Utilizing natural language processing, we intend to (1) explore patterns of discourses associated with the data breach fatigue, (2) extract coping mechanisms from the discourses, and (3) compare coping mechanisms of employees, identified from the survey and those derived from the data mining.",1651060,3,personal information
Helena Mentis,,mentis@umbc.edu,University of Maryland Baltimore County,Secure &Trustworthy Cyberspace,SaTC: CORE: Small: Negotiating Cyber Systems Access for Older Adults with Mild Neurocognitive Disorder,"Older adults are rapidly increasing their use of online services such as banking, social media, and email -- services that come with risks around security and privacy. These risks may be especially important for older adults who suffer from mild neurocognitive disorder (miNCD), which can reduce their ability to recognize scams such as email phishing, follow recommended password guidelines, and consider downstream implications of sharing personal information. One way older adults with miNCD cope with their impairments is through the help of caregivers, including partners, children, and professional health personnel, when using and managing online services. This, too, carries risks: sharing personal information with caregivers can raise questions of embarrassment, agency, autonomy, and information leakage; caregivers also do not always act in their charges' best interest. Through a series of interviews, design work, and evaluation, this project will study the issues that arise around caregivers helping older adults with miNCD use security- and privacy-sensitive online services, and how new system designs can reduce the privacy, security, and relational risks involved while increasing older adults' autonomy and access to online services. Beyond that specific context, the work will deepen understanding of both how cognitive impairments affect technology use and how to design accessible technologies that people with impairments use through collaborating with others -- a common situation for people with a variety of physical and mental impairments. The work will also inform interdisciplinary course materials aimed at technology and aging and provide both undergraduate and graduate students research experiences working with these populations.\n\nThe work is organized around three main activities. The first aims to determine how older adults with miNCD and their caregivers currently manage access to such services, and problems with current practices. To do this, the team will study pairs of caregivers and older adults (both with and without miNCD, to isolate whether findings are related to age versus cognitive disorder), using a person-centered design approach that combines short-duration interview and observational work in comfortable environments and is well-suited to working with both older and cognitively impaired individuals. These findings will be used to develop requirements for the second main activity, a participatory design (PD) process aimed at designing interaction mechanisms for cooperative negotiation around access to online banking, social media, and email accounts. The PD process will also follow the person-centered philosophy, using an ""invisible device"" paradigm that prompts participants for design ideas by showing video scenarios in which actors use and discuss unspecified devices to address issues identified in phase 1. The team will then synthesize ideas generated by caregiver-older adult pairs into user interaction prototypes for each of the three domains and test them for basic usability. The third main activity is a six-month longitudinal deployment of these prototypes to 40 older adults (20 with and 20 without miNCD). Participants will keep a diary of adverse events and their reactions; the team will analyze these, along with surveys, interviews, and video observations, to assess how the technologies affect participants' perceptions of their caregivers and autonomy, their involvement in and satisfaction with decision-making around online access, and their ability to manage risks around security and privacy.",1714514,3,personal information
Wei Xu,,xu.1265@osu.edu,Ohio State University,ROBUST INTELLIGENCE,CRII: RI: Learning a Timely Semantic Resource from Social Media Data,"One key challenge in text mining and natural language processing research is that a single meaning can be expressed in many different ways, i.e., paraphrases. There has been steady progress towards large paraphrase resources, and a significant increase in its applications: from information retrieval, information extraction, and natural language generation to IBM's Watson, Google's Knowledge Graph, and many more. This research aims to create better paraphrase acquisition techniques and larger scale semantic resources, which could be of great use in various natural language processing tasks and social media data analytics in social science, national security, and other related fields. One example of potential applications is text simplification, which automatically rephrases complex texts into simpler language for children or people with reading disabilities.\n\nThe technical innovation of this study focuses on joint modeling of word- and phrase-level alignments between sentence pairs to address the challenges of extracting semantic knowledge from informal data sources (such as social media), which exist in very large quantities rather than just formal sources, such as newswire as per previous work. The model design extends multiple instance learning via two methods, a graphical model and neural network, and can flexibly permit the exploration of different assumptions and models the importance of words or phrases. The modeling advancements can be generalized to other natural language understanding tasks, which require analyzing sentences based on word-level composition or word meaning in a given context, and natural language generation tasks that benefit from learning what words and phrases to remove or rephrase.\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",1755898,18,social media data
Helen Nissenbaum,,hfn1@nyu.edu,Cornell University,Secure &Trustworthy Cyberspace,EAGER: Collaborative: A Research Agenda to Explore Privacy in Small Data Applications,"One of the crucial ideas behind Privacy by Design (PbD) is that privacy should be taken into consideration in the process of design, not merely after-the-fact, as so often happens. Yet, this ideal has failed to gain widespread practical traction, challenged, in part, by the lack of developed methodologies and also because of privacy's conceptual complexity, which hampers its operationalization. This project addresses both challenges simultaneously, seeking (i) to demonstrate how a robust operationalization of privacy can lead to meaningful PbD and (ii) to contribute methodological insight by engaging with ongoing research and development in the area of small data applications, namely, systems that advance wellness and personal productivity by utilizing digital traces from individuals' day-to-day activities, such as e-mail, grocery shopping, TV watching, transportation, mobile devices, and so forth. Adopting the definition of privacy as contextual integrity, the project will focus on selected small-data applications currently ""on the drawing board"" in PI Deborah Estrin's Small Data lab. With these design cases, the project rises to one of the PbD challenges, namely consideration of privacy early on in development and, as a research enterprise, its primary aim is to uncover more and less productive methodological approaches for doing so, resulting in system characteristics well correlated with privacy requirements. \n\nAt the same time, the project will provide invaluable insight into how to operationalize contextual integrity, which conceives of privacy as appropriate flow of personal information, modeling appropriate flow as conformance with context-specific informational norms, which, prescribe (and prohibit) information flows according to three parameters: actors (subject, sender, recipient), information types, and transmission principles (functional constraints on flow). Adopting contextual integrity as an operational definition means that researchers will assess privacy properties by carefully mapping data flows, and evaluating these flows in terms of the context of application and use. The project also extends past work on formal representations of informational norms by demonstrating how they may be integrated into design practices. In addition to its substantive contributions this project embodies an innovative collaborative model -- a novel pairing of a computer scientist, Deborah Estrin (Cornell), with a philosopher, Helen Nissenbaum (NYU), in an equal partnership to forge technologies that embody meaningful privacy.",1818586,3,personal information
Kevin Steinmetz,,kfsteinmetz@ksu.edu,Kansas State University,Secure &Trustworthy Cyberspace,SBE: Small: Technological Con-Artistry: An Analysis of Social Engineering,"One of the most serious threats in the world today to the security of cyberspace is ""social engineering"" - the process by which people with access to critical information regarding information systems security are tricked or manipulated into surrendering such information to unauthorized persons, thereby allowing them access to otherwise secure systems. To date, little systematic research has been conducted on social engineering. This research will fill this void by examining who social engineers are, why they engage in social engineering, the processes they use to conceive of and implement social engineering projects, and how they view information privacy and security and justify their behavior. Further, to understand how organizations affected by social engineering cope with the threat it poses, this research also examines the perspectives on social engineering of IT professionals who oversee organizational computer systems and the security of potentially sensitive information. Through gaining a deeper and more accurate understanding of social engineering - a phenomenon currently shrouded in myth and misconception for many - this research will contribute to important advances in criminology and other fields with a vested interest in learning about the human dimensions of information security and inform the development of information security strategies. \n\nThis study uses a cross-sectional, non-experimental research design that employs both qualitative and quantitative data. The qualitative component involves semi-structured interviews of social engineers ""in the wild,"" security auditors, and IT professionals. Open-ended interview questions will be used to elicit this data. In addition, these interviews will be used to gather quantitative data to measure demographic, computer use, and other social characteristics of social engineers. A set of structured survey questions will be administered by the interviewer as part of the interview process. To select a sample of subjects, a nonprobability, purposive, ""snowball"" sampling design is used, which is well-suited for studying ""hidden"" populations such as social engineers. To analyze the qualitative data, grounded theory techniques are used which involve the transformation of data into concepts, which are then summarized into broader analytic categories, leading to the isolation of patterns in the data. Quantitative data will be analyzed through an assortment of univariate, bivariate, and multivariate techniques.",1616804,3,sensitive information
Chuan Yue,,chuanyue@mines.edu,Colorado School of Mines,"SPECIAL PROJECTS - CISE, Secure &Trustworthy Cyberspace",EAGER: Investigating Elderly Computer Users' Susceptibility to Phishing,"One of the most severe and challenging threats to Internet security and privacy is phishing, which uses fake websites to steal users' online identities and sensitive information. Existing studies have evaluated younger users' susceptibility to phishing attacks, but have not paid sufficient attention to elderly users' susceptibility to phishing in realistic environments. As the elderly population in the United States and the world continues to grow rapidly, the elderly Internet user population also continues to grow, and seniors have become very attractive targets for online fraud. \n\nTraditional forms of phishing have been prevalent for over a decade; in contrast, web single sign-on phishing is a more modern strategy, with unique characteristics that make it more profitable, insidious, and harder to detect than traditional phishing. The goal of this project is to systematically compare younger and older computer users' susceptibility to both the traditional and the newly emergent web single sign-on phishing. We build a comprehensive computer testbed that measures phishing susceptibility in a realistic environment. We hypothesize that older adults will differ from younger adults in terms of their susceptibility to both types of phishing, and that this susceptibility can be explained by differences in cognitive abilities, specifically executive functioning and decision-making skills. \n\nThe results of this project will advance our knowledge on how and why elderly users may fall victim to phishing, and will provide a solid basis for researchers to further design effective mechanisms to protect elderly users against phishing from both technical and cognitive perspectives.",1624149,3,sensitive information
Jay McCarthy,,jay.mccarthy@gmail.com,University of Massachusetts Lowell,Secure &Trustworthy Cyberspace,TWC: Small: Automated Protocol Design and Refinement,"Online security relies on communication protocols that establish trust and authentication. New protocols are created regularly, such as when Software-as-a-Service companies expose their software through new Web services. In the ideal case, network engineers and protocol experts collaborate to develop a protocol: one responsible for its efficiency and the other for its security. Unfortunately, this ideal is rarely realized. Protocol experts are rare and their techniques are too complicated for solo network engineers to use, who instead just follow informal ""best practices."" As a result, most of these protocols end up with security problems. This research investigates an automated protocol expert that provides the network engineer with the service normally given by the human protocol expert. The availability of this open-source expert will broadly impact the trustworthiness of cyberspace by increasing the security and reliability of the online services that use it.\n\nThe PI will construct this expert after three technical advances: first, a new security property specification language based on protocol goals, as opposed to the details of the operation of a protocol, for use by network engineers; second, a new theory of protocol construction based on the composition of disjoint authentication protocols with restrictions from linear logic used to limit the sharing of sensitive information; and third, a theory of protocol optimization based on the attack calculus already used to prove that protocols are secure. These three new theories advance their respective sub-fields and coalesce into the necessary foundation for the automated protocol expert.",1617307,3,sensitive information
Ben Zhao,,ravenben@cs.uchicago.edu,University of California-Santa Barbara,INFO INTEGRATION & INFORMATICS,"III: Small: Analysis and Models of Social Network Structure, Growth and Dynamics","Online social networks (OSNs) such as Facebook and LinkedIn are valuable infrastructures for communication and interactions between a large volume of Internet users. For years, researchers have been trying to answer fundamental questions about the formation of these complex networks, their ongoing evolution, formation of internal structures, and change at different time scales. Since answering these questions requires real dynamics datasets at scale, most prior studies have been significantly constrained by a lack of data. The Principal Investigators have been granted access by an OSN provider to a uniquely detailed and complete trace of dynamics over 2+ years of a social network. The goal is to mine and analyze the traces of network dynamics to validate existing models and guide new models for fine grain network dynamics. Objectives include analysis of the preferential attachment model at different stages of network growth, developing new models of network dynamics at fine granularity in both time and graph topology, and explorations of applications driven by novel metrics of graph dynamics.\n\nThe work has the potential to dramatically change our understanding of dynamics in online social networks. By taking an empirical, data-driven approach to network modeling, they can shed light on how traditional models of network dynamics deviate from ground truth. In addition, they are developing empirical models that are more effective at accurately predicting network events at small scales. Both PIs Zhao and Zheng are heavily invested in educational and outreach programs for female and minority students: female students and postdocs often outnumber male counterparts in their lab. The PIs will disseminate their results to their collaborators atRenren and LinkedIn, and also share results with researchers at Twitter, Zynga, Facebook and Google through existing technical contacts and informal visits/talks. For further information, please see the project webpage (http://sandlab.cs.uchicago.edu/dynamics/).",1321083,19,linkedin
Jeannette Wing,,WING@COLUMBIA.EDU,Carnegie-Mellon University,TRUSTWORTHY COMPUTING,TC: Medium: Semantics and Enforcement of Privacy Policies: Information Use and Purpose,"Organizations, such as hospitals, financial institutions, and universities, that collect and use personal information are required to comply with privacy regulations, such as the Health Insurance Portability and Accountability Act (HIPAA), the Gramm-Leach-Bliley Act (GLBA), and the Family Educational Rights and Privacy Act (FERPA). Similarly, to ensure customer trust, web services companies, such as Google, Facebook, Yahoo!, and Amazon, publish privacy policies stating what they will do with the information they keep about customers' individual behaviors. These policies impose constraints on disclosure (or transmission) of personal information, articulate obligations (e.g., notifying customers about privacy breaches), and identify purposes for which personal information may or may not be used. Prior work has focused on formalisms for disclosure and obligations, but no such foundation has been developed for information use for specified purposes.\n\nIntellectual Merit. This project addresses the central problem of developing a formal semantics that explains what it means to use information for a set of purposes, a logic for specifying such policies, and algorithmic methods for their enforcement. It advances the state of knowledge in the field of privacy by providing a foundation for a concept that is commonly used in practice, but has not been the subject of careful scientific study. The project also investigates the interaction of this concept with the previously studied concepts of disclosure and obligation, thereby enabling a more comprehensive understanding of privacy. The formal semantics the project develops is novel and draws on insights from prior work on philosophical theories of causation and intentions, and from the computer science literature on formal methods, information flow, and planning. The model is validated through user studies and its application through case studies in the healthcare domain. \n\nBroader Impacts. The project addresses a problem of significant and growing importance to society. It initiates a new direction in providing foundations for privacy by studying the concept of information use for a purpose. This concept appears in privacy policies published by organizations in sectors as diverse as finance, web services, healthcare, insurance, education, and government - the cornerstones of modern society. The semantic foundation serves as the basis for developing practical tools to support the enforcement of such policies in such organizations. The project provides opportunities for engaging graduate and undergraduate students. The PIs plan to integrate the research results into their existing security and privacy courses, and, for wider dissemination, leverage outreach programs in Carnegie Mellon's Computer Science Department and CyLab aimed at K-12, women, persons with disabilities, and underrepresented minorities.",1064688,30,personal information
Nicholas Weaver,,nweaver@icsi.berkeley.edu,International Computer Science Institute,Secure &Trustworthy Cyberspace,TWC: Medium: Understanding and Illuminating Non-Public Data Flows,"Our lives are surrounded by a constant web of data, picked up by a global network of unseen programs that gather, coalesce, combine, and merge every scrap of data they can acquire. These programs and companies operate out of public view, collecting and exchanging data for profit without clear public knowledge. This is a complex ecosystem, the original collectors of data are likely unaware of eventual uses, users of data may be unaware of the original source. This project seeks to illuminate this ecosystem through a series of experiments by attempting to measure and perturb unseen data pools by selectively adding or retrieving information. Additionally, this project focuses on creating traps and triggers, artificial data that future data providers might employ, enabling discovery of new collection and use of data. Finally, simply researching the phenomenon is insufficient: a final critical factor is education and outreach, empowering the public with an understanding of these otherwise unseen programs. The philosophy of this project is simple: If these data pools affect our lives, we must know what they are and what they do. \n\nThe technical focus of this project involves perturbing the data systems and soundly measuring the results. Some data brokers provide user access, allowing the direct validation of inferences. The project also involves creating ?personas?, artificial identities designed to leave traces in data pools. If a data broker purchases and acts on this data, this creates a causal link between data source and data consumer, allowing attribution of data flows within the data ecosystem. Other portions of the project involve purchasing data directly from brokers, evaluating the potential damage that such brokers may entail, and deliberately seeding multimedia content which includes various levels of identifiable information to detect when data brokers begin scraping these multimedia sources.",1514509,3,scraping
Michael Rosulek,,rosulekm@eecs.oregonstate.edu,Oregon State University,Secure &Trustworthy Cyberspace,CAREER: Getting the Most out of Secure Multi-Party Computation,"Our society is becoming increasingly reliant on powerful and interconnected computing devices that store much of our personal information. These devices present an ever-growing tension between the desire for our personal information to be private, and the desire to put our personal information to good use for our own convenience. In cryptography, problems that involve requirements of useful computation and privacy are understood through the lens of secure multi-party computation (SMPC). Specifically, SMPC refers to the problem of how mutually distrusting parties can securely and collaboratively perform tasks that involve private information.\n\nIn this project, the PI will develop new techniques for understanding and overcoming the challenges and limitations posed by SMPC protocols. In particular, the following major themes will be explored: (1) is it possible in the real world to design protocols that remain secure regardless of other concurrently running protocols? (2) what security properties can be embedded into objects like ciphertexts and digital signatures? (3) how can shared data be maintained persistently while remaining private? The PI will develop new materials for undergraduate courses in cryptography and algorithms, based on principles of pedagogically sound visualization. The PI will also create resources for the cryptography research community, in the form of an online database of cryptographic constructions and primitives.",1420062,3,personal information
Srini Devadas,,devadas@mit.edu,Massachusetts Institute of Technology,"CYBERCORPS: SCHLAR FOR SER, Secure &Trustworthy Cyberspace",TWC: Small: Ascend: Architecture for Secure Computation on Encrypted Data,"Outsourcing computation to the cloud has a difficult set of privacy challenges, a primary one being that the client cannot really trust cloud or application software. Encrypted computation achieves privacy by having the user specify encrypted inputs to a program in the cloud and returning encrypted results.\n\nThe design and implementation of a secure processor architecture, called Ascend, that guarantees privacy of data computed upon by untrusted programs and run on an untrusted operating system (OS) is underway. Our security goal is to only trust the Ascend processor chip and show that it is secure against software attacks and power analysis attacks on its pins even though application and system software can be malicious. Our performance goal is to show that execution time and energy overheads of encrypted computation are reasonable. The key idea in Ascend to guarantee privacy is obfuscated program execution: from the perspective of the Ascend chip's input/output and power pins, an untrusted server cannot learn anything about private user data regardless of the program run.\n\nThrough innovations in architectural mechanisms, security protocols, and applied cryptography, we hope to show that it is viable to only trust hardware and not trust any software in some security-conscious applications, thereby substantially minimizing the trusted computing base for these applications. The development of simulator infrastructure and hardware prototypes will allow the fruits of the research to be widely disseminated. This project will introduce high-school students to research in applied cryptography and security through an innovative high-school outreach program.",1317763,3,"private user, user data, private user data"
Xing-Dong Yang,,Xing-Dong.Yang@Dartmouth.edu,Dartmouth College,"CRII CISE Research Initiation, EPSCoR Co-Funding",CRII: SaTC: Lendable: Designing Modular Hardware and Unobtrusive Interactions to Enable Convenient and Trustworthy Lending of Small Personal Computing Devices,"People often want to borrow or lend personal computing devices for many tasks: to make a phone call, to take some pictures, to navigate, to watch a video. Lending devices is risky, however, because of the amount of personal information they store and the lack of good tools for managing access to that information. This leads lenders to closely monitor borrowers' use, or to not lend the devices at all, both of which limit the usefulness of device lending and can lead to awkward interactions. This project will develop mechanisms to provide lenders with better control over the functions and data that borrowers can use, exploring two main ideas. The first is to use smartwatches, which are increasingly common, to support unobtrusive remote control of loaned smartphones. The second is to use modular designs, in which the data is stored on a base unit and features are provided through detachable modules. When separated, the modules enter a 'guest mode' where they can communicate with the base to access needed functions such as location or cellular network services, but are unable to access the owner's personal data. This work will extend knowledge of how to develop miniature and modular technologies designed to be used by multiple people while protecting privacy. The project will also provide PhD and undergraduate students opportunities to gain interdisciplinary research experiences, and open-source software and hardware prototypes that can be taken up by other researchers, designers, and makers. \n\nThe work will proceed in three main stages. The first stage will explore the privacy needs and interaction techniques around using a smart watch as the base unit for a smart phone. Through deploying prototypes with varying levels of guest access, the researchers will determine the common kinds of privacy requirements associated with common lending tasks. They will then design and evaluate the usability of unobtrusive interaction techniques, such as which hand a device is loaned with or the orientation of the device, for specifying privacy settings when the device is lent. The second stage will focus on designing modular smartphone systems such that the phone's functions are naturally divided among modules to balance usability and privacy concerns. For this stage, the researchers will start with focus groups using mock-ups to do requirements analysis aimed at determining how features can best be allocated to modules to support lending while still providing lenders with useful functionality. They will then develop physical prototypes that realize these requirements and, as with the first stage, design and evaluate the usability of unobtrusive mode-switching interactions, such as the direction or order in which modules are detached. Finally, inspired by prior work on dual-face smartwatches and the goal of continued miniaturization of computing devices, the researchers will marry the first two thrusts, designing modular smartwatch prototypes and the appropriate interaction techniques for managing access to them.",1657141,25,personal information
Richard Wash,,wash@msu.edu,Michigan State University,"SPECIAL PROJECTS - CISE, Secure &Trustworthy Cyberspace",SaTC: CORE: Small: Using Stories to Improve Computer Security Decision Making,"People regularly need to make security and privacy decisions; however, they often don't realize they are making these decisions, and when they do, they often lack the experience and ability to make good choices. Based on studies of how people make decisions ""in the wild"", this project looks to improve people's security education, training, and awareness (SETA) by (1) using short stories about regular users' security behaviors, rather than expert advice, facts, and warnings, to raise awareness of and suggest responses to security risks, and (2) deliver those stories at exactly the times people might need them, rather than as a separate training program divorced from people's regular use and needs around security. Through a series of interviews the project team will learn more about how experts versus non-experts make security-related decisions in the moment. Using these insights and theories of decision-making, the team will develop and test a set of story-based training materials for common security decisions including selecting passwords, ignoring phishing emails that lure people to download malware or give personal information to fake websites, and avoiding sites that present invalid security credentials. These experiments will increase knowledge of how people make security decisions and how to design materials to support SETA, as well as directly improving security at the lead researcher's institution through live testing with students and staff. The PI will also involve both undergraduate students and people from underrepresented groups in the research and publicly release the materials the team develops.\n \nThe project seeks to test the hypothesis that telling end users stories about security incidents can better train them to resist semantic attacks than traditional facts-and-advice training. The researchers will first develop a detailed understanding of how people make everyday in-the-moment security decisions, using Critical Decision Method and Experience Sampling Method-based approaches that focus on specific past attacks. The team will interview both experts and non-experts to learn what features they use to recognize attacks and how they identify actions to take; comparing expert to non-expert behavior will help identify vulnerabilities and inform both effective training goals and materials. These insights will be used in developing a set of story-based training materials that emphasize important constructs suggested by the theory of Naturalistic Decision Making including incident typicality, social norms around responses, causality (linking responses to outcomes), and empowerment and efficacy in security decision-making. Through a series of field experiments in collaboration with security mangers at the lead researcher's institution, the team will iteratively improve the training materials while developing theoretical knowledge of how stories about security incidents can support security decision-making in naturalistic settings.",1714126,3,personal information
Gookwon Suh,,gs272@cornell.edu,Cornell University,Secure &Trustworthy Cyberspace,TWC: Small: Flash Memory for Ubiquitous Hardware Security Functions,"Performing financial transactions on a smartphone raises a number of security concerns. How can a bank be certain that a request is authentic? How do we prevent the same transaction to be unintentionally repeated? How can we ensure your sensitive information cannot be copied even if a phone is lost? Strong hardware security functions such as device fingerprints and true random number generators are essential in addressing these questions. However, traditional hardware security functions are difficult and expensive to build. This project investigates using off-the-shelf Flash memory, which is already in most digital systems today, to provide security functions like device fingerprints, random number generators, and secure information storage. These security functions will be extracted in a plug-and-play fashion from today's Flash without any customized modification, enabling hardware-based security in virtually all electronic devices. Therefore, this project will greatly enhance security and privacy in an era where computing devices are everywhere. Also, the project will train and educate a new generation of interdisciplinary engineers who can understand both security and semiconductor device.\n\nTo enable the proposed security functions, this project taps into inherent analog behaviors of Flash memory such as hidden variations, noises, aging, etc. For example, random numbers can be generated from thermal or quantum noise in Flash memory. The device fingerprints can be extracted from program/erase timing variations of each memory cell, which cannot be predicted or controlled even by the Flash memory manufacturer. Information hiding can be achieved through selective stressing of bits to create probabilistic differences. Such hidden information will be very difficult to copy or even detect unless a specific secret key is known. These analog behaviors can be observed through the standard Flash memory interface without interfering with normal memory functions. Therefore, the proposed security functions will be broadly applicable to electronic systems with Flash memory.",1223955,30,sensitive information
Akhilesh Tyagi,,tyagi@iastate.edu,Iowa State University,Secure &Trustworthy Cyberspace,SaTC: STARSS: Metric & CAD for DPA Resistance,"Physical side channels pose a big threat to the security of embedded hardware. The differential power analysis (DPA) attack is a well known side channel threat which exploits the linear dependence of the power on the secret data or an intermediate value correlated to the secret data through statistical model building. This project addresses the DPA vulnerability by deploying a technology cell library consisting of private gates. The technique developed will make embedded hardware less vulnerable to side-channel attacks, thereby securing private user data and transactions.\n\nThis project develops logic level and netlist level metrics to model the DPA vulnerability of a circuit. The logic level metric, called normalized variance, is efficiently computed for a logic network with switching probability and switched capacitance estimation through BDDs or other methods. The project develops a more accurate but computationally more expensive metric, which is a refinement of the normalized variance and is deployed at circuit level.",1441640,3,"private user, user data, private user data"
Pramod Viswanath,,pramodv@uiuc.edu,University of Illinois at Urbana-Champaign,"COMM & INFORMATION FOUNDATIONS, Secure &Trustworthy Cyberspace",CIF: Small: Statistical Data Privacy: Fundamental Limits and Efficient Algorithms,"Privacy is a fundamental individual right. In the era of big data, large amounts of data about individuals are collected both voluntarily (e.g., frequent flier/shopper incentives) and involuntarily (e.g. US Census or medical records). With the ready ability to search for information and correlate it across distinct sources (using data analytics and/or recommender systems), privacy violation takes on an ominous note in this information age. \n\nAnonymization of user information is a classical technique, but is susceptible to correlation attacks: by correlating the anonymized database with another (perhaps publicly available) deanonymized database, a user's privacy could still be divulged. A way out of the limitations of anonymization is to release a randomized database; this offers plausible deniability of any user identity breached via the data release. A systematic way of providing guarantees for the deniability of user presence/absence is the technical field of differential privacy, providing strong privacy guarantees against adversaries with arbitrary side information. It is of fundamental interest to characterize privacy mechanisms that randomize ""just enough"" to keep the released database as true to the intended one as possible, providing maximal utility. \n\nBased on recent work connecting the areas of information theory and statistical data privacy (via a hypothesis testing context) and demonstrating novel privacy mechanisms that exponentially improve (in terms of variance of noise added, say) upon the state of the art for medium and low privacy regimes, the objective of the project is threefold: (a) characterize the fundamental limits to tradeoffs between privacy and utility in a variety of canonical setting; (b) discover (near) optimal mechanisms that can be efficiently implemented in practice; and (c) seek natural notions of statistical data privacy (beyond differential privacy) using the operational context of hypothesis testing. \n\n\nPrivacy is a central, and multifaceted, social and technological issue of today's information age. This project is focused on the technical aspect of this multifaceted area, and seeks to discover fundamental limits to privacy-utility tradeoffs in the context of currently well established notions of privacy (differential privacy). The expected results expected are fundamental and immediately applicable to a variety of practical settings. Specifically, two concrete practical settings involving genomic data release and smart meter data release will be studied in detail. Due to privacy concerns, genomic and smart meter data is simply unavailable at large -- depriving widespread data analytics and practical implications of such analysis. This project will build and release a software suite of sanitization tools, involving the privacy mechanisms discovered as part of this project.",1422278,3,user information
Ling Liu,,lingliu@cc.gatech.edu,Georgia Tech Research Corporation,Secure &Trustworthy Cyberspace,TWC: Medium: Privacy Preserving Computation in Big Data Clouds,"Privacy is critical to freedom of creativity and innovation. Assured privacy protection offers unprecedented opportunities for industry innovation, science and engineering discovery, as well as new life enhancing experiences and opportunities. The ability to perform efficient and yet privacy preserving big data computations in the Cloud holds great potential for safe and effective data analytics, such as enabling health-care applications to provide personalized medical treatments using an individual's DNA sequence, or enabling advertisers to create targeted advertisements by mining a user's clickstream and social activities, without violation of data privacy. The PrivacyGuard project is developing algorithms, systems and tools that provide end-to-end privacy guarantees over the life cycle of a data analytic job. The end-to-end privacy guarantee can be measured by how difficult one can learn about some of the original sensitive data from the sanitized data releases, the intermediate results of execution and the output of an analytic job. The ultimate goal of PrivacyGuard is to develop a methodical framework and a suite of techniques for ensuring distributed computations to meet the desired privacy requirements of input data, as well as protecting against disclosure of sensitive patterns during execution and in the final output of the computation.\n\nThe PrivacyGuard project advances the knowledge and understanding of privacy preserving distributed computation from three perspectives: (1) It designs formal mechanisms to formulate a data owner's end-to-end privacy requirement for each data release, for example, by associating each data release with a well-defined usage scope to confine the set of data analytics models and algorithms that can operate on the released data. (2) It develops a suite of execution privacy guards with dual objectives: to audit and enforce privacy compliances during distributed computation against data-flow based privacy violations and to guard the compliance of input privacy. (3) It devises a proactive approach to output privacy against information leakages associated with mining output, for example, by leveraging differential privacy model to maximize the upper bound for data privacy guarantee and minimize the lower bound for data utility losses. The PrivacyGuard project is the first effort towards a practical and systematic implementation framework for ensuring the end-to-end privacy in distributed big data computations. Furthermore, by integrating the PrivacyGuard research with the curriculum development on big data systems and analytics courses at Georgia Institute of Technology, it contributes to the education and training of new generation of data scientists to be the privacy compliance advocates.",1564097,3,clickstream
Ryan Henry,,henry@indiana.edu,Indiana University,Secure &Trustworthy Cyberspace,SaTC: CORE: Small: Batch Techniques for Practical Private Information Retrieval,"Private information retrieval (PIR) is a cryptographic primitive that solves the seemingly impossible problem of letting users fetch records from untrusted and remote database servers without letting those servers learn which records the users are fetching. The research literature on PIR is vast; for over two decades, the cryptography, privacy, and theory research communities have studied PIR intensively and from a variety of perspectives; compelling applications for PIR abound in the resulting research literature. Alas, despite a series of significant advances, existing PIR techniques remain notoriously inefficient and none of the numerous PIR-based applications proposed in the research literature have been deployed at scale to protect the privacy of users ""in the wild"". This project entails an integrated research agenda that couples a strong theoretical component with an ambitious practical component centered around developing, analyzing, and implementing novel ""batch"" IT-PIR techniques, which can potentially alleviate the ""prohibitive cost"" problem for so-called information-theoretic private information retrieval (IT-PIR), the most performant and well-studied category of PIR protocols. Beyond improving performance, the new batch techniques also improve the ""expressiveness"" of PIR, exposing intuitive APIs through which applications can safely, easily, and efficiently interact with IT-PIR protocols.\n\nPIR has long provided compelling solutions in theory to a wide array of important problems, but it has seen little adoption in practice due in part to the inefficiency and limited expressiveness of existing techniques. Indeed, traditional PIR constructions let users fetch just one data record at a time by encoding the record's index (i.e., its physical locations relative to the other records in the database) in a cryptographically protected query. This project builds on preliminary results by the PI, which extend that basic functionality to not only let users fetch several records (i.e., a ""batch"" of records) for a lower cost than that of fetching each record separately, but also to let users fetch such batches of records using ""contextual"" queries that specify which data they seek, as opposed to ""positional"" queries that specify where those data happen to reside in the database. The main research goals are to (i) develop theoretical frameworks to better understand the mathematics underlying batch IT-PIR, to (ii) use insights gained from these frameworks to improve upon and generalize the known constructions, and to (iii) use the improved constructions to implement practical, privacy-respecting alternatives to a selection of existing privacy-agnostic products and services. The new batch IT-PIR constructions will be incorporated into the open-source Percy++ library, an effort which will deeply involve both graduate and undergraduate students.",1718475,3,private information
Dean Chang,,deanc@umd.edu,University of Maryland College Park,"ENGINEERING EDUCATION, IUSE, COLLABORATIVE RESEARCH, I-Corps, I-Corps - Nodes, I-Corps - Sites","I-Corps Node: DC, Maryland, Virginia Region","Project Description:\nThis effort involves the creation and implementation of an I-Corps Regional Node in DC/MD/VA (DMV) region through a partnership involving the University of Maryland College Park (UMD), the George Washington University (GWU), and Virginia Tech (VT). The proximity of the DMV node to the Washington, DC area's vibrant entrepreneurial R&D ecosystem is providing a valuable set of resources to the national I-Corps network. The node is leveraging the respective strengths of the three institutions to help guide the transition of scientific discoveries into technologies and products that benefit society. It is implementing two initiatives that are specifically designed to increase the success rate of participating teams: (1) establishing a formal DMV I-Corps Mentor Network designed to attract, train, and retain top-notch mentors and (2) offering a post I-Corps Support program to help teams with a series of follow-on activities (e.g., continued customer development, minimum viable product prototyping, technology transfer and licensing, fundraising, legal services, and hiring executive talent). The node is implementing an Online Nodal Network (ONN) that ties together and augments existing tools (e.g., LinkedIn, YouTube, Google Docs, Quora, LaunchPad Central); addressing the needs that are particularly valuable to the nature of I-Corps teams. The ONN aggregates rich content and makes it more readily accessible to the I-Corps user community. It further enables the mentor network to pool and share their expertise with multiple I-Corps teams. In addition, the node is studying the effect of I-Corps training on: (1) any adjustments in orientation toward firm creation, (2) the proportion of teams that reach initial profitability, (3) the time required to reach initial profitability, and (4) the resources expended (time, money) in the start-up process.\n\nBroader Significance:\nIn addition to providing training to NSF team cohorts, the DMV node is managing a Regional DMV I-Corps program that is designed to train an additional 150 teams over a three-year period. Thirty-five of the top two hundred U.S. universities in R&D expenditures are within a 4-hour drive of the DMV node. The node is focusing on attracting teams from these top regional academic institutions, as well as from the many federal and state research labs that are unique to the DMV region (e.g., NASA, NIH, ARPA-E). The node is also engaging underrepresented minority participation and HBCUs through cooperation with the Graduate Degrees for Minorities in Engineering and Science, Inc. (GEM) consortium. It is implementing a GEM-endorsed initiative to increase the awareness of research commercialization opportunities. Small grants are being directed to promising HBCUs' teams in order to provide innovation training. The node is additionally leveraging the DMV's mentor network to support the HBCU teams.",1304387,4,"quora, linkedin"
Zhong Shao,,zhong.shao@yale.edu,Yale University,Secure &Trustworthy Cyberspace,SaTC: CORE: Small: Formal End-to-End Verification of Information-Flow Security for Complex Systems,"Protecting the confidentiality of information manipulated by a computing system is one of the most important challenges facing today's cybersecurity community. Many complex systems, such as operating systems, hypervisors, web browsers, and distributed systems, require a user to trust that private information is properly isolated from other users. Real-world systems are full of bugs, however, so this assumption of trust is not reasonable. The goal of this proposed research is to apply formal methods to complex security-sensitive systems, in such a way that we can guarantee to users that these systems really are trustworthy. Unfortunately, there are numerous prohibitive challenges standing in the way of achieving this goal. One challenge is how to specify the desired security policy of a complex system. In the real world, pure noninterference is too strong to be useful. It is crucial to support more lenient security policies that allow for certain well-specified information flows between users, such as explicit declassifications. A second challenge is that real-world systems are usually written in low-level languages like C and assembly, but these languages are traditionally difficult to reason about. A third challenge is how to actually go about conducting a security proof over low-level code and then link everything together into a system-wide guarantee.\n\nIn this effort, the PI proposes to design and implement a new set of formal techniques and tools for overcoming all of these challenges. First, the PI will develop a new methodology for formally specifying, proving, and propagating information-flow security policies using a single unifying mechanism, called the ""observation function."" A policy is specified in terms of an expressive generalization of classical noninterference, proved using a general method that subsumes both security-label proofs and information-hiding proofs, and propagated across layers of abstraction using a special kind of simulation that is guaranteed to preserve security. Second, to demonstrate the effectiveness of the new methodology, the PI will build an actual end-to-end security proof, fully formalized and machine-checked in the Coq proof assistant, of a nontrivial concurrent operating system kernel. Third, the PI will also demonstrate the generality and extensibility of the methodology by extending the kernel with a virtualized time feature allowing user processes to time their own executions. The goal is to prove that user processes cannot exploit virtualized time as an information channel. The technology for building certified secure system software will dramatically improve the reliability and security of many key components in the world's critical infrastructure. It will advance human knowledge in the specification and understanding of software and catalyze a cultural change in U.S. universities by pushing new courses on formal methods into the existing cybersecurity curriculum.",1715154,3,private information
Racquel-Maria Yamada,,racquel@ou.edu,University of Oklahoma Norman Campus,DEL,RAPID: Documenting Endangered Languages Outreach Video Series,"Recent advances in technology have created new methodologies in documenting endangered languages. This project will provide an overview of these advances in video format and will be presented by experts in field. The purpose of the project is to support production of the highest quality of documentary linguistic products and stimulate research in these areas. The primary audience for these outreach videos are linguists or language experts from minority communities who may engage in language documentation but do not have access to state-of-the-art instruction. Over the long term, greater participation in documenting endangered languages will contribute new knowledge of a wider array of under-documented endangered languages each of which provide unique information on human language and cognition.\n\nThe videos will be uploaded as YouTube videos and will be freely available online. They will include information on tools for language analysis (transcription, interlinear gloss creation, and acoustic analysis) and linguistic analysis needed for dictionary, grammar, and corpus creation. Experts will also review current standards in metadata creation and archiving, and funding mechanisms for documentary linguistic research.",1500695,21,freely available online
Brian Ziebart,,bziebart@uic.edu,University of Illinois at Chicago,INFO INTEGRATION & INFORMATICS,"III: Medium: Collaborative Research: Computational Tools for Extracting Individual, Dyadic, and Network Behavior from Remotely Sensed Data","Recent technological advances in location tracking, video and photo capture, accelerometers, and other mobile sensors provide massive amounts of low-level data on the behavior of animals and humans. Analysis of this data can teach us much about individual and group behavior, but analytical techniques that lead to insight about that behavior are still in their infancy. In particular, these new data can provide an unprecedented window into the lives of wild animals, augmenting the traditional time-consuming first-hand observations from field biologists. Unfortunately, the interpretation of low-level (i.e., unprocessed) data from animal-borne electronic sensors still poses a significant bottleneck in leveraging all of the available data to better understand the individual, pairwise, and group behavior of animal populations. This project will develop tools for scaling the expert knowledge needed to interpret high-level behaviors from low-level sensor data using tools from statistical machine learning and network analysis. These data and analytical tools promise to fundamentally change our understanding why animals do what they do, at high resolution and across multiple scales, from individuals to entire populations. The results of the project will be applicable in many settings where massive sensor data is overwhelming traditional insight derived from observational approaches. As part of the project, unique data on primate behavior that will bridge the low-level data and expert knowledge will be collected at Mpala Research Centre, Kenya. Undergraduate, graduate, and postdoctoral students from computer science and animal behavior will collaborate across continental and disciplinary boundaries. \n\nThe technical aims of this project include developing structured prediction methods that improve behavior recognition at multiple levels (individual, pair-wise, and group), using network properties to improve the identification of group activities, and advancing active learning in the structured prediction setting so that ""expensive"" expert knowledge and supplemental data collection will be judiciously utilized for maximum benefit in learning behavior recognition models. Recognizing animal behavior from low-level sensor data is hierarchical in this approach, with individual activities recognized directly from data and the context of these data, the inferred individual activities informing pair-wise behavior recognition, and inferred pair-wise behavior informing group-level activity recognition. The benefits of improving the accuracy of individual and pair-wise behavior for recognizing group-level behavior will enable expert annotations to be requested that improve behavior recognition the most across all levels. These advances will enable field-biologists to investigate new hypotheses about fundamental evolutionary, ecological, and population processes at scale without the burdens of complete manual annotation of collected data. The methods will be applicable beyond field biology to understanding the hierarchy of behavior from individual entities to groups, from humans to cells, in scientific, educational, and business contexts. The team will leverage the interdisciplinary and international nature of the project to continue its ongoing work to increase participation of women and minorities in STEM research at undergraduate and graduate levels.",1514126,19,location tracking
Margaret Crofoot,,mccrofoot@ucdavis.edu,University of California-Davis,INFO INTEGRATION & INFORMATICS,"III: Medium: Collaborative Research: Computational Tools for Extracting Individual, Dyadic, and Network Behavior from Remotely Sensed Data","Recent technological advances in location tracking, video and photo capture, accelerometers, and other mobile sensors provide massive amounts of low-level data on the behavior of animals and humans. Analysis of this data can teach us much about individual and group behavior, but analytical techniques that lead to insight about that behavior are still in their infancy. In particular, these new data can provide an unprecedented window into the lives of wild animals, augmenting the traditional time-consuming first-hand observations from field biologists. Unfortunately, the interpretation of low-level (i.e., unprocessed) data from animal-borne electronic sensors still poses a significant bottleneck in leveraging all of the available data to better understand the individual, pairwise, and group behavior of animal populations. This project will develop tools for scaling the expert knowledge needed to interpret high-level behaviors from low-level sensor data using tools from statistical machine learning and network analysis. These data and analytical tools promise to fundamentally change our understanding why animals do what they do, at high resolution and across multiple scales, from individuals to entire populations. The results of the project will be applicable in many settings where massive sensor data is overwhelming traditional insight derived from observational approaches. As part of the project, unique data on primate behavior that will bridge the low-level data and expert knowledge will be collected at Mpala Research Centre, Kenya. Undergraduate, graduate, and postdoctoral students from computer science and animal behavior will collaborate across continental and disciplinary boundaries. \n\nThe technical aims of this project include developing structured prediction methods that improve behavior recognition at multiple levels (individual, pair-wise, and group), using network properties to improve the identification of group activities, and advancing active learning in the structured prediction setting so that ""expensive"" expert knowledge and supplemental data collection will be judiciously utilized for maximum benefit in learning behavior recognition models. Recognizing animal behavior from low-level sensor data is hierarchical in this approach, with individual activities recognized directly from data and the context of these data, the inferred individual activities informing pair-wise behavior recognition, and inferred pair-wise behavior informing group-level activity recognition. The benefits of improving the accuracy of individual and pair-wise behavior for recognizing group-level behavior will enable expert annotations to be requested that improve behavior recognition the most across all levels. These advances will enable field-biologists to investigate new hypotheses about fundamental evolutionary, ecological, and population processes at scale without the burdens of complete manual annotation of collected data. The methods will be applicable beyond field biology to understanding the hierarchy of behavior from individual entities to groups, from humans to cells, in scientific, educational, and business contexts. The team will leverage the interdisciplinary and international nature of the project to continue its ongoing work to increase participation of women and minorities in STEM research at undergraduate and graduate levels.",1514174,19,location tracking
Arun Ross,,rossarun@cse.msu.edu,Michigan State University,Secure &Trustworthy Cyberspace,TWC: Small: Imparting Privacy to Biometric Data in Cyberspace,"Recent work has established the possibility of deriving auxiliary information from biometric data. For example, it has been shown that face images can be used to deduce the health, gender, age and race of a subject; further, face images have been used to link a pseudonymous profile in the Web with a true profile, thereby compromising the privacy of an individual. The objective of this work is to design and implement techniques for imparting privacy to biometric data such as face, fingerprint and iris images. \n\nIn this regard, the following tasks are being conducted: (a) methods to modify biometric data such that the modified data can be used for re-identifying an individual but cannot be used to derive auxiliary information about the subject, such as gender and age; (b) methods to generate multiple privacy-enhanced templates from the same biometric data in such a way that these templates cannot be linked using a biometric matcher; and (c) methods to decompose and store the biometric data of a subject across entities such that individual entities cannot determine the identity of the subject, but collaboration across entities is essential for eliciting the identity of the subject. Finally, methods to assess the degree of privacy of a biometric image are being developed in order to quantify the amount of private information that can be derived from it. The proposed methods are being evaluated on publicly available face, fingerprint and iris datasets in order to determine their efficacy in the context of cyberspace applications.",1618518,3,private information
Donald Towsley,,towsley@cs.umass.edu,University of Massachusetts Amherst,Secure &Trustworthy Cyberspace,EAGER: USBRCCR: Improving Network Security at the Network Edge,"Recent years have seen the Internet playing an increasingly critical role in our daily lives with home networks hosting PCs, tablets, mobile devices along with more specialized devices such as smart televisions, thermostats, and other Internet-of-Things (IoT) devices. While these devices offer users an array of services and conveniences, they come at the cost of introducing security vulnerabilities into the home network. Thus users are confronted with the dual challenges of securing their networks and devices against malicious software (malware) and botnets that may perform distributed denial of service attacks on commercial and public websites and of maintaining the privacy of increasingly personal flows of data through IoT devices.\n\nThis project takes a multifaceted look at the problem of securing home networks in the face of these challenges. Specifically, it includes a partnership with a Brazilian Internet Service Provider giving access to data from thousands of home network connections. This allows the creation of a baseline of network behavior against which to identify malicious behavior due to malware or compromised devices. Second, the project will develop behavior models of typical use of IoT in the wild. This will allow a better understanding of how sensitive and personal information can leak from IoT devices to IoT providers. The baseline and the IoT behavior models will lead to new methods for identifying the presence of anomalous/malicious behavior as well as leakage of privacy information. The research conducted in this project provides significant benefits to society. First, the results will allow users to enhance the security of their home networks and better protect personal and sensitive information. Second, the project will provide substantial opportunities for students to develop software and research skills along with cybersecurity skills.\n\nThis project tackles the problem of securing modern home networks. The approach to this problem will be analytical and empirical. The project will consist of:\n(i) Development of techniques based on statistical analysis and machine learning that rely on data gathered in home networks to detect and classify malicious network activities. These techniques will focus on malicious activities both within and outside home networks.\n(ii) Fingerprinting of home network traffic to enable detection of compromised devices and characterization of the behavior of such devices even when flows are encrypted. \n(iii) Development of tools that will help users control access to their data.",1740895,3,"sensitive information, personal information"
Brian Magerko,,brian.magerko@lmc.gatech.edu,Georgia Tech Research Corporation,Cyber-Human Systems (CHS),HCC: Small: Social Agents and Robots for Open-Ended Domains,"Recreational activity is a fundamental aspect of human existence and an important part of the human condition within familial and social groups, where it serves to strengthen social ties by increasing affect between individuals and as a form of education in creative thinking. Despite a sizable accumulation of knowledge about such activity from sociology, anthropology, and psychology, ""play"" as a first-class concept has not been studied through the lens of computation. When today's agents engage with humans, they do so in the context of structured environments and are highly dependent on well-defined goals and/or behaviors. Contrast this to the domain of pretend play, which involves non-goal directed peer-to-peer activity in a shared imaginary second reality that is continually altered. Pretend play is a common form of engagement that is relevant to an array of social domains, such as elder care, peer learning, or social skills therapy for children with autism spectrum disorders. The PI's goal in this research is to imbue robot systems with procedural and declarative representations of play so that they are capable of engaging in such activity with humans as peers. The work aims to discover how to develop humanoid robots with the ability to engage, improvise, and create with humans in unstructured environments. Such robot capability would foster perceptions of lifelikeness, social acceptance, and companionship similar to the experience of playing with other people. These agents would encourage spirited behaviors and creativity in people. They would elicit high levels of interest, intrinsic motivation, and positive affect, which in turn would lead to better concentration, learning, and personal investment by the human participant. To achieve these goals, the PI will leverage his prior work on creativity and cognition to conduct a study of adults engaging in object-based pretend play to elicit a formal understanding of it in dyads. The findings will subsequently be applied to building social robots that engage, based on the team's expertise in human-centered AI and human-robot interaction. The resulting robot architecture will be evaluated to see how it can enhance robot affect and social acceptance. \n\nBroader Impacts: This research will create a new academic research direction of Computational Play within the field of social robotics that has the potential to contribute a solid and unique advance to the field, and also to change how we interact with intelligent agents thereby increasing agents' social value and acceptance by the humans around them. The work will increase via empirical study our understanding of human engagement, and in particular of the knowledge and social dynamics involved in pretend scenarios. The playful robots to be designed, implemented and formally evaluated in this work will inform the human-robot interaction community as to how such activity can be used within HRI contexts to increase affect. This work also has the potential of improving the learning and creativity of those that interact with social agents, making computational play a valuable research direction for education. The project will provide a fertile ground for interdisciplinary training of graduate and undergraduate students, and a wealth of interaction data that will be shared with the scientific community.",1320520,20,interaction data
Guru Prasadh Venkataramani,,guruv@gwu.edu,George Washington University,Secure &Trustworthy Cyberspace,STARSS: Small: Defending Against Hardware Covert Timing Channels,"Safeguarding sensitive user information stored in computer systems is a fast growing concern, especially as computers are universally used everywhere from national defense to mobile phones. Malicious hackers have found unscrupulous ways to steal sensitive information largely by exploiting the vulnerabilities in existing hardware and software. Among the many forms of information leakage, covert timing channels exfiltrate secrets from a trojan process with higher security credentials to a spy process with lesser credentials by exploiting the access timing of system resources. Such covert timing channels are elusive since they communicate indirectly through timing modulation without leaving any physical trace in memory. By detecting the presence of covert timing channels and defending against them, our project offers significant benefits to society through preventing undesired sensitive data leakage to malicious parties. \n\nThe three central objectives of this project are to: 1. Investigate the science behind the construction and operation of hardware covert timing channels, 2. Devise mechanisms to detect hardware timing channels, 3. Mount cost-effective, runtime defense strategies to undermine the reliability of covert timing channels. We investigate scientific ways to identify key indicator events, and analyze their roles in the operation of covert timing channels. We design mechanisms to capture the runtime system behavior, and detect the presence of a covert timing channel. Finally, to prevent information leakage, we use hierarchical, multi-level defense strategy where we estimate the agility of the adversary by measuring the defense?s effectiveness after deployment, and automatically recalibrate our defense to decimate the timing channel activity.",1618786,3,"user information, sensitive information"
Randal Burns,,randal@cs.jhu.edu,Johns Hopkins University,COLLABORATIVE RESEARCH,Interdisciplinary Scientific Data Management,"Scientists are increasingly limited by their ability to analyze the large amounts of complex data available. These data sets are generated not only by instruments but also computational experiments; the sizes of the largest numerical simulations are on par with data collected by instruments, crossing the petabyte threshold this year. The importance of large synthetic data sets is increasingly important, as scientists compare their experiments to reference simulations. All disciplines need a new ?instrument for data? that can deal not only with large data sets but the cross product of large and diverse data sets. \n\nWhile the largest data sets have captured most of the public attention, they only represent the tip of the iceberg. What is often missed is that scientific data sets have a power law distribution. At one end are the very large data collections compiled by hundreds of scientists collaborating over multiple years. These projects typically have coherent data management plans and organization to ensure that the data products are accessible to a wide community. Nevertheless, the long-term curation of the data is still an unsolved problem. \n\nAt the other end of the distribution, in the ""long tail"", are the very large numbers of small data sets, such as the images, spreadsheets and tables collected in laboratories and field studies. While the individual files are small, their numbers add up; in fact, there is as much data aggregated in these small items as in the biggest collections. On the other hand, these data sets are often not as well documented as their bigger counterparts. For most scientists there is little reward in becoming a data management expert and devoting the time required to documenting the data for later reuse. In fact, the process of manually cleaning data sets has been called the strip mining of big data: an ugly and resource intensive effort that leaves big scars.\n\nScientists at the Johns Hopkins University have built innovative frameworks to publish scientific data across a wide range of disciplines, from astronomy to turbulence, and environmental science. These projects already share some common components for data management. This project will connect more of the existing independent components into a coherent one, explore how to scale the data services to deal with the ""long tail"" of the data distribution, and demonstrate the overlap in the basic data management tasks across disciplines. The project has four parts: (i) continue and enhance the efforts on the Sloan Digital Sky Survey, (ii) turn large numerical simulations into easy-to-use numerical laboratories, (iii) enhance an existing end-to-end system for environmental sensors and integrate it with other field data, (iv) enhance and generalize a set of core collaborative tools, and apply these to help with the challenge of the ""long tail"" of scientific data.\n \nThe projects involve the Sloan Digital Sky Survey (SDSS) -- the world's most used astronomy facility -- and its CASJOBs/MyDB collaborative environment The framework will be extended to other areas of science, like in-situ environmental monitoring and field biology. This will be demonstrated by integrating data in soil ecology from the Baltimore Ecosystem Study project with data collected automatically, via a wireless sensor network. The project will also test, how a simple, ""DropBox""-like interface (i.e., online storage and sharing) can be used to overcome some of the barriers that prevent scientists from publishing much of their value-added data. Finally, the project will explore how smaller and larger numerical simulations can be placed into interactive, publicly accessible numerical laboratories, using data sets currently from turbulence and astronomy. \n\nThe funds will support people: a combination of data scientists, database administrators, postdoctoral fellows, students and programmers working together to ""connect the dots"" and bring additional data sets on line. The project will enhance the public interfaces of several publicly available data sets, prototype an easy-to-use environment to upload small user data into a collaborative environment, and create a framework for a new citizen-science project in environmental science.",1244820,30,user data
Manoj Prabhakaran,,mmp@uiuc.edu,University of Illinois at Urbana-Champaign,Secure &Trustworthy Cyberspace,TWC: Medium: Collaborative Research: Transformative New Approaches to Efficient Secure Computation,"Secure Computation is a powerful concept from cryptography that enables collaboration in the absence of trust. Despite its great potential for solving practical problems in collaborative situations, it has not yet been widely adopted in practice. Indeed, until recently there were few practical implementations of secure computation protocols, and even recent implementations were forced to restrict themselves to weak forms of security for the sake of efficiency. This is because the dominant paradigm for achieving strong security, since the invention of the first such protocols, has relied on zero-knowledge proofs, and yields protocols that are too inefficient even for simple computations.\n\nWe are developing radically different new architectures for efficient secure computation protocols that bypass the need for such zero-knowledge proofs. Our architectures are based on a novel principled approach to developing new secure computation protocols, with consequences to the theory and practice of modern cryptography. Our research will identify new (partial) security properties inherent in simple protocols, and study how such properties can add up to strong security guarantees through carefully developed methods for composing protocols.\n\nSecure multiparty computation is an idea whose time has come, as evidenced by the several projects around the world engaged in translating the theoretical results to practical implementations, and the number of research projects outside cryptography that seek to exploit it. Widespread availability and deployment of secure computation protocols has the potential to be a disruptive technology, enabling new avenues of cooperation in areas with sensitive information. Apart from the technological impact, we strive to bring the advances in cryptography to a broader computer science audience, by integrating our research into graduate and undergraduate education, and outreach efforts.",1228856,30,sensitive information
Samuel Madden,,madden@csail.mit.edu,Massachusetts Institute of Technology,BD Spokes -Big Data Regional I,BD Spokes: SPOKE: NORTHEAST: Collaborative: A Licensing Model and Ecosystem for Data Sharing,"Sharing of data sets can provide tremendous mutual benefits for industry, researchers and nonprofit organizations. For example, companies can profit from the fact that university researchers explore their data sets and make discoveries, which help the company to improve their business. At the same time, researchers are always on the search for real world data sets to show that their newly developed techniques work in practice. Unfortunately, many attempts to share relevant data sets between different stakeholders in industry and academia fail or require a large investment to make data sharing possible. A major obstacle is that data often comes with prohibitive restrictions on how it can be used (e.g., requiring the enforcement of legal terms or other policies, handling data privacy issues, etc.). In order to enforce these requirements today, lawyers are usually involved in negotiation the terms of each contract. It is not atypical that this process of creating an individual contract for data sharing ends up in protracted negotiations, as both sides struggle with the implications and possibilities of modern security, privacy, and data sharing techniques. Worse, fears of missing a loophole in how the data might be (mis)used often prevents many data sharing efforts from even getting started. To address these challenges, our new data sharing spoke will enable data providers to easily share data while enforcing constraints on the use of the data. This effort has two key components:(1) Creating a licensing model for data that facilitates sharing data that is not necessarily open or free between different organizations and (2) Developing a prototype data sharing software platform, ShareDB, which enforces the terms and restrictions of the developed licenses. We believe these efforts will have a transformative impact on how data sharing takes place. By moving data out of the silos of individuals and single organizations and into the hands of broader society, we can tackle many societally significant problems.\n\nThis new data sharing spoke will enable data providers to easily share data while enforcing constraints on the use of the data. Many services and platforms that provide access to data sets exist already today. However, these platforms generally promote completely open access and do not address the aforementioned issues that arise when dealing with proprietary data. Thus, the effort has three key components: (1) Creating a licensing model for data that facilitates sharing data that is not necessarily open or free between different organizations, (2) developing a prototype data sharing software platform, ShareDB, which enforces the terms and restrictions of the developed licenses, and (3) developing and integrating relevant metadata that will accompany the datasets shared under the different licenses, making them easily searchable and interpretable. To ensure that the developed tools and licenses are useful, the project will form the Northeast Data Sharing Group, comprising many different stakeholders to make the licensing model widely accepted and usable in many application domains (e.g., health and finance). The intellectual merit of this proposal is to design a licensing model and a data sharing platform that is widely accepted and usable as a template in many different domains. While there exist other efforts to enable data sharing (e.g., Creative Commons), they focus on the case where the data owner is willing to openly share the data on the Internet. This licensing model and the ecosystem is different since it allows data owners to enforce certain requirements stated in a data sharing agreement (e.g., on who is allowed to access the data) and also provides tools to make data sharing of sensitive information safe. The licenses and software we propose to investigate will make it easier for organizations to open up their data to the appropriate organizations, while maintaining the ability to ensure it is protected, that access is revocable, and that access controls and audit logs are maintained.",1636766,15,sensitive information
Jason Dedrick,,jdedrick@syr.edu,Syracuse University,"Science of Organizations, Secure &Trustworthy Cyberspace",EAGER: Data Privacy for Smart Meter Data: A Scenario-Based Study,"Smart electric meters comprise one key technology element in an overall strategy to modernize the nation's energy infrastructure. Smart meters capture data on household energy usage at frequent intervals and transmit those data to utility companies, who use the data to automate meter reading and billing, detect and respond to outages, and manage grid operations. Data collected over time can be used to forecast demand, understand customer behavior and develop new service and pricing plans. In the long run, these data can drive forecasting and control models that allow utilities to respond rapidly to fluctuations in power demand with compensatory load control strategies. This capability can reduce the magnitude of peak demand, thereby reducing both infrastructure costs and the consumption of fossil fuels that power peak generation facilities. Despite these advantages, smart meter data also appear to create powerful customer privacy concerns that may inhibit the adoption of smart meter technology by utilities. To address privacy concerns pertaining to the smart meter data, this project proposes three studies: Study 1 uses public data on electricity usage to develop a set of privacy scenarios that will be tested in focus groups to explore consumer privacy concerns. Study 2 involves a quasi-experiment to assess key dimensions of the scenarios and help identify which scenario has the highest level of acceptability to consumers. Study 3 involves one-on-one discussions with utility company representatives to develop a toolbox of communication methods and content that balance consumers' privacy concerns with the goals and constraints of utilities. \n\nThe current theoretical model of privacy and technology, based on an information boundary framework, posits that consumers' willingness to share private information is rooted in the nature of the relationship with the party with whom the information is shared. Although a number of researchers have applied this information boundary framework to intra-organizational situations, the framework has not been tested in the context of smart meter data privacy. In the proposed sequence of three studies, this project gathers data from informants with the intention of understanding whether the information boundary framework is applicable to examining the relationships between consumers and regulated monopolies. The proposed project provides insights into public attitudes and concerns, as well as industry practices and policies regarding privacy of smart meter data. Through direct interaction with utility companies, as well as dissemination at industry conferences, in trade journals and through the media, the researchers will share their findings with practitioners, regulators and others who can create solutions that give customers confidence about the protection of their data.",1447589,3,"public data, private information"
Thomas Eisenbarth,,teisenbarth@wpi.edu,Worcester Polytechnic Institute,Secure &Trustworthy Cyberspace,TWC: Small: MIST: Systematic Analysis of Microarchitectural Information Leakage on Mobile Platforms,"Smart phones have permeated all facets of our lives facilitating daily activities from shopping to social interactions. Mobile devices collect sensitive information about our behavior via various sensors. Operating systems (OS)enforce strict isolation between apps to protect data and complex permission management. Yet, apps get free access to hardware including CPU and caches. Access to shared hardware resources result in information leakage across apps. Microarchitectural attacks have already proven to succeed in stealing information on PC and even on virtualized cloud servers. This project (MIST) quantifies the vulnerability of mobile platforms to microarchitectural attacks and develops countermeasures. \n\nMIST systematically explores which resources enable these attacks on the mobile platform, identifies mechanisms that are essential to an attack, and quantifies the amount of information obtainable from given resources. By making use of machine learning techniques, methods for detecting malicious code exploiting microarchitectural leakage are developed. To remedy microarchitectural attacks, MIST explores countermeasures that can manage resources in a way that they are no longer exploitable by side channels. In addition, tools that help app developers prevent leakage when writing code processing sensitive information are provided. The studied detection and prevention techniques apply to a wide range of interactions as experienced in mobile computing today. Thereby, MIST helps to secure personal information stored on mobile platforms with immediate benefits to virtually all mobile platform users. Many of the envisioned Internet of Things hubs build on the same platforms, further increasing the impact of this research.",1618837,3,"sensitive information, personal information"
Sherry Emery,,emery-sherry@norc.org,University of Illinois at Chicago,INFO INTEGRATION & INFORMATICS,III: Small: Collaborative Research: Reducing Classifier Bias in Social Media Studies of Public Health,"Social media creates a new opportunity for public health research, giving greater reach at lower cost than traditional survey methods. Online content offers several potential advantages over traditional survey data; one can in real-time measure how behaviors and attitudes change in response to rare events such as legal changes, new products, and marketing campaigns. Machine learning techniques for classification can be used to tailor interventions that improve health outcomes while minimizing costs. However, online content is not a random sample, potentially biasing the outcomes. This proposal develops techniques to overcome this problem, enabling effective use of publicly available social media data for public health research. The approaches are evaluated against a traditional survey-based approach to evaluate end-to-end effectiveness in a real-world public health scenario, determining effectiveness of smoking cessation campaigns.\n\nThe project builds on well-grounded statistical approaches to eliminate classifier bias. Key innovations are extending this to the high-dimensional, noisy domain of textual social media data (specifically Twitter), robustness to confounding variables, and scalable methods to identify comparison groups. Noisy data will be addressed through advancing multiple imputation techniques. The project will develop a model-based approach to identifying comparison groups that addresses confounding variable issues. The methods will be evaluated in the context of an actual public health study of smoking cessation, based on historical Twitter data and traditional surveys conducted before and after a CDC campaign as well as a survey of smokers on perceived risk factors of e-cigarettes.",1524750,19,"social media data, twitter data"
Mor Naaman,,mor.naaman@cornell.edu,Cornell University,INFO INTEGRATION & INFORMATICS,III: Small: Collaborative Research: Detection and Presentation of Community and Global Event Content from Social Media Sources,"Social media sites such as Twitter, Facebook, YouTube, and Flickr host an ever-increasing amount of user content captured or produced in association with real-world events, from presidential inaugurations to community-specific events. Unfortunately, the existing tools to find, organize, and present the social media content associated with events are extremely limited. This project will address critical end-to-end information processing and presentation methods that will transform public access to real-world event information from social media sources. In particular, this work will increase the digital presence of currently underrepresented communities and address their information needs: for these communities, events are often not covered by mainstream media, but are increasingly available on social media services. As a distinctive characteristic, the project will draw on several research areas, namely, information retrieval and databases, human-computer interaction, and social media, thus contributing to educating multidisciplinary students. The PIs will continue to include undergraduate students and students from underrepresented populations in the research.\n\nThe project will result in new data analysis and visualization techniques for event-based information tasks, addressing human and computational factors in social media systems to handle vast collections of noisy, user-contributed content of widely varying structure and quality. To enable effective browsing, search, and presentation of event content, this work will use the wealth of social media documents to address several fundamental problems. The first\nproblem is the detection of events in repositories of social media content. Such content, increasingly posted by users in real time, is noisy and highly heterogeneous, but can help in the early detection of a wide range of events of all sizes. The second problem is the comprehensive identification of content related to detected or known events, currently fragmented across social media sites and often hard to find and collect. The third problem is content presentation, which requires the development of novel presentation and visualization techniques for social media event content. The amount of content\navailable even for a single event can be overwhelming and hinder data exploration and sense-making. \n\nThe project will create new tools that will transform the viewing experience of the event information. These tools will allow users to create and share personalized views of the event data as a story-telling practice. Finally, as a main outcome, the data used in the research will be made available to other researchers whenever possible. Moreover, another main outcome will be a publicly available prototype system based on this research, designed to help connect computing and information science challenges to the activities and natural interests of a diverse set of users.",1444493,19,flickr
Prateek Mittal,,pmittal@princeton.edu,Princeton University,Secure &Trustworthy Cyberspace,CAREER: Trustworthy Social Systems Using Network Science,"Social media systems have transformed our societal communications, including news discovery, recommendations, societal interactions, E-commerce, as well as political and governance activities. However, the rising popularity of social media systems has brought concerns about security and privacy to the forefront. This project aims to design trustworthy social systems by building on the discipline of network science. First, the project is developing techniques for analysis of social media data that protect against risks to individual privacy; new research is needed since existing approaches are unable to provide rigorous privacy guarantees. Second, the project is developing new approaches to mitigate the threat of ""fake accounts"" in social systems, in spite of attempts by the creators of those accounts to elude detection. Both deployed and academic approaches remain vulnerable to strategic adversaries, motivating the development of novel defense mechanisms based on network science. The findings and new designs from this research will directly impact the security and privacy of a broad class of social network users.\n\nThe private network analytics thrust builds on the ideas of differential privacy, ensuring sufficient uncertainty in results to hide individual relationships. The project introduces dependent differential privacy, which protects against disclosure of information associated with an individual, as well as mutual information privacy, an entropy-based measure. The Sybil mitigation thrust is based on the idea of adversarial machine learning: the creators of fake accounts are presumed to adapt their mechanisms to changing detection approaches. This work exploits new features, such as temporal dynamics of the network, to address this problem. Finally, the project aims to integrate the research with an educational initiative for developing pedagogical approaches and content for trustworthy social systems.",1553437,3,social media data
Apu Kapadia,,kapadia@indiana.edu,Indiana University,"SPECIAL PROJECTS - CISE, Secure &Trustworthy Cyberspace",CAREER: Sensible Privacy: Pragmatic Privacy Controls in an Era of Sensor-Enabled Computing,"Social networking and sensor-rich devices such as smartphones are becoming increasingly pervasive in today's society. People can share information concerning their location, activity, fitness, and health with their friends and family while benefiting from applications that leverage such information. Yet, users already find managing their privacy to be challenging, and the complexity involved in doing so is bound to increase. Usable techniques are required to enable people to effectively manage the dissemination of their private information as sensed by their mobile devices and sensors in their environment. \n\nThis project explores `sensible' privacy controls and feedback mechanisms that allow people (or automated mechanisms acting on their behalf) to respond to unanticipated patterns and actual uses of their information in a way that is usable and intuitive. This approach has two advantages: 1) people need only care about the subset of data and usage scenarios that have the potential to violate their privacy, thus reducing the amount of data to which they must regulate access; and 2) people make better decisions concerning such access when these decisions are made in a context where they know how their data is being used. \n\nSensible privacy mechanisms can have a profound and positive societal impact by not only helping people control their privacy, but also potentially increasing their participation in sensor-enabled computing because of this added control. This project firmly integrates education into the research through research experiences for underrepresented groups and the development of course modules on privacy at the undergraduate and graduate levels. \n\n",1252697,3,private information
Michail Polychronakis,,mikepo@cs.stonybrook.edu,Columbia University,Secure &Trustworthy Cyberspace,TWC: Small: Virtual Private Social Networks,"Social networking services have been transformed from one-stop websites, to social interaction platforms deeply integrated with third-party websites, applications, and even operating systems. As prominent examples, social plugins such as Facebook's Like and Google's +1 buttons enable websites to offer personalized content and allow their visitors to seamlessly share and interact with their social circles, while Facebook and Twitter support is already integrated in iOS 6. These social features offer multifaceted benefits to both users and content providers, and have driven their widespread adoption across the web and the mobile application ecosystem. However, this increasing integration has raised concerns about the implications of these social features to user privacy, as they enable social networking services to track a growing part of their members' activity, including their browsing histories, locations, and communications.\n\nThe research in this project seeks to address these privacy concerns by exploring a novel design for privacy-preserving virtual private social networks, which fulfills two seemingly contradicting requirements: it protects user privacy by minimizing the transmission of user-identifying information to the social networking platform, while preserving all existing functionality by delivering the same personalized content. The main insight of this approach is to shift content personalization from a server-side to a client-side process, by decoupling the retrieval of potentially sensitive social information from the presentation of personalized content that uses that information. The PIs are developing a personalized ""information overlay"" that prefetches information from a user's social circle independently of third-party accesses, and keeps this information consistent across the user's devices. The outcomes of this research effort are expected to significantly improve the privacy of members of social networking services, without degrading the current personalization experience to which they have grown accustomed.",1318415,3,private social
Jaideep Vaidya,,jsvaidya@rbs.rutgers.edu,Rutgers University Newark,Secure &Trustworthy Cyberspace,TWC SBE: Medium: Collaborative: Building a Privacy-Preserving Social Networking Platform from a Technological and Sociological Perspective,"Social networks provide many benefits, but also give rise to serious concerns regarding privacy. Indeed, since privacy protections are not intrinsically incorporated into the underlying technological framework, user data is still accessible to the social network and is open to misuse. While there have been efforts to incorporate privacy into social networks, existing solutions are not sufficiently lightweight, transparent, and functional, and therefore have achieved only limited adoption. This project develops a privacy-preserving social network (Trusted-Space) where user data are protected from the social network itself, other social network users, and advertisers. The project synthesizes solutions from a technological and sociological perspective to ensure that all of the required functionalities for both users and advertisers to participate effectively in the social network are available. The project develops compact data representations and usability driven functionalities that are privacy-preserving. The project also evaluates people's expectations about the implications of their own actions on their privacy and the future behavior of the algorithms, towards creating a simple, transparent, and predictable environment to facilitate widespread adoption. \n\nSince social networks permeate modern life, the project will have significant impact through enabling the incorporation of usable privacy technology into the social network fabric. The project also cultivates the integration of research and education, by providing opportunities for research to undergraduates and graduates, while increasing the awareness of privacy issues among the general public through outreach activities.",1564034,3,user data
Bilal Khan,,bkhan@jjay.cuny.edu,CUNY John Jay College of Criminal Justice,"DATANET, Data Infrastructure",TOWARDS A CELLPHONE-BASED INFRASTRUCTURE FOR HARVESTING DYNAMIC INTERACTION NETWORK DATA,"Social networks--that is, collections of individuals linked by inter-relationships--are by now well-known important factors in understanding social and behavioral phenomena. Missing from prior considerations is the fact that these relationships represent sequences of dynamic short-term interactions, where each interaction reflects the concerns of a particular environment. Present survey-based research methods are not able to capture accurate, detailed data on interactions within social networks over long timescales, and yet, such data would likely lead to significant new insights into the contextualized behavior of individuals, the way in which communities emerge, and the flow of information/transformation within human societies. \n\nThis project brings together a multi-disciplinary team from sociology, anthropology, criminology, psychology, public health, and education, to identify critical research problems whose resolution would be advanced by the availability of dynamic interaction data. The research requirements of these diverse disciplines will be synthesized by computer scientists, leading to the design of a new cellphone-based system which will be capable of revealing the form and evolution of dynamic interaction networks in a privacy-preserving manner. To evaluate the design, a small-scale prototype will be developed, and applied in a case study exploring the impact of dynamic student interactions on individual academic performance. Potential future iterations of this tool will ensure the confirmed willing participation of study participants by the inclusion of a component requirement to 'opt in' in order to take part in the automated data-collection process.\n\nBy leveraging recent advances in proximity-based network technology and data science to yield the design of a general-purpose cellphone-based system that can provide much-needed access to dynamic interaction network data, this project furthers a deeper understanding of human societies by enhancing long-term research capabilities across a range of social, behavioral and economic sciences. The case study will validate the system design, while also providing new insights on patterns of interaction in social networks, and their potential impact on student achievement.",1338485,9,interaction data
David Gleich,,dgleich@purdue.edu,Purdue University,INFO INTEGRATION & INFORMATICS,III: Small: Spectral clustering with tensors,"Society is awash in complex data that is recorded by high-resolution sensors and through our traces of online activity. One of the key challenges in turning this data into actionable insight is finding hidden groups in these data. For instance, finding similar types of land based on its spectral composition, or finding groups of like-minded people in a social network. While there are a variety of methods to solve these problems that are known already, there are none that take advantage of the multi-dimensional features of the data in an integrated way. The work in this proposal will provide an entirely new type of data clustering, or grouping. The method exploits the rich features of these complex modern datasets. Having access to such a method will help society improve its understanding of the results of complex scientific experiments, produce new insights into the common patterns in social networks, and extract valuable information from large databases of sensor information. The methods developed will be general, and thus, they will have broad applicability that will enrich our capability to use data to benefit society.\n\nSpectral clustering is a technique to identify cohesive groups in a database based on simple pairwise relationships between the items. In a social network, for example, people are connected based on pair-wise friendship relationships. This two-way, or two-dimensional, association between people does not capture the richness of real-world social interactions that often involve multiple people. These higher-order interactions among groups will be represented as tensors and hyper-graphs in this project. For instance, an interaction among three people corresponds to a hyper-edge, which will be represented by a three-dimensional tensor. Thus, this project will investigate novel formulations of the spectral clustering problem that work on multi-dimensional relationships represented with tensors. These new methods and their variations will be evaluated based on the insights that they provide into (i) hyper-spectral imaging data, where each pixel has a large number of frequencies measured; (ii) in social network data, where groups of interactions create higher-order relationships; and (iii) in complex simulation data, where parametric variation creates multi-dimensional data and relationships based on space, time and parameters. This project will also investigate fast algorithms to identify the clusters based on the new formulations of spectral clustering, as well as relations between these algorithms and emerging work in computational topology.\n\nAll of the software developed as part of this project will be released as open source software in order to maximize its impact. The investigator will organize tutorials on these new formulations at major conferences to attract additional applications.\n\nFor further information see the project web site at: https://www.cs.purdue.edu/homes/dgleich/tensors",1422918,19,social network data
Long Lu,,l.lu@northeastern.edu,SUNY at Stony Brook,"SPECIAL PROJECTS - CISE, Secure &Trustworthy Cyberspace",TWC: Small: STRUCT: Enabling Secure and Trustworthy Compartments in Mobile Applications,"Society's dependence on mobile technologies rapidly increases as we entrust mobile applications with more and more private information and capabilities. Existing security research follows a common threat model that treats apps as monolithic entities and only captures attack surface between apps. However, recent research reveals that app internal attacks are emerging quickly as complex entities with conflicting interests are commonly included inside a single app to allow for rich features and fast development. \n\nThis project, known as STRUCT, systematically investigates app compartmentalization as a novel and general approach to mitigating the critical yet unaddressed internal threats of apps. It applies this approach to major mobile platforms via solving four challenging and interesting research problems: (1) Deriving principles and models for designing intra-app security mechanisms; (2) Building compiler toolchains for automatically and securely compartmentalizing apps; (3) Building system-level enforcement mechanisms for open platforms; (4) Building app-level system-agnostic enforcement mechanisms for closed platforms. Solutions to these challenges together form a foundation to the design and implementation of intra-app security isolation and policy enforcement, which is currently nonexistent but in high demand. \n\nSTRUCT has its broader impact in fostering a new direction in mobile security research and education as well as increasing society's adoption of mobile technology in security-sensitive scenarios.",1421824,3,private information
Milind Buddhikot,,milind.buddhikot@alcatel-lucent.com,Lucent Technologies Bell Laboratories,"INFORMATION TECHNOLOGY RESEARC, RES IN NETWORKING TECH & SYS",NeTS: Medium: Collaborative Research: Detecting and Localizing Spectrum Offenders Using Crowdsourcing,"Software defined radio (SDR) is emerging as a key technology to satisfy rapidly increasing data rate demands on the nation's mobile wireless networks while ensuring coexistence with other spectrum users. When SDRs are in the hands and pockets of average people, it will be easy for a selfish user to alter his device to transmit and receive data on unauthorized spectrum, or ignore priority rules, making the network less reliable for many other users. Further, malware could cause an SDR to exhibit illegal spectrum use without the user's awareness. The FCC has an enforcement bureau which detects interference via complaints and extensive manual investigation. The mechanisms used currently for locating spectrum offenders are time consuming, human-intensive, and expensive. A violator's illegal spectrum use can be too temporary or too mobile to be detected and located using existing processes. This project envisions a future where a crowdsourced and networked fleet of spectrum sensors deployed in homes, community and office buildings, on vehicles, and in cell phones will detect, identify, and locate illegal use of the spectrum across a wide areas and frequency bands. This project will investigate and test new privacy-preserving crowdsourcing methods to detect and locate spectrum offenders. New tools to quickly find offenders will discourage users from illegal SDR activity, and enable recovery from spectrum-offending malware. In short, these tools will ensure the efficient, reliable, and fair use of the spectrum for network operators, government and scientific purposes, and wireless users. New course materials and demonstrations for use in public outreach will be developed on the topics of wireless communications, dynamic spectrum access, data mining, network security, and crowdsourcing.\n\nThere are several challenges the project will address in the development of methods and tools to find spectrum offenders. First, the project will enable localization of offenders via crowdsourced spectrum measurements that do not decode the transmitted data and thus preserve users? data and identity privacy. Second, the crowd-sourced sensing strategy will implicitly adapt to the density of traffic and explicitly adapt to focus on suspicious activity. Next, the sensing strategy will stay within an energy budget, and have incentive models to encourage participation, yet have sufficient spatial and temporal coverage to provide high statistical confidence in detecting illegal activity. Finally, the developed methods will be evaluated using both simulation and extensive experiments, to quantify performance and provide a rich public data set for other researchers.",1563928,28,public data
Daniel Kifer,,duk17@psu.edu,Pennsylvania State Univ University Park,SPECIAL PROJECTS - CISE,CAREER: An Axiomatic Basis for Statistical Privacy,"Statistical privacy is the art of releasing the datasets that provide useful information about population trends without revealing private information about any individual. Recent high-profile attacks on datasets released by AOL and Netflix demonstrate the need for rigorous application-specific privacy definitions to guide the anonymization of data. The goal of this project is to develop modular components, called privacy axioms, that can be chained together to create customized privacy definitions and anonymized data for statistical privacy applications. Such modularity can enable data curators without extensive expertise in statistical privacy to release anonymized data while providing privacy guarantees that are more interpretable and reliable.\n\nIntellectual merit: this project is designed to provide a unifying framework for statistical privacy that can bring about a deeper understanding of privacy issues and provide guidance for the safe anonymization and release of sensitive data. In addition to theoretical developments, this research plan also targets specific existing applications at Penn State and the U.S. Census Bureau.\n\nBroader impact: the systematic approach to privacy pursued by this project can enable access to and analysis of anonymized data in domains where access to data is otherwise heavily restricted. \nThis project aims to build upon the investigator's prior experience with outreach programs such as the Summer Research Opportunities Program (SROP) by involving undergraduates in the proposed research. To prepare students for future work that requires analysis of anonymized data, this research is also being integrated into machine learning courses at Penn State.\n\nFor further information see the project web site at the URL:\nhttp://www.cse.psu.edu/~dkifer/axiomatizingprivacy.html",1054389,30,"private information, anonymized data"
Fengjun Li,,fli@ittc.ku.edu,University of Kansas Center for Research Inc,"INFORMATION TECHNOLOGY RESEARC, INFO INTEGRATION & INFORMATICS",RAPID: III: Data Collection and Risk Evaluation Learning in Identifying High Risk Ebola Subpopulations for the Intervention and Prevention of Large-scale Ebola Virus Spreading,"The 2014 Ebola epidemic is the largest in history, affecting multiple countries in West Africa, and now impacting the US and other countries worldwide. The US Center for Disease Control and Prevention (CDC) and partners are taking precautions to prevent the further spread of Ebola within the United States. There is a lack of public understanding of the risks associated with Ebola; witness the inconsistently applied local responses (such as quarantines) that do not match CDC recommendations. This project will develop technology to enable individuals to evaluate risks associated with their own past and planned activities and travel. This will both enable those at risk to take appropriate action, and reduce unwarranted demand on the healthcare system by reassuring those whose activities have not placed them at risk. This project will use data gathered from the CDC and other public sources to develop risk models, and develop a mobile app that will use this data along with the user's own location and activity history and plans to report individual risk to the user. An individual's data never leaves their own device, ensuring personal privacy. The resulting lessons learned will ease the process of developing similar individualized risk assessment tools for future epidemics, providing long-term benefits beyond the Ebola virus epidemic.\n\nThe research will address three main issues. The first is focused crawling of structured (CDC Contact Tracing reports) and unstructured (social media, web blogs) information on time, location, and activities of Ebola patients. A second research challenge is patient activity modeling: Given the returned information, developing a time/space/activity model determining the risk of the patient acting as a transmission agent. Finally, the project will develop a mobile app that tracks time, location, and activities of the mobile device user, and retrieves the patient activity models developed from public data to determine if the user is at risk of infection. This is a complex problem, as the data may be non-specific and require inferential techniques to estimate risk (e.g., being in the same time/location as a transmission agent poses very different risk if the location is a sports stadium as opposed to a restaurant); the project will develop ontologies for activities to use in estimating risk. The project will use expert opinion to seed regression models for risk assessment. Lessons learned from this project will also identify challenges for future research in information integration, risk analysis, machine learning, and privacy preserving technologies.",1513324,19,public data
Casimer DeCusatis,,casimer.decusatis@marist.edu,Marist College,"Cyber Secur - Cyberinfrastruc, Campus Cyberinfrastrc (CC-NIE)",CC*DNI Integration (Area 4): Application Aware Software-Defined Networks for Secure Cloud Services (SecureCloud),"The Application-Aware, Software-Defined Networks for Secure Cloud Services (SecureCloud) project designs and builds an innovative cyberinfrastructure for cloud computing networks and enhancing data security without degrading network performance. SecureCloud is a response to the growth of new, highly sophisticated cybersecurity threats that have accompanied the emergence of cloud computing and placed national cybersecurity infrastructure at risk. Cyber attacks are responsible for a range of recent problems ranging from theft of personal information and interfering with financial transactions to threatening national security and government operations.\n\nSecureCloud is studying 1) how to minimize the impact of multi-component malware infiltration and 2) how to improve threat visibility and response time. The project develops, tests, and deploys a fully automated security system that is implemented throughout the cloud infrastructure. The system is prototyped in a software-defined networking (SDN) test bed at Marist College, and first deployed into production across regional and New York State-wide networks. SecureCloud uses SDN to dynamically re-provision network resources in response to impending attacks. The infrastructure includes control of virtual network functions such as virtual router/firewall combinations, traffic flow segregation in metropolitan area networks, and integration of threat intelligence from other data sources using network traffic analytics. The overarching goals are to 1) enable new network functionality to protect networks from malware and denial of service attacks; 2) improve visibility and response time to security threats, 3) quarantine infected computers, and 4) contribute new open source software to the community for integration with a wide range of applications.",1541384,16,personal information
David Kempe,,dkempe@usc.edu,University of Southern California,INFO INTEGRATION & INFORMATICS,"III: Small: Robustness in Social Network Analysis: Models, Inference, and Algorithms","The burgeoning field of ""Social Network Analysis"" focuses on extracting useful insights from such social network data. Implemented or envisioned applications range from learning about the nature and driving forces behind human interactions, to targeted product or activity recommendations and even homeland security. Contrary to other networks, such as transportation or computer networks, massive uncertainty and noise are practically always associated with social network data: data pertaining to individuals are often not observable, or are observed incorrectly. The primary goal of this project is to understand the risks and implications of such noisy data, and to design network analysis algorithms that are significantly more robust to noise and missing data. Given the importance that mathematical models play in social networks analysis, a closely related thread of the project is to analyze the fit between typical social network models and real-world data, in particular regarding high-level connectivity properties. The project website will be used to disseminate research prototypes and data that are collected as part of the project.\n\nSpecifically, three connected research thrusts that integrate the PIs' expertise in machine learning and theoretical computer science will be explored: (1) How well do standard random graph models fit real-world social network data, in particular with regard to expansion and spectral properties? Since the answer likely is ""poorly,"" how well do modifications based on requiring local or global structure remedy this problem? (2) What is the impact of missing observations of diffusion or activation processes on the inferred social networks when learning from some contagious behavior? How can this impact be mitigated by algorithms that take the possibility of missing data into account? (3) If social network data are observed with significant (and possibly non-random) noise, under what conditions can stability of an algorithmic output be ensured? How ""obvious"" does the right answer have to be to not get obscured by noise in the data? Can ""obvious"" answers be found more efficiently? The proposed research has the potential to impact the way in which social network inference and optimization are addressed. The PIs are committed to a suite of activities, among them inclusion of undergraduate students in the proposed research and outreach to local high school students, for broader impacts.",1619458,19,social network data
Phillip Rogaway,,rogaway@cs.ucdavis.edu,University of California-Davis,Secure &Trustworthy Cyberspace,SaTC: CORE: Small: Crypto-for-Privacy,"The collection, retention, and misuse of private information can pose threats to individuals, institutions, and collective values. Cryptography can be a useful tool to help address these concerns, enhancing our security and our ability to feel secure. Historically, however, this potential benefit has been largely unrealized, perhaps because most of the research in cryptography have gone to securing electronic commerce and to investigating foundational questions in the field. Cognizant of this, the PI's research aims to develop and popularize scientific work in cryptography that is more directly connected to securing people's communication. The work aims to be as scientifically well founded as the most rigorous theory-focused cryptography, but to be more directly oriented towards improving people's communication security and preserving for the individual some protected, private space.\n\nThe research will follow the traditions of practice-oriented provable security, which deals with definitions, practical protocols, and concretely analyzed proofs. The first aim is to treat constructs like onion routing and anonymous key-distribution within this framework. The goal is to have clear, quantitative, and self-contained definitions, protocols, and proofs for privacy problems that are well-known, particularly to practitioners, but that lack satisfactory foundations. Next, the research will investigate a new approach for secure messaging: communications-efficient messaging, in the star topology, with an untrusted server as the hub. Senders send their messages to a server, and receivers pluck their messages from it, but neither the server nor a global adversary will know who has sent what to whom. Finally, the research will study the privacy and authenticity of secret-sharing schemes, including deterministic secret-sharing of high-entropy secrets. This work will inform the development of a state-of-the-art secret-sharing tool for at-risk stakeholders. Some of the work will be facilitated by a new definitional paradigm, oracle silencing, in which a nave game-based cryptographic definition is automatically modified, using a notion's correctness condition, to withhold credit for trivial adversarial wins.",1717542,3,private information
Alexander Withers,,alexw1@illinois.edu,University of Illinois at Urbana-Champaign,Software Institutes,SI2-SSE: AttackTagger: Early Threat Detection for Scientific Cyberinfrastructure,"The cyber infrastructure that supports science research (such as the cyberinfrastructure that provides access to unique scientific instrumentation such as a telescope, or an array of highly distributed sensors placed in the field, or a computational supercomputing center) faces the daunting challenge of defending against cyber attacks. Modest to medium research project teams have little cyber security expertise to defend against the increasingly diverse, advanced and constantly evolving attacks. Even larger facilities that have with security expertise are often overwhelmed with the amount of security log data they need to analyze in order to identify attackers and attacks, which is the first step to defending against them. The challenges of the traditional approach of identifying an attacker are amplified by the lack of tools and time to detect attacks skillfully hidden in the noise of ongoing network traffic. The challenge is not necessarily in deploying additional monitoring but to identify this malicious traffic by utilizing all available information found in the plethora of security, network, and system logs that are already being actively collected. This project proposes to build and deploy, is needed in research environments, an advanced log analysis tool, named AttackTagger, that can scale to be able to address the dramatic increase in security log data, and detect emerging threat patterns in today's constantly evolving security landscape. AttackTagger will make science research in support of national priorities more secure.\n\nAttackTagger will be a sophisticated log analysis tool designed to find potentially malicious activity, such as credential theft, by building factor graph models for advanced pattern matching. AttackTagger will integrate with existing security software so as to be easily deployable within existing security ecosystems and to offload processing and computational work onto better suited components. It can consume a wide variety of system and network security logs. AttackTagger accomplishes advanced pattern matching by utilizing a Factor Graph model, which is a type of probabilistic graphical model that can describe complex dependencies among random variables using an undirected graph representation, specifically a bipartite graph. The bipartite graph representation consists of variable nodes representing random variables, factor nodes representing local functions (or factor functions , and edges connecting the two types of nodes. Variable dependencies in a factor graph are expressed using a global function, which is factored into a product of local functions. In the practice of the security domain, using factor graphs is more flexible to define relations among the events and the user state compared to Bayesian Network and Markov Random Field approaches. Specifically, using factor graphs allows capturing sequential relation among events and enables integration of the external knowledge, e.g., expert knowledge or a user profile.",1535070,8,"user profile, log data"
Margrit Betke,,betke@cs.bu.edu,Trustees of Boston University,Cyberlearn & Future Learn Tech,"INT: Collaborative Research: Detecting, Predicting and Remediating Student Affect and Grit Using Computer Vision","The Cyberlearning and Future Learning Technologies Program funds efforts that support envisioning the future of learning technologies and advance what we know about how people learn in technology-rich environments. Integration (INT) projects refine and study emerging genres of learning technologies that have already undergone several years of iterative refinement in the context of rigorous research on how people learn with such technologies; INT projects contribute to our understanding of how the prototype tools might generalize to a larger category of learning technologies. This INT project integrates prior work from two well-developed NSF-sponsored projects on (i) advanced computer vision and (ii) affect detection in intelligent tutoring systems. The latter work in particular developed instruments to detect student emotion (interest, confusion, frustration and boredom) and showed that when a computer tutor responded to negative student affect, learning performance improved. The current project will expand this focus beyond emotion to attempt to also detect persistence, self-efficacy, and the trait called 'grit.' The project will measure the impact of these constructs on student learning and explore whether the grit trait (a persistent tendency towards sustained initiative and interest) can be improved and whether and how it depends on other recently experienced emotions. The technological innovation enabling this research into the genre of broadly affectively aware instruction is Smartutors, a tool that uses advanced computer vision techniques to view a student's gaze, hand gestures, head, and face to increase the ""bandwidth"" for automatically detecting their affect. One goal is to reorient students to more productive attitudes once waning attention is recognized.\n\nThis research team brings together a unique blend of leading interdisciplinary researchers in computer vision; adaptive education technology and computer science; mathematics education; learning companions; and meta-cognition, emotion, self-efficacy and motivation. Nine experiments will provide valuable data to extend and validate existing models of grit and emotion. In particular, the team will gather fine-grained data on grit, assess the impact of tutor interventions in real-time, and contribute thereby to a theory of grit. Visual data of student behavior will be integrated with advanced analytics of log data of students' actions based on the behavior of over 10,000 prior students (e.g., hint requests, topic mastery) to provide individualized guidance and tutor responses in a timely fashion. This will allow the researchers to measure the impact of interventions on student performance and attitude, and it will uncover how grit levels relate to emotion and what impact emotions and grit combined have on overall student initiative. By identifying interventions that are sensitive to individual differences, this research will refine theories of motivation and emotion and will reveal principles about how to respond to student grit and affect, especially when attention and persistence begin to wane. To ensure classroom success, the PIs will evaluate Smartutors with 1,600 students and explore its transferability by testing it in a more difficult mathematics domain with older students.",1551572,5,log data
David Irwin,,irwin@ecs.umass.edu,University of Massachusetts Amherst,"CYBER-PHYSICAL SYSTEMS (CPS), Secure &Trustworthy Cyberspace",Breakthrough: Enhancing Privacy in Smart Buildings and Homes,"The design of smart electric grids and buildings that automatically optimize their energy generation and consumption is critical to advancing important societal goals, including increasing energy-efficiency, improving the grid's reliability, and gaining energy independence. To enable such optimizations, smart grids and buildings increasingly rely on Internet-connected sensors in smart devices, including digital electric meters, web-enabled appliances and lighting, programmable outlets and switches, and intelligent HVAC systems. However, a key barrier to the broad adoption of energy-related optimizations is that prior work has shown that Internet-connected sensors inadvertently leak sensitive private information about user behavior. For example, a high or variable home energy usage typically correlates with a home being occupied. To address the problem, this research will design low-cost, non-intrusive, privacy-enhancing techniques that reduce the sensitive information leaked through smart sensor-driven devices, while still permitting the sophisticated analytics, control, and verification necessary to enable energy optimizations for smart grids and buildings. \n\nThe research includes developing both consumer- and utility-driven mechanisms to preserve sensor-data privacy. The consumer-driven mechanisms leverage batteries, elastic appliances, noise injection, and renewable energy sources to obfuscate private information in externally visible energy usage data at low cost. The utility-driven mechanisms leverage cryptographic techniques within the devices themselves to enable utilities to implement critical electric grid optimizations, such as demand response, time-of-use billing, and fault localization, without requiring consumers to provide utilities, or other third-parties, with their raw sensor data. The research also develops an approach to controllable privacy, which enables users to control the amount of information smart devices leak to third parties. In this case, consumers voluntarily use smart devices, which are able to verify that consumers engage in some particular energy-efficient behavior without directly revealing sensitive information. The research includes implementing and evaluating the techniques in a prototype programmable building, which includes programmable smart devices, batteries, and renewable energy sources. The research and prototype provide awareness of smart grid privacy and its implications on public policy, and contribute to both graduate courses on smart grids and energy, as well as undergraduate research projects.",1505422,3,"sensitive information, private information"
Alessandro Acquisti,,acquisti@andrew.cmu.edu,Carnegie-Mellon University,Secure &Trustworthy Cyberspace,"EAGER: Collaborative: Design, Perception, and Action - Engineering Information Give-Away","The design of social media interfaces greatly shapes how much, and when, people decide to reveal private information. For example, a designer can highlight a new system feature (e.g., your travel history displayed on a map) and show which friends are using this new addition. By making it seem as if sharing is the norm -- after all, your friends are doing it -- the designer signals to the end-user that he can and should participate and share information. This research focuses on two broad themes: what are the effects of design choices on changing what users think is appropriate to share and with whom? and how do norms interact with design to impact these decisions? Understanding how disclosure decisions are made and manipulated is critical as corporate and individual interests can be quite different. This is because norm-shaping can be used for benevolent purposes, such as guiding the end-user through an unfamiliar interface, but can also be used to manipulate the end-user and cause him or her to share information he or she would have preferred to keep private. The fact that such design patterns can be used both ways makes them particularly interesting: the user has no way of inferring the designer's intent, and policy makers and well intentioned designers have no mechanism for assessing the norm-shaping properties of their design choices. This research contributes to the development of tools to study user interfaces as embodiments of social norms as well as contributing more broadly to the discourse of privacy and sharing online.\n\nThe specific research goals are to (a) identify design patterns that shape disclosure norms, (b) experimentally determine the mechanisms by which they work (e.g., how patterns modify perception of norms and thus behavior), and (c) integrate these observations into existing theoretical frameworks (e.g., the ""privacy calculus"") that model how disclosure decisions are made. The PIs plan to use experiments to identify the impact of design on the perception of social norms and subsequent information divulging behavior. The experiments combine methodologies from experimental economics with Human Computer Interaction (HCI) methods. Additionally, the PIs will test econometrically an extension of the privacy calculus model that includes a preference for norm compliance, estimating an individual's willingness to trade-off between privacy preserving behavior and compliance with sharing norms. This research will demonstrate how tools from different disciplines can be used to enhance understanding of design in privacy and HCI. The results would feed back to the privacy, economics, and HCI communities.",1537143,3,private information
Alexander Vardy,,vardy@ece.ucsd.edu,University of California-San Diego,COMM & INFORMATION FOUNDATIONS,"CCF-BSF: CIF: Small: Distributed Information Retrieval: Private, Reliable, and Efficient","The digital age is predicated on information being ubiquitous. The ability to access relevant data stored on remote servers ""in the cloud"" has become an indispensable resource in everyday lives. Numerous online services let users query public datasets for data items such as map directions, stock quotes, and flight prices, to name a few. Digital content providers also rely on user queries to identify the content desired by the user. Unfortunately, such queries have the potential to reveal highly-sensitive information *about the users*, thereby compromising their privacy. For example, institutional investors querying a stock-market database for the value of certain stocks may prefer not to reveal their interest in these stocks since it could influence their price. As another example, most people are deeply uncomfortable with exposing their media consumption diet to a centralized server that can be targeted by hacking or subpoena. It can be convincingly argued that access to such media consumption profiles can reveal the person's sexual orientation, political leanings, and cultural affiliations.\n\nA great deal of research has been devoted to methods that guarantee the security and integrity of *the data*. Much less work, however, has been devoted to protecting the privacy of *the user*. Efficient and reliable retrieval of information from distributed databases, with information-theoretic guarantees of user privacy, is the focus of this project. The relevant area of research is known as private information retrieval. While most of the existing results in this area are theoretical, a major goal in this project is to bridge the gap between the theory of private information retrieval and the practice of distributed storage. As such, potential outcomes of this investigation may extend beyond the scope of academic research, contributing to new technologies and products.\n\nPrivate information retrieval (PIR), conceived in the seminal papers of Chor, Goldreich, Kushilevitz, and Sudan over 20 years ago, has been traditionally studied in theoretical computer science and cryptography, with emphasis on the complexity of the communication between the user and the servers that store the database. While major breakthroughs have been achieved in this area over the years, the prevailing paradigm has always been that of replicating the database on several non-communicating servers. Such replication leads to a significant storage overhead, which is undesirable. Moreover, motivated by advances in coding for distributed storage, it was recently recognized that if database replication is replaced by *database coding*, the full power of coding-theoretic methods can be brought to bear on the problem. While extremely promising, this line of research is still in its infancy. The goal of this project is to follow-up on the database coding idea, and follow it through to its ultimate potential. In pursuit of this goal, the following questions are addressed:\n\n(1) What is the information-theoretic capacity of PIR? That is, what is the maximum amount of information that can be privately retrieved per downloaded bit, under various scenarios?\n\n(2) What is the optimal storage overhead of PIR? Can we achieve both privacy and efficient communication (on the download and upload) without replicating the stored data even once?\n\n(3) How can both privacy and download efficiency be maintained in the presence of impediments such as malicious or colluding servers, unsynchronized data, and/or communication errors?\n\n(4) Codes for distributed storage systems tolerate and repair node failures while making the data available to several users at once. How can we combine PIR protocols with such coding?\n\n(5) What is the best possible tradeoff between the various desirable PIR features, such as download efficiency, storage overhead, and resilience to errors/collusions/node-failures?\n\nThis proposal is a natural outgrowth of the research recently carried out by the PIs and others in this area. Prior related work will provide a springboard for rapid progress toward the ambitious research objectives of this project. Knowledge, techniques, and qualitative insights gained through this investigation are expected to contribute to the foundations of the field, and to help bridge the gap between the theory of PIR and the practice of distributed storage.",1719139,26,private information
Anna Scaglione,,Anna.Scaglione@asu.edu,Arizona State University,COMM & INFORMATION FOUNDATIONS,EAGER: The Identification of Social Systems Trust: Theory and Experimental Validation,"The effect of social network contacts is generally believed to be much stronger than either advertising or online reputation/rating systems. There are many theoretical models that try to predict how opinions spread among individuals that trust each other, but the experimental evidence to back them quantitatively in a real setting is still elusive. This project aims to close the gap between the theory of opinion diffusion and the several empirical studies that have been made recently on social media data. The novel idea is to exploit the presence of influential nodes that are ?stubborn?, i.e. agents who trust only themselves, to determine how opinions fluctuate in the network as a result of their activities and the relative trust among all agents in the social network. The mathematical insights from the theoretical models of opinion diffusion indicate that stubborn agents excite the social system in such a way that reverberations of their opinions in comments from others can allow one to fit appropriate system equations to this phenomenon. Another key insight is that using steady state models is a more robust method to match real data, since the fluctuations of opinions are not directly observable, only the individual actions and comments are.\n \nThis effort, if successful, would be akin to realizing a ?social network Radar,? capturing a tomographic image of the hidden medium of a social group mutual trust. Preliminary results in the case of the celebrated linear De Groot's opinion diffusion model applied to Facebook data show remarkably good agreement between the graph one can extract from our method and first-hand knowledge of the group analyzed. This is a high risk high payoff project as several questions still need to be answered and tested to verify the results.",1553746,26,"social media data, facebook data"
Parameswaran Ramanathan,,parmesh.ramanathan@wisc.edu,University of Wisconsin-Madison,Secure &Trustworthy Cyberspace,CPS: Synergy: Preserving Confidentiality of Sensitive Information in Power System Models,"The electric power grid is a national critical infrastructure that is increasing vulnerable to malicious physical and cyber attacks. As a result, detailed data describing grid topology and components is considered highly sensitive information that can be shared only under strict non-disclosure agreements. There is also increasing need to foster cooperation among the growing number of participants in microgrid-enabled electric marketplace. However, to maintain their economic competitiveness, the market participants are not inclined to share sensitive information about their grid with other participants. Motivated by this need for increased cyber-physical security and economic confidentiality, the project is developing techniques to obfuscate sensitive design information in power system models without jeopardizing the quality of the solutions obtained from such models. Specifically, solution approaches have been developed to hide sensitive structural information in Direct Current (DC) Optimal Power Flow models. These approaches are currently being extended to Alternating Current (AC) Optimal Power Flow models. The project is also developing secure multi-party methods where the market participants collectively optimize the grid operation while only sharing encrypted private sensitive information. Finally, the project is incorporating secure market operations in jointly solving the Optimal Power Dispatch problem without revealing sensitive private information from each participant to other participants. The techniques developed in this project have the potential to broadly impact areas beyond power systems. The general principles developed in the project can be used to mask sensitive information in many problems that can be formulated as a linear or non-linear programming optimization.",1329452,3,"sensitive information, private information"
Andrea Tapia,,atapia@ist.psu.edu,Pennsylvania State Univ University Park,Cyber-Human Systems (CHS),CHS: Small: Collaborative Research: Automating Relevance and Trust Detection in Social Media Data for Emergency Response,"The goal of this project is to develop means to improve information quality and use in emergency response, increasing the value of using messaging and microblogged data from crowds of non-professional participants during disasters. Despite the evidence of strong value to those experiencing the disaster and those seeking information concerning the disaster, there has been very little effort in detecting the relevance and veracity of messages in social media streams. The problem of data verification is one of the largest problems confronting emergency-response organizations contemplating using social media data. This research directly addresses this known problem by methods to measure relevant and verifiable information. The results of this research will have a direct pipeline to organizations involved in emergency response. Therefore the research has the potential to help organizations, which respond to emergencies, make use of large amounts of citizen-produced data, which in turn may improve the speed, quality, and efficiency of emergency response leading to better support to those who need them, and more lives saved.\n\nThis research will contribute to the field of Emergency and Disaster Studies by mapping the key decisions made during an emergency response, the information needs, type, form and flow during those decision points, and most importantly, assessing data quality and verifiable standards for each. It will also investigate relevant and verifiable identifiers (or features), provide weights, incorporate these into an analytical framework, and use the results of the analysis as input to scalable computational models. The work will design algorithms that can estimate the relevance and veracity of messages in a high-volume streaming text comprised of short messages. Given the diverse backgrounds of the team, it will contribute to the use and development of socio-technical systems theory to analyze the integration of technical and social systems. The output of the models will match the organizational needs of responding organizations.",1526678,20,social media data
Huan Liu,,hliu@asu.edu,Arizona State University,INFO INTEGRATION & INFORMATICS,III: Small: Transforming Feature Selection to Harness the Power of Social Media,"The growth of social media data in size and variety accelerates rapidly as more people use social media such as Facebook, Twitter, LinkedIn, among others. It is a massive ""treasure trove"" interesting to researchers and practitioners of different disciplines, and a great source for data mining. However, attribute-value data in classic data mining differs from social media data besides both are large-scale. Social media data are noisy, incomplete, comprised of multiple sources, and form multi-modal and and multi-attributed networks. Furthermore, such data are not independent and identically distributed (i.i.d.). These unique properties present new challenges for mining social media data. \n\nThis project investigates a novel approaches to feature selection in linked data in general and social media data in particular. Specifically, it seeks to exploit link information in supervised as well as unsupervised feature selection for social media data. Because social media data are drawn from multiple noisy, partial, or redundant sources, the proposed approach to feature selection seeks to select relevant sources and use them together to guide linked feature selection in multi-modal, multi-attributed social media.\n\nThe project lies at the confluence of feature selection, social media analysis, and data mining. The project offers an opportunity to engage students who are adept users of social media in developing computational tools that can harness the power of social media. Some broader impacts of this research include integration of social media analytics into undergraduate and graduate courses as well as student research projects; enhanced research-based training opportunities for students from under-represented groups; and powerful social media analytics tools for understanding collective behavior in social media, employing social media for crisis response and disaster relief, and studying social and political movements. The results of the project (including publications, software, etc.) will be made available through the project web site: http://www.public.asu.edu/~huanliu/projects/NSF12",1217466,30,"social media data, linkedin"
Scott Mahlke,,mahlke@eecs.umich.edu,University of Michigan Ann Arbor,COMPUTING RES INFRASTRUCTURE,CI-ADDO-NEW: MobiLab -- A Global-Scale Live Laboratory to Support Mobile Computing Science,"The importance of mobile computing is rapidly increasing but the field is held back from its potential by the difficulty of evaluating new ideas. The effectiveness of mobile computing systems depends on variable network, application, and user behaviors, many of which are only visible in large-scale systems. To enable transformative research on mobile computing and related disciplines, this project creates a global-scale live laboratory composed of mobile devices, networks, and their users that will support the validation of research ideas. This live laboratory, referred to as MobiLab, makes two key contributions: (1) it provides an online repository of real-time and archived mobile user data, including vital information such as location-specific network conditions, per-application energy usage, and user behavior and (2) through a set of software libraries and tools, it simplifies the tasks of developing, deploying and evaluating research applications. Instead of re-building applications to monitor network, application, mobile system, and user behavior, researchers can gather information online from MobiLab tools via easy-to-use application programming interfaces (APIs). The target communities include researchers in several disciplines, such as mobile systems and networks, embedded systems, network security, mobile data privacy, and social networking. \nMobiLab is suitable for both undergraduate lab-intensive introductory courses as well as graduate-level courses that expose students to mobile computing, distributed computer system design, and distributed sensing. MobiLab will also have a lasting impact on the research community by providing a central repository for mobile system research data, software artifacts created by the community, and results based on the data.",1059372,30,user data
Daniel Holcomb,,holcomb@engin.umass.edu,University of Massachusetts Amherst,Secure &Trustworthy Cyberspace,CAREER: Supply Chain Security for Integrated Circuits,"The integrated circuits (ICs) that underpin critical systems in modern society are produced by a global supply chain that involves a variety of actors in many countries. Some of the actors are trusted, but others are not. Untrusted actors give rise to supply chain threats such as counterfeit ICs of uncertain quality or the possibility of malicious changes to the function of ICs. To secure electronic systems in defense, critical infrastructure, and healthcare, it is increasingly important to secure the global IC supply chain. This research project pursues new strategies for securing ICs against supply chain threats.\n \nThis research explores a new framework for supply chain security organized around three main thrusts. The first thrust explores application-specific designs with post-fabrication programmability to prevent targeted Trojan insertion by an untrusted foundry. The second thrust of the work addresses the design side of the supply chain, and seeks new formal abstractions for reasoning about, and protecting against, entire classes of malicious circuit modifications. The third thrust addresses the distribution side of the supply chain with a low-cost authentication technique to verify the provenance of packaged ICs.\n \nThis research project has the potential to positively impact national security and well-being of individuals by helping to keep malicious parts out of critical systems such as defense aircraft and medical devices. The work supports economic competitiveness of US semiconductor companies by trying to prevent billions of dollars in annual losses from counterfeits. The project supports cybersecurity education through a variety of contest-based learning activities, and through a supply chain testbed for lab courses that supports both education and research.\n \nPublic data, tools, and design artifacts associated with this project will be made publicly available through the completion of the project at https://github.com/danholcomb/supply-chain-security.",1749845,3,public data
Akbar Siami Namin,,akbar.namin@ttu.edu,Texas Tech University,Secure &Trustworthy Cyberspace,SBE: Medium: User-Centric Design of a Sonification System for Automatically Alarming Security Threats and Impact,"The Internet has become an integral part of everyday life. The great benefits of the Internet also come with potential risks, security issues, and privacy concerns. Internet security products are usually employed to inform users about security incidents. There are three major problems using these security products: (1) these tools often overwhelm users with a great many features making their usability a serious issue, (2) these tools are not always accessible to every type of user such as those who are visually impaired, and (3) most of the Internet users are unfamiliar with various types of security threats and thus their impacts.\n\nThis project translates security warnings and threats into various forms of sounds. The introduced user-centric sonification takes a security warning and maps out the risks and consequences associated with the underlying security threat into a certain type of sound that reflects the emotional feelings of the potential risks and harm caused by the threat. The project builds a repository of sounds tagged with their emotional impacts such as fear, happiness, and sadness. The project also builds a second repository of security threats tagged with similar emotional impacts such as fear of loss of sensitive information, impersonation, and privacy exposure. The intellectual merit of this project includes introduction of a user-centric sonification framework in which a cyber threat, along with its risks, severities, and impacts to users, is mapped out to a representative sound that reflects the emotional impacts of the threat to the user. The sonified security warnings undergo extensive usability testing in order to assess the effectiveness of the sonified security threats. The broader impact of this sonification project includes introduction of two repositories of possible sounds and security attacks tagged with their emotional impacts. The constructed repositories can be useful for music and computer security industries such as Amazon, anti-virus software developers, and producers of major Web browsers. The sonification-based technology for translating security warnings to sounds can be integrated into any popular Internet Web browsers. The project will have direct impact on the development of technologies that enhance accessibility for Internet users who are visually impaired and will increase their Internet safety.",1564293,3,sensitive information
Jon Kleinberg,,kleinberg@cs.cornell.edu,Cornell University,Big Data Science &Engineering,BIGDATA: IA: Harnessing Language and Interaction Dynamics at Multiple Scales to Maximize the Benefits of Group Interaction,"The Internet has enabled new kinds of interaction among people at a wide variety of scales, ranging from small groups to global social networks. The data arising from these new forms of interaction have been challenging to integrate because of how they span this wide range of scales; yet without this integration, we cannot understand how micro-level group interactions build up to macro-scale behavior or how macro-level factors shape small group interaction. A key data-science challenge is to bridge this gap between the micro- and macro-levels; and a crucial but relatively unexplored part of this challenge resides in the lack of conceptually useful models and techniques at the intermediate scales in between. This project investigates online social interaction data at different levels of scale, and develops methods for bridging the extremes by taking into account not just the global scales but also the intermediate meso-scale interaction (constituting interactions among hundreds or thousands of participants). Developing methods for handling the meso-level of data not only poses scientifically rich questions in its own right but also allows for transformative theory building across micro- and macro-level phenomena. \n\nThe project brings together researchers from a wide range of backgrounds to develop new ways of understanding and designing online interaction across different levels of scale. The research seeks to identify new forms of sub-structure that arise as the scale of the group increases to meso-scale and on to global scale; to identify new ways of maintaining an effective flow of ideas and processes for reaching consensus when the number of participants in a discussion grows signi&#64257;cantly; and to develop techniques for addressing the increased potential for conflict and polarization when people can participate at a boundary between recognizability and anonymity. \n\nThe project team combines multiple research perspectives, with broad expertise in analyzing social interaction data and developing models of social network dynamics; opinion extraction, summarization and argument mining; modeling conversational behavior; large-scale data analysis of pragmatics of language, including persuasion and information spread; and computer-mediated communication, social computing and human-computer interaction. In addition to the underlying research questions, the project also seeks to provide new knowledge that companies and organizations building meso-scale platforms can adopt to maximize the effectiveness of their sites; to add to research infrastructure through the release of datasets, code, and working applications; to impact education through the interdisciplinary training of graduate and undergraduate students, including students from underrepresented groups; and more generally to create mechanisms that can make online interactions more productive.",1741441,2,interaction data
Aleksandar Kuzmanovic,,akuzma@northwestern.edu,Northwestern University,"RES IN NETWORKING TECH & SYS, Secure &Trustworthy Cyberspace",NeTS: Small: Endpoint User Profile Control,"The Internet has transformed from a small non-profit network into a gigantic infrastructure that creates revenues measured in billions of dollars. Unfortunately, this has created an unprecedented pressure on the end user's privacy by trackers and information aggregators, because a user's profile is used to determine which personalized content (e.g., ads, search results, recommendations, etc.) will be served to the given user. The common goal of the trackers is to gather a more perfect user profile in an attempt to increase revenues from content personalization. Such user profiling can often be misused. For example, signs of price and search discrimination have been recently reported. Others argue that content personalization, which creates the so-called 'filter bubble' effect, has an even more profound impact on the society and in particular on the future of democracy.\n\nThe PI proposes endpoint user profile control as a comprehensive approach to the above personalization-induced problems. In the PI's approach, the user has the ability and means, which this project will develop, to explicitly define and implicitly control its profile at all possible trackers at once. The PI will develop and implement endpoint user profile controller as a thin sub-layer that 'sits' between the end-user and endpoint client applications. The controller imprints the desired user profiles by leaving controlled online footprints in the public domain, e.g., by actively visiting web pages from a set of selected topics, sending search queries semantically linked with the user-defined profile etc. This allows users to submit their preferences to all possible trackers they encounter, granting the user a single point of control over their profiles. \n\nIntellectual Merit: The proposed research will address fundamental questions that are key to developing and deploying effective endpoint user profile control. The main challenges are how to effectively control numerous ad trackers at once and how to achieve this in a scalable and effective manner. The PI's focus will be on the control of (i) ad trackers, (ii) web site trackers, (iii) search engine trackers, (iv) social-network trackers, and (v) information aggregators. The PI will develop statistical methods to accurately evaluate the properties of these trackers in order to effectively control them. The proposed techniques range from statistical rule mining to developing comprehensive scoring, noise-reduction, and orthogonal probing algorithms that are necessary for effective control.\n\nBroader Impact: By empowering the user to explicitly and comprehensively control his user profile on the Internet, the system will not only help users regain their online privacy, but also enhance human liberties and promote free and open society. This will be possible to achieve without scrutinizing the online advertising industry, which is effectively funding today's Internet. The proposed system is capable of achieving this goal in a realistic and feasible way, without the adoption of a complex infrastructure. The PI plans to design and disseminate endpoint user profile controller as easy-to-use browser extensions, plug-ins, and mobile applications, which will enable effective endpoint user profile control.",1319086,3,user profile
Sanjay Srivastava,,sanjay@uoregon.edu,University of Oregon Eugene,"SOCIAL PSYCHOLOGY, Cyber-Human Systems (CHS)",Personality Reputation Formation and Network Structure on Computer Technologies,"The last decade has seen explosive growth of online social networks, which are now a pervasive part of everyday life for many people. More and more, people encounter one another for the first time through social media and make important social decisions based on those encounters. Individuals make decisions about who to hire and who to trust based on the impressions others convey on-line. People may decide with whom they will exchange information, and can even decide with whom to pursue close personal relationships based solely on online encounters. Given the multitude of outcomes, people's online social behavior likely reflects their different goals about how they wish to be seen by others, in addition to their personality. Despite the importance of understanding how these issues operate in communication technologies, most research on self-presentational goals, personality traits, and impressions is based on traditional laboratory and survey methodologies. The goal of this project is to study how users' personalities and self-presentation goals are reflected in their social media presence, how people form accurate or biased impressions of one another online, and how people make consequential social decisions based on those impressions. The project will also produce important tools for other scientists. By combining the expertise of a personality psychologist with that of a computer scientist, the project will create new methods and techniques for doing large-scale, automated studies in social psychological science and personality psychology. The outcomes of this research will also inform policy discussions on online privacy and on the use of social media in hiring, in spreading news and information online, and other important social and economic transactions. \n\nIn their project, the investigators propose to merge ""big data"" methods from computer science with rigorous laboratory methods from psychology to study personality and reputation on Twitter, a popular online social network with a large and diverse user base in the United States. Five studies focus on a series of related questions about how people convey who they are through Twitter, and how others form impressions by observing such behavior. (1) Data-driven analyses will be used to identify patterns in Twitter users' publicly available information (including profile information, network size and structure, and tweet content) to determine what important stable attributes distinguish users. (2) A sample of Twitter users will complete validated psychometric assessments of personality and self-presentation goals to see how these are reflected in the users' public Twitter data. (3) Research will examine how others form personality impressions based on such Twitter profiles, and the ways those impressions are accurate or biased. (4) Studies will examine how others make important social decisions based on their accurate or inaccurate impressions of Twitter profiles. (5) Finally, research will examine how personality and impressions affect how people decide to affiliate and form online communities. This research will produce new scientific insights about how people express themselves and interact online. It will also generate new tools for assessing personality and reputation in online settings, enabling future ""big data"" studies in psychology.",1551817,20,twitter data
A. Selcuk Uluagac,,suluagac@fiu.edu,Florida International University,Secure &Trustworthy Cyberspace,SaTC: TTP: Small: Collaborative: Privacy-Aware Wearable-Assisted Continous Authentication Framework,"The login process for a mobile or desktop device does not guarantee that the person using it is necessarily the intended user. If one is logged in for a long period of time, the user's identity should be periodically re-verified throughout the session without impacting their experience, something that is not easily achievable with existing login and authentication systems. Hence, continuous authentication, which re-verifies the user without interrupting their browsing session, is essential. However, authentication in such settings is highly intrusive and may expose users' sensitive information to third parties. To address these concerns, this project develops a novel privacy-aware wearable-assisted continuous authentication (WACA) framework. User specific data is acquired through built-in sensors on a wearable device. The user data is goes through privacy-preserving operations throughout the authentication process. This login procedure can be applied to a wide variety of existing enterprise authentication systems such as university campuses, corporate Information Technology divisions, and government agencies. Prototype deployments at Florida International University (FIU) and Florida Atlantic University (FAU), both of which serve large and diverse student populations, provide valuable feedback for future improvements. Continuous authentication and digital privacy are timely and relevant topics in today's Internet-centric always-on society. \n\nThis project exploits the ubiquitous nature of sensor-based wearables by designing an innovative usable continuous authentication mechanism. By leveraging the expertise of the project team on authentication, privacy-preservation, and machine learning, this project addresses the following problems: 1) Investigation of novel sensory features on wearable smartwatches and identification of an optimal subset of these features along with distance measures and machine-learning algorithms to strike the balance between accuracy and speed; 2) Discovery of novel privacy-preserving mechanisms based on secure noise-tolerant template generation and comparison techniques, multi-party computation, and homomorphic encryption; 3) Trade-offs between privacy and performance to optimize the scheme in terms of accuracy, efficiency, and security; 4) Security of sensor-based keystroke dynamics against some common attacks such as simple zero-effort, imitation, and more complex statistical attacks including, but not limited to, classical keyboard-only keystroke dynamics attacks; and 5) Development, testing, and deployment of the proposed framework with a rich set of users, devices, and usage context in a prototype system. The success of the WACA project will contribute to the growth of knowledge in privacy and authentication domains and to societal understanding of these matters.",1718116,3,"sensitive information, user data"
Jie Gao,,jgao@cs.sunysb.edu,SUNY at Stony Brook,RES IN NETWORKING TECH & SYS,"NeTS: Small: Geometric and Topological Analysis on Trajectory Sensing: Collection, Classification and Anonymization","The maturing of mobile devices and systems provide an unprecedented opportunity to collect a large amount of data about real world human motion at all scales. The rich knowledge contained in these data sets can have a huge impact in many fields ranging from transportation to health care, from civil engineering to energy management, from e-commerce to social networking. While the applications are paradigm-transforming, recent studies show that the trajectory data can raise serious privacy concerns in revealing personally sensitive information such as frequently visited locations or social ties. These concerns become the major hurdle in utilizing these data sets. This project systematically studies the issue of anonymizing trajectory data, from the bottom layer of trajectory sensing and data collection, to the middle layer of trajectory representation and anonymity, to the application layer of how the anonymized trajectory data can be used.\n\nBy the nature of trajectories as being time stamped sequence of points, in this project novel geometric and topological algorithms that directly work on distributed sensors collecting the trajectories are developed for achieving the objective. Queries to such decentralized sensors are made to ensure no sensitive information is released. The intellectual contribution lies in the following aspects. 1) The topological representation of trajectories, i.e., how trajectories pass around obstacles and landmarks in the domain is adopted. The topological representation is compact and descriptive, introducing novel discrete and combinatorial problems to study. 2) A novel framework is developed for distributed sensors to directly learn, classify and compare the topological types of the target trajectories, using harmonic one-forms and Hodge decomposition from algebraic topology. The new framework can substantially reduce the communication cost within the network, while maintaining the requirement of user privacy from the very beginning of sensing and data collection. 3) A family of anonymization algorithms using different ideas are developed, by altering the way to connect the time-stamped points into trajectories, by adjusting the topological resolution to reach a balance between data anonymity and utility, and by sensing and recording randomized hash data to answer popular trajectory queries. 4) The trajectory data sets are often huge, so algorithms for handling large scale trajectory data sets are developed in both centralized and decentralized settings.",1618391,28,sensitive information
Erez Zadok,,ezk@cs.stonybrook.edu,SUNY at Stony Brook,Secure &Trustworthy Cyberspace,TTP: Small: NFS4Sec: An Extensible Security Layer for Network Storage,"The Network File System (NFS) is a popular method for computers to access files across networks. The latest major version of this IETF protocol, version 4, is widely accepted and includes numerous new features to improve security, performance, and usability when used over wide-area networks. However, the NFSv4's security focus is on network-wide encryption (ensuring that user data cannot be intercepted) and user authentication (ensuring that only legitimate users can access their files); it does not address end-to-end data security (i.e., persistently stored data), does not address data integrity (malicious or benign data corruptions), and more.\n\nThis project extends NFSv4 with a security layer that allows one to develop multiple, composable plugin modules to enhance the protocol's security. This layer allows for interception of protocol requests between clients and servers to perform various useful security functions: logging access to files by users and hosts, useful for regulatory compliance reports and audits; inspecting files for malware patterns and automatically quarantining them; verifying the integrity of long-lived files against malicious changes (e.g., Trojan intrusions) and benign but serious ones (e.g., storage media degradation and hardware corruptions); detecting denial-of-service attempts and ensuring quality-of-service to legitimate users through load-balancing and redirection; automatic snapshotting and logging to allow for forensic analysis and recovery from failures and intrusions. In a cloud-based era where more data lives longer and is accessed over wide-area, insecure networks, this project helps elevate the level of security of every user's data files.",1223239,30,user data
Marco Gruteser,,gruteser@winlab.rutgers.edu,Rutgers University New Brunswick,Secure &Trustworthy Cyberspace,TWC: Small: Redesigning Mobile Privacy: Helping Developers to Protect Users,"The objective of this project is to help developers in making applications? usage of personal information transparent to mobile phone users, system integrators, and other evaluators. Recent well-publicized mobile privacy incidents have demonstrated all these parties have lost count of what information mobile devices collect, store, and transmit. A successful project would lead to improved privacy and application transparency, and would help prevent future privacy compromises. Project results could be adopted into mobile operating systems and could guide FTC policy on mobile privacy. The project includes close collaboration with industry stakeholders to facilitate dissemination of new ideas. Work will be conducted with graduate and undergraduate students, which will not only provide them with training in research methodology, but also expose them to important ethical questions surrounding privacy issues in mobile applications.\n \nThis project will aid developers and nudge them to follow privacy principles by making their usage of personal information transparent. Towards this end, this project will develop guidelines for privacy-aware system APIs that encourage developers to employ privacy-by-design techniques. For example, system APIs designed from a privacy perspective will make it easier to obtain more general information, rather than potentially more sensitive, fine-grained personal information. Through case studies with developers and users, this project is expected to lead to novel insights into the effectiveness of these techniques.",1223977,30,personal information
Geoffrey Fox,,gcf@indiana.edu,Indiana University,Secure &Trustworthy Cyberspace,TWC: Small: Secure Data-Intensive Computing on Hybrid Clouds,"The ongoing effort to move data intensive computation to low-cost public clouds has been impeded by privacy concerns, as today's cloud providers offer little assurance for the protection of sensitive user data. This problem cannot be addressed by existing cryptographic techniques alone, which are often too heavyweight to manage the computation involving a large amount of data. As a result, many computing tasks have to be run on individual organizations? internal systems whenever they touch even a very small amount of sensitive information.\n\nThe research in this project seeks practical solutions to this critical security challenge. The PIs are working on an approach to split a computing job over a hybrid-cloud platform, delegating to a public cloud the computation over public data, while keeping the computation on sensitive data within a private cloud. \n\nSpecifically, the PIs are developing a privacy-aware MapReduce system, which transparently partitions a computing job and schedules its components across the public/private clouds according to the security levels of the data involved. The system is designed to achieve high security assurance and outsource most of its workload when possible, at small computational and communication overheads. It includes support for analyzing and transforming the code for legacy jobs as well as developing new jobs. We are also working to extend these techniques to facilitate other secure work-flow processing over hybrid clouds. This research involves industry collaborators and contributes to secure processing of a wide range of computing jobs, from commercial data analysis, to DNA analysis, to intrusion detection.",1223495,30,"public data, user data"
Lakshmish Ramaswamy,,laks@cs.uga.edu,University of Georgia Research Foundation Inc,CyberSEES,CyberSEES: Type 1: Meghdoot: A Multi-Cloud Infrastructure for Enhancing Sustainability via Effective Monitoring of Inland Waters and Coastal Wetlands,"The overall goal of the project is the design and prototype implementation of a multi-cloud computing framework that seamlessly integrates community observations, remote sensing measurements, and advanced multimedia analytics for effective environmental monitoring. This infrastructure, called Meghdoot, will be employed for early detection of salt marsh stress in coastal areas and cyanobacterial harmful algal blooms in inland waters. This framework will harness multiple clouds including community clouds, sensor clouds, and computational clouds to develop efficient and timely event detection strategies. In addition, efficacious mechanisms for motivating and enabling community members to contribute high-quality content for enhanced environmental monitoring will be investigated. The first research thrust of the proposed project is to invent mechanisms to effectively leverage community clouds for monitoring lakes and salt marshes. In this community-as-sensors paradigm, data produced by individuals of the community (i.e., blogs, images, tweets) in online social media platforms (Facebook, Twitter, Flickr, etc.) will be used to generate trustworthy, actionable information. This, in turn, will act as the initial trigger for activating the traditional sensing infrastructure. The data from sensor clouds comprised of high resolution cameras and hyperspectral radiometers will be processed through the computational cloud using specialized techniques such as segmentation, feature extraction, registration, indexing, and spectral models for producing estimates of the cyanobacteria concentration and marsh biophysical characteristics from the study sites. The final research thrust of this project will focus on the optimization of the deployment and operation of the sensor cloud based on the information from the community cloud and the results from the computational cloud.\n\nThis project addresses two environmental issues important to coastal states in southeastern United States, namely, harmful algal blooms and marsh browning events. Over the last decade the frequency and magnitude of these environmentally detrimental events have gone up primarily because of drought as well as sea level rise. Therefore, accurate, cost-effective, and targeted monitoring of these events is indispensable to sustainable management of the environment. The proposed cyber-infrastructure-based warning system will enable early detection and timely implementation of preemptive measures to reduce the frequency and severity of future events while ensuring environmental conservation and sustainability. The project plans to engage students, community leaders, resource managers, and the general public via training, workshops, and social media in various aspects of the research starting from crowd sourcing to environmental sensor deployments, data acquisition, processing, and interpretation. The success of the proposed project will pave the way for a state-wide automated early detection, warning and rapid response system that can be adopted by state-level environmental agencies to alert restoration officials and lay citizens of impending risks associated with these events.",1442672,10,flickr
Kameshwar Munagala,,kamesh@cs.duke.edu,Duke University,Big Data Science &Engineering,BIGDATA: F: DKA: Collaborative Research: Dealing Efficiently with Big Social Network Data,"The past decade has seen dramatic growth in systems that collect data from human activities. Online social networks record not just friendships, but interactions, messages, photos, and interests. Mobile devices track location via GPS information. Online stores monitor millions of customers as they explore and transact. Sensors, wearable and otherwise, produce detailed behavioral data. Collectively, this provides ever-larger collections of human social-activity information -- we refer to this as Big Social Data. While Big Social Data is growing rapidly, the available processing resources -- CPU, memory, communication -- are growing at a slower pace. To realize the promise of big social data, we need algorithms that use only sublinear resources, that is, resources growing much less than the growth of the data in suitable parameters. Designing these algorithms will be the core activity of this research project. This work will be in consultation with practitioners handling Big Social Data, leading to many opportunities for technology transfer. The research program both enables and benefits from an education and outreach program that will help develop the new breed of algorithmically-trained data scientists for Big Social Data.\n\nEmerging systems -- MapReduce, Hadoop, Spark, Storm, etc. -- use large scale distributed computation: clusters of machines not only gathering and storing data in parallel, but also working together to perform computations. Often, these systems and applications work via incremental processing, storing and returning only approximate solutions, trading off quality and certainty for efficiency. In addition, these systems take a data-centric view, wherein the data is stored as <Key, Value> pairs. This project will address fundamental problems with Big Social Data -- search, ranking, and optimization, etc. in these modern computing and data models. For these problems, this project will design algorithms that are sublinear in the relevant parameter -- number of keys, size of values, computing time per key or over all keys, and other variations that map to underlying storage, number of machines, bandwidth and other computational constraints.\n\nFor further information, see the project web site at http://www.stanford.edu/~ashishg/socialdata.html .",1447554,2,social data
Salim El Rouayheb,,salim.elrouayheb@rutgers.edu,Illinois Institute of Technology,"COMM & INFORMATION FOUNDATIONS, Secure &Trustworthy Cyberspace",CAREER:Information Theoretic Methods for Private Information Retrieval and Search in Distributed Storage Systems,"The recent data revolution is driving many aspects of modern societal and economic progress. Most of this massive data is now stored in the cloud to enable easy access for a myriad of users who wish to share information including, for example, photos, videos, publications, opinions, and scientific data. Unfortunately, this has come at the expense of the user's privacy whose online activity can be used to profile him/her, making large parts of the population an easy target for discrimination and possible persecution. This research aims at addressing the privacy challenge of data in the cloud by focusing on the problem of Private Information Retrieval (PIR) and Search in distributed storage systems (DSSs). PIR schemes enable users to query data without revealing information about the queries and hence their personal preferences, tendencies, health, or other traits.\n\nClassical information theoretic PIR schemes require data to be replicated, which is not a scalable solution given the exponential growth of data. This research aims at creating a unified framework for studying coding schemes that, in addition to providing data reliability, cater to the need of private queries. The focus of the proposed research is on (i) explicit constructions of codes and PIR schemes that address practical and important aspects of distributed storage, such as storage cost, network communication cost, disk reads, latency and computations; (ii) explicit constructions of codes and schemes for private keyword search; (iii) characterization of the fundamental limits and tradeoffs between reliability, privacy and the different system overheads; (iv) testing software implementations of the schemes on real genomic and social science data. The project also incorporates several educational and outreach efforts, including the development of new publicly accessible online content on information theory, security, and privacy in distributed storage systems as well as pre-college outreach through the Global Leaders Program at the PI's institution.",1652867,3,private information
Chandan Reddy,,reddy@cs.vt.edu,Virginia Polytechnic Institute and State University,INFO INTEGRATION & INFORMATICS,EAGER: An Integrated Predictive Modeling Framework for Crowdfunding Environments,"The research aims to study data analytics tools for improving crowdfunding project success rate. Crowdfunding provides seed capital for start-up companies, creating job opportunities and reviving lost business ventures. In spite of the widespread popularity and innovativeness in the concept of crowdfunding, however, many projects are still not able to succeed. A deeper understanding of the factors affecting investment decisions will not only give better success rate to the future projects but will also provide appropriate guidelines for project creators who will be seeking funding. The crowdfunding domain poses several new challenges from the data analytics perspective due to the heterogeneous, complex and dynamic nature of the data associated with project campaigns. This project develops a systematic data-driven approach to resolve these challenges by utilizing vast amounts of historical data which can be leveraged to accurately predict the success of crowdfunding projects. Though the proposed methods are primarily developed in the context of crowdfunding, they are applicable to various other forms of social data that will be collected in other disciplines such as social science, engineering, and finance.\n\nThis project develops an integrated predictive modeling framework to solve some of the complex underlying problems related to bringing success to crowdfunding based projects. Existing approaches in data analytics for classification and regression cannot tackle this project success prediction problem since the goal is to estimate the time for a project to reach its success. The research team develops a unified probabilistic prediction framework which simultaneously integrates classification and regression together. In addition, a novel iterative imputation mechanism, which calibrates the time to project success, is proposed for reducing the bias in the model estimators. This project can demonstrate the power of data analytics in delivering better insights about various categories of real-world projects by not only accurately estimating the chances of being successful but also quantitatively assessing the factors that are responsible for bringing success in crowdfunding environments. The progress of the project and the research findings are disseminated via the project website (http://dmkd.cs.vt.edu/projects/crowdfunding/).",1646881,19,social data
Samuel Madden,,madden@csail.mit.edu,Massachusetts Institute of Technology,INFO INTEGRATION & INFORMATICS,III: Medium: Collaborative Research: DataHub - A Collaborative Dataset Management Platform for Data Science,"The rise of the Internet, smart phones, and wireless sensors has resulted in a vast trove of data about all aspects of our lives, from our social interactions to our personal preferences to our vital signs and medical records. Increasingly, ""data science"" teams want to collaboratively analyze these datasets, to understand trends and to extract actionable business, scientific, or social insights. Unfortunately, while there exist tools to support data analysis, much-needed underlying infrastructure and data management capabilities are missing. To this end, ""DataHub"", a collaborative platform for cleaning, storing, understanding, sharing, and publishing datasets, will be developed. DataHub will be a publicly accessible platform that will host private user datasets as well as public datasets retrieved from online sources. DataHub will serve as the common substrate for data science, freeing up end users from tedious dataset book-keeping tasks, and instead supporting them in their search for useful insights. DataHub will be deployed on a large scale at MIT; partnerships with organizations and groups from a variety of sectors will be leveraged upon to show benefits for real data scientists and to ensure that the proposed techniques meet real-world big data challenges. The curriculum development part of this project will lead to the training of new data scientists, and the project will also provide opportunities for graduate and undergraduate students to participate in research and learn how to do collaborative research.\n\nUnlike most systems that focus on improving performance or on supporting even more sophisticated analyses, DataHub will instead focus on simplifying and automating many fundamental book-keeping operations that are a pre-requisite to data science. Key features of DataHub will include: (1) a flexible, source code control-like versioning system for data, that efficiently branches, merges, and differences datasets; (2) new data ingest, cleaning, and wrangling tools designed to automate data cleaning process; (3) the ability to search for ""related"" tables and to integrate them into the analysis process; and (4) the ability to selectively share and collaborate on data sets across users and teams. Overall, DataHub will significantly reduce the amount of effort involved on the part of data scientists for preparing, analyzing, sharing, and managing data.\n\nFor more information, see the project website at: http://data-hub.org",1513443,19,private user
William Enck,,enck@cs.ncsu.edu,North Carolina State University,Secure &Trustworthy Cyberspace,CAREER: Secure OS Views for Modern Computing Platforms,"The security architecture of consumer operating systems is currently undergoing a fundamental change. In platforms such as Android, iOS, and Windows 8, each application is a separate security principal that can own data. While this distinction is a vast improvement over traditional user-focused security architectures, sharing data between applications results in an unexpected loss of control of that data, potentially exposing security and privacy sensitive information. This research improves the security of these modern consumer operating systems by providing a holistic view of data protection. In particular, this work proposes a new operating system abstraction for transparently tracking and controlling access to all data, allowing policy to determine if a reader is given the true value, a fake or modified value, or no value at all. To efficiently and practically accomplish this goal, this work combines several existing and new techniques to track and control access to data. The new abstraction provided by this work not only solves a significant problem affecting modern consumer operating systems by enabling applications to retain pervasive control over their data, but also more broadly provides a new abstraction on which a variety of new security solutions can be built.",1253346,3,sensitive information
Seung-Jong Park,,sjpark@cct.lsu.edu,Louisiana State University & Agricultural and Mechanical College,S&CC: Smart & Connected Commun,SCC-Planning: Promoting Smart Technologies in Public Safety and Transportation to Improve Social and Economic Outcomes in a US EDA-Designated Critical Manufacturing Region,"The U.S. Economic Development Administration, through its Investing in Manufacturing Communities Partnership (IMCP) initiative, recently named the entire east-west, 200-mile span of South Louisiana centered around Baton Rouge (BR) as one of the nation's strategic manufacturing regions. This ""chemical corridor"" is the home of hundreds of chemical manufacturing facilities and refineries worth billions of dollars. Despite the major economic significance of this region, the BR area suffers from critical problems including i) crippling transportation issues, and (ii) high levels of crime. Heavy traffic congestion is one of the main reasons why manufacturing industry in BR is reluctant to build new plants and hire more people in this region; it is also a key factor in making the region unattractive for new investments from elsewhere in the U.S. The violent crime rate is substantially higher than that of similarly- sized regions, further affecting economic development in this important region. The goal of this S&CC planning proposal is to build a partnership between community stakeholders and a multidisciplinary team of academic researchers. By considering economic and social issues in a holistic manner, this partnership will develop research concepts that will promote and employ S&CC technologies to help stakeholders tackle the major factors affecting the region's economic progress. Through multidisciplinary team- building and strong community engagement, a proposed integrative S&CC research concept will have significant impacts on various disciplines and communities for planning and developing smarter cities. In particular, the research concept will be directly aligned with the strategic plans determined by the city's Smart City Committee and Subcommittees. Through the proposed Web portal, PIs will share the developed outcomes and information collected from the project with researchers from other higher education including Southern University , a local HBCU. The most significant impact will be seen through quality of life measures pre- and post-project implementations. Building community-wide tools to help stakeholders of all missions addressing crime- and traffic-related challenges for all citizens in the region has real impact on quality of life and economic health, making the region more attractive for growth.\n\n\nFrom a technical perspective, this project proposes to build a multidisciplinary research team by integrating different research groups with significant research strength in the areas of High Performance Computing (LSU CCT), Big Data Analysis and Cybersecurity (Computer Science), Sensors (Electrical Engineering), Violence Prevention (Social Work and Sociology), and Transportation (Civil Engineering). The intellectual goal is to define challenging problems and develop research concepts via offline workshops, tutorials, and an online Web portal, which allows an easy access to integrative cyberinfrastructure for computing, storage, and software tools. The software tools we plan to develop will enable predictive analyses on heterogeneous data collected from city infrastructure, public open data from the city of Baton Rouge, and relevant social network data. The developed research concepts will increase the research capacity of every individual research group, enhance the understanding of cross-disciplinary demands, and advance state-of-art technologies for the design of smart and connected communities.",1737557,14,social network data
Husheng Li,,husheng@eecs.utk.edu,University of Tennessee Knoxville,Secure &Trustworthy Cyberspace,TWC: Small: Collaborative: Multi-Layer Approaches for Securing Enhanced AMI Networks against Traffic Analysis Attacks,"The U.S. power grid is being replaced with a smart grid, a complex network of intelligent electronic devices, distributed generators, and dispersed loads, which requires communication networks for management and coordination. Advanced metering infrastructure (AMI) networks are one part of the smart grid to provide two-way communications between smart meters at the consumers' side and the utility companies. AMI networks allow utilities to collect power consumption data at high frequency rates. However, it needs too much communication bandwidth for smart meters to frequently send power consumption data even when the power consumption does not change. Since using cellular networks is one of the best options to AMI networks, the cost of sending this large amount of data is prohibitive. This project considers enhanced AMI networks, where the meters send power consumption data only when there is a significant change. This can significantly reduce the amount of bandwidth needed for sending the power consumption data; however, it creates a new privacy problem. Practical experiment results have confirmed that by observing the data transmission rate and using traffic analysis techniques, the attackers can infer sensitive information about consumers. Therefore, this new privacy problem must be studied and addressed, and strong countermeasures should be developed.\n\nThe proposed research systematically combines efforts from privacy, networking, and communication communities. The project promotes a research program designed to: (a) develop schemes for countering traffic analysis in AMI networks by considering different network and adversary models; (b) quantitatively measure the privacy protection provided by the schemes; and (c) evaluate the schemes in a prototype system for validating the proposed research and enabling hands-on experience for both undergraduate and graduate students. The project will significantly contribute to the research on smart grid, as well as computer system security and privacy. The proposed research will lead to a body of knowledge that can be leveraged by the designers of other networks. The proposed project also lends itself to teaching, training and learning of students. A new graduate course focusing on security and privacy aspects of smart grid will be developed. The achievement of the proposed research will be disseminated to academic community and industry via academic conferences and industrial connections.",1617394,3,sensitive information
Nick Nikiforakis,,nick@cs.stonybrook.edu,SUNY at Stony Brook,Secure &Trustworthy Cyberspace,TWC: Small: Cross-Application and Cross-Platform Tracking of Web Users: Techniques and Countermeasures,"There are many applications in business and end-user applications where user tracking is part of the core functionality or feature set. However, user tracking can intrude on user privacy and even may lead to online crimes. Recent research has shown that tracking companies have started using advanced web tracking techniques that are more subtle and less transparent than traditional online tracking. The ever increasing adoption of mobile devices further exacerbates the tracking problem since these mobile devices are saturated with personal information while the details of mobile-specific tracking techniques are largely unknown. \n\nThe project seeks a deeper understanding of the existing and potential technical capabilities and limitations of tracking, and the trade-offs between tracking and privacy. The project will conduct a systematic study of techniques and technologies that are used for tracking web users, starting with new variations of traditional tracking methods and proceeding to advanced and stealthy users and device fingerprinting. The research will involve both in-depth analysis and large-scale measurement studies. The gained knowledge will inform security and privacy research communities of the technical capabilities and limits posed by the next generation of user tracking on mobile devices, paying particular attention to the under-researched area of stateless tracking.",1527086,3,personal information
Jelena Mirkovic,,mirkovic@isi.edu,University of Southern California,CYBERCORPS: SCHLAR FOR SER,"TWC: Small: Critter@home: Content-Rich Traffic Trace Repository from Real-Time, Anonymous, User Contributions","There are very few publicly available network traces that contain application-level data, because of the enormous privacy risk that sharing such data creates. Application-level data is rich with personal and private information, such as human names, social security numbers, etc. that criminals can monetize. Yet such data is necessary for realistic testing of research products, and for understanding trends in the domain of networking and network applications.\n\nThis project develops a publicly accessible, diverse and fresh archive of content-rich network data, contributed by volunteer users, called Critter-at-home. Users join the Critter overlay whenever online, offering their data to interested researchers. Privacy of data contributors is protected by several means. First, contributors may opt to host their own data on their machines, thus retaining full control over it. Second, we process contributed data to modify all personal and private information (PPI) and we encrypt it. Third, no human apart from the contributor ever accesses the raw, PPI-sanitized, data. Instead, researchers query the data via our Critter-at-home framework, and they receive aggregate statistics (counts, distributions, etc.) of the traffic features they query for. Four, all contact with a contributor is at her discretion and is done through an anonymous network, where contributor identities are hidden.\n\nThe archive this project creates will greatly advance security research by providing necessary data for its validation and for data mining. This archive will further be valuable to a broader networking e.g., for realistic traffic generation, as ground truth in traffic classification, and for many other purposes.",1224035,30,private information
Eleanor O'Rourke,,eorourke@northwestern.edu,Northwestern University,Cyber-Human Systems (CHS),CRII: CHS: Automatically Praising Learning Process to Promote the Growth Mindset in Computer Science,"There is a pressing need to train large numbers of computer scientists to meet the demands of our nation's economy, but many students struggle in introductory programming courses. Recent studies show that these courses often promote the fixed mindset, or the belief that programming aptitude is an inborn trait. Psychology research shows that students with the fixed mindset view mistakes as indications of low ability and perform poorly in the face of challenge. In contrast, students with the growth mindset believe that programming aptitude is malleable and excel when challenged. This project aims to design, build, and evaluate programming tools that help students develop the growth mindset by automatically detecting and praising good learning behaviors as students write code. This research will contribute scientific knowledge about the growth mindset in the domain of computer science and provide insights about the process of learning to program. The project team will deploy the tools to hundreds of students at their own university and release them for free online for any student or teacher to use. If successful, this intervention has the potential to improve the experiences, skills, and diversity of students who successfully complete programming courses and go on to participate in employment and research in STEM fields.\n\nThis project aims to develop a new growth mindset intervention that leverages the programming environment by using artificial intelligence techniques to automatically detect and praise good learning processes in real time. Programming environments provide a unique opportunity to track and understand student learning behaviors, and offer a scalable environment for praising good practices automatically. By exposing and praising the learning process, this intervention will teach students to attribute their successes and failures to malleable learning processes, rather than an innate aptitude for computer science. This research will be conducted in two phases. First, the project team will develop heuristics that detect good learning processes using behavioral log data, leveraging the computer science education literature and studying the behavior of fixed and growth mindset students to identify good processes. Second, the team will iteratively design and build a programming environment extension that uses the validated heuristics to automatically detect and praise good learning process, and evaluate this intervention through a controlled ten-week study with university students.\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",1755628,20,log data
James Allan,,allan@cs.umass.edu,University of Massachusetts Amherst,INFO INTEGRATION & INFORMATICS,Strategic Workshop on Information Retrieval,"This award provides support for U.S. researchers to participate in the Strategic Workshop on Information Retrieval, to be held in Lorne, Victoria, Australia in February 2012. The goal of the workshop is to explore and define long-range research issues of general and personal information management in the context of information that is constantly changing, that is in a diverse set of formats, and that is stored in numerous locations. This workshop brings together about 40 junior and senior researchers from around the world with expertise spanning the information retrieval, information science, and search communities to develop common understandings and goals of the long-term challenges and opportunities. The workshop will explore at least the following questions: (1) How can Information Retrieval algorithms be adapted to this new paradigm where information is distributed, multimedia, and dynamic? (2) In this setting, what are the key challenges that researchers, graduate students, and funding agencies should be aware of? (3) How can the successful evaluation models for Information Retrieval be extended to this setting where data availability and privacy are major challenges? (4) How should teaching about Information Retrieval be modified to reflect likely changes in the field? \n\nThis workshop is expected to yield research directions that will impact the information retrieval and science research and development communities as well as research groups that tackle problems arising within this context (e.g., machine learning, human computer interaction). The immediate beneficiaries of the workshop's publications will include graduate students, faculty members, industrial researchers, and developers of search-related technology. In addition, it is expected that general users of search tools will benefit from resulting advances in technology. Results addressing the questions above will be disseminated on a dedicated website (http://www.cs.rmit.edu.au/swirl12/) and will also be published in the widely distributed SIGIR Forum newsletter and its web site.",1216764,30,personal information
Paul Hines,,paul.hines@uvm.edu,University of Vermont & State Agricultural College,CRISP - Critical Resilient Int,CRISP Type 2/Collaborative Research: Understanding the Benefits and Mitigating the Risks of Interdependence in Critical Infrastructure Systems,"This Critical Resilient Interdependent Infrastructure Systems and Processes (CRISP) project will identify new strategies to increase resilience in interdependent electric power, communication and natural gas networks. These three critical systems increasingly depend on one another to keep our energy and communication systems running. In some ways connections between these systems can make them work better, but in other ways connections can increase the chance of disastrous failures that could leave millions of people without heat, electricity or the ability to communicate. For example, a severe winter storm in the Northeastern United States could lead to both power grid failures and natural gas failures, leading to failures in telephone and Internet services, making it even more difficult to restore these critical services. Such ""cascading failures"" make it even harder for these systems to recover from natural disasters and intentional attacks. This project will identify strategies to make interdependent infrastructure systems more resilient to these cascading failures. Four Research Directions will combine to address this problem. Research Direction 1 will adapt new computational algorithms, such as Influence Graphs that can identify non-obvious critical connections and the Random Chemistry algorithm that can rapidly find critical triggering events, to the particular problems of cascading failures in interdependent infrastructure systems. Research Direction 2 will create new models of interdependence among natural gas, electric power and communication networks, which will form a testbed for computational algorithms. The resulting models will balance computational complexity and engineering detail by using detailed dynamical models of each system when necessary and simplified mathematical models when abstractions can be validated from real data. Research Direction 3 will develop and evaluate engineering solutions and coordination strategies that can mitigate harmful interdependencies and leverage beneficial interconnections. These will leverage insights from the application of new computational algorithms to the interdependence testbed, such as the identification of critical failure paths, to develop both real-time dynamic rescheduling algorithms and cost-effective long-term planning strategies. Research Direction 4 will use stakeholder interviews to evaluate the diverse ways that the electricity, natural gas, and communications industries understand risk, and facilitate discussion among key industry participants regarding interdependencies among these systems. The results will reveal the most effective paths to integrating new control and planning strategies to increase resilience in these diverse systems.\n\nThis project will create significant societal benefits by uncovering new ways to reduce the risk of catastrophic failures among critical infrastructure systems. Because of interdependence among infrastructures, low probability, high cost cascading failures, which can have billions of dollars of economic and societal impacts, can contribute more to overall risk, relative to more frequent, small events. Reducing this risk can have enormous benefits to society. To ensure that results from this project have practical impacts the team will be guided by a Research Advisory Board that includes a large power grid operator (ISO New England), a software vendor for the electricity industry (GE/Alstom), a natural gas company (Vermont Gas), and the MITRE corporation. Furthermore, the project will integrate education and research through new curriculum and outreach to high school students. Public data that result from this project will be released through the github repository at: https://github.com/phines/infrastructure-risk, as well as through the project web site at http://www.uvm.edu/~tesla/project/nsf-crisp/. All research data associated with this project, including public and non-public data, will be preserved for at least 5 years after the end of the project.",1735513,6,public data
Piotr Jankowski,,pjankows@mail.sdsu.edu,San Diego State University Foundation,"GEOGRAPHY AND SPATIAL SCIENCES, Secure &Trustworthy Cyberspace",Doctoral Dissertation Research: Geoprivacy Attitudes and Personal Location-Masking Strategies of Internet Users,"This doctoral dissertation research project will examine public attitudes towards geoprivacy and the extent to which Internet users attempt to obscure or mask their personal locations in order to protect privacy when they engage in online activity. The project will provide new knowledge about the strategies individuals employ to prevent their personal identifiability when location is requested, and it will provide new insights into the relationships between privacy attitudes and behavior in responding to location requests. While much work on geoprivacy focuses on strategies for researchers to protect the anonymity of human subjects, this project will emphasize the role individuals play in managing their own location profiles. This project will enhance basic understanding of the factors associated with the propensity to mask location, which will inform future research that assumes self-reported location to be factual by providing a sounder basis for error calculation. Improved knowledge about who is masking their data online and why they are doing so will help application developers better understand how to serve those individuals. Results will be disseminated broadly to a wide range of potential beneficiaries, including non-profit groups that seek to protect digital privacy and governmental organizations like the National Geospatial Advisory Committee. As a Doctoral Dissertation Research Improvement award, this award also will provide support to enable a promising student to establish a strong independent research career.\n\nGeoprivacy refers to the right of individuals to control when and how their personal location data are shared. Amid increasingly pervasive locational data collection, attempts by the public to exercise this control largely remain unexplored. This doctoral student conducting this project will examine the ways in which adults mask their location data online as well as how geoprivacy knowledge and attitudes influence this location-masking behavior. She will conduct an online survey of adults in California using a probability sample with a mail contact method to enhance response rate and an open sample with an online solicitation. She will analyze the correlates of both the resolution and factuality of location provided in the survey with logistic regression. Among intervening variables that the student will consider are previous privacy infringement, experience with hacking or identity theft, and data industry experience. She also will test for spatial clusters of higher and lower levels of location-masking activity.",1657610,3,locational data
David Kotz,,David.F.Kotz@Dartmouth.edu,Dartmouth College,Secure &Trustworthy Cyberspace,TWC: Frontier: Collaborative: Enabling Trustworthy Cybersystems for Health and Wellness,"This frontier project tackles many of the fundamental research challenges necessary to provide trustworthy information systems for health and wellness, as sensitive information and health-related tasks are increasingly pushed into mobile devices and cloud-based services. The interdisciplinary research team includes expertise from computer science, business, behavioral health, health policy, and healthcare information technology to enable the creation of health & wellness systems that can be trusted by individual citizens to protect their privacy and can be trusted by health professionals to ensure data integrity and security. Although these problems are motivated by a nationally important application domain (health and wellness), the solutions have applications far beyond that domain.\n \nThis project is developing methods to authenticate clinical staff to tablet computers in a continuous and unobtrusive way, and to provide patients a usable way to control the information that mobile sensors collect about them. One of the goals is to manage security of healthcare devices in the home and in remote clinics, without adding burden on the homeowner or clinical staff; towards this end the investigators are developing methods to verify medical directives issued to remote devices. One approach being investigated is segmenting access to medical records from mobile devices to limit information exposure, and developing methods to audit behavior of this complex ecosystem of devices and systems. The investigators will design tools to handle genomic data in the cloud while enabling patient control over information, detect malware in medical devices through power analysis, and provide contextual information to those who use health data collected in the field.",1329686,3,sensitive information
John Jost,,john.jost@nyu.edu,New York University,SOCIAL PSYCHOLOGY,INSPIRE: Computer Learning of Dynamical Systems to Investigate Cognitive and Motivational Effects of Social Media Use on Political Participation,"This INSPIRE award is partially funded by Human-Centered Computing Program and by Social-Computational Systems Program both in the Division of Information and Intelligent Systems in the Directorate for Computer & Information Science & Engineering, and by the Social Psychology Program in the Division of Behavioral and Cognitive Sciences and the Political Science Program in the Division of Social and Economic Sciences in the Directorate for Social, Behavioral and Economic Sciences.\n\nWith regards to intellectual merit, the goal of this project is to forge an interdisciplinary collaboration that examines the impact of social media on political behavior. First, from social psychology and political science, fundamental hypotheses will be developed about how, why and when social media affects citizens' cognition and motivation with respect to political participation. Second, these questions will be expressed as testable hypotheses derived from behavioral models. And third, drawing from biology and computer science, the project adapts sophisticated computational methods of approximate inference and machine learning (adapting methods developed for the analysis of Systems Biology data) to evaluate the behavioral models using extremely large social media and social network datasets. \n\nThe scientific opportunities afforded by the use of social media are readily apparent when we consider the richness and precision of data on participation in elections, protests, riots, and other spontaneous political events. This project will construct a comprehensive data set of incoming and outgoing social media messages messages using systematically structure formats that are ideally suited to machine learning methods, and this information will be integrated with information on social network connectivity and a vast array of metadata on individuals and their social contacts. By developing new methods to harvest and combine these data sources effectively, it will be possible to transform the scientific study of social and political attitudes and behavior. Every time individuals use social media, they leave behind a digital footprint of what was communicated, when it was communicated, and, to whom it was communicated. Typically, such precise estimates of these variables are available only to laboratory investigators working in artificial settings. No previous study has successfully used fine-grained social influence data such as these to predict consequential behavioral outcomes, such as attendance at a given protest or rally. The structure of the data means that we will have panel data on respondents, many of potentially long duration. In addition, the investigators will conduct a panel survey, which is essential for drawing causal inferences about the cognitive and motivational processes whereby social media use facilitates political participation.\n\nWith regards to broader impacts, this research will enhance interdisciplinary training for graduate and undergraduate students. These include students in psychology, political science, computer science, and biology and also includes students from groups that are underrepresented in these sciences. In addition, opportunities will be provided for high school students to become involved in the research process. The research program will foster broad dissemination of scientific understanding by leveraging past experience of the principal investigators with disseminating large code-bases, data-bases, and data-sets to share work with other scientists (pre-publication). Finally, the researchers are committed to making their research available to the general public and have extensive experience doing so.",1248077,30,digital footprint
Seth Blumsack,,blumsack@psu.edu,Pennsylvania State Univ University Park,CRISP - Critical Resilient Int,CRISP Type 1/Collaborative Research: Computable Market and System Equilibrium Models for Coupled Infrastructures,"This interdisciplinary research will develop computational tools and public data sets to enable the joint modeling of interdependent energy infrastructures. The work is motivated by increasing interdependencies between the U.S. electric power and natural gas infrastructures. These interdependencies arise from the dual, increasing roles of natural gas as a base-load resource (replacing coal-fired power plants retiring for economic or environmental reasons) and a balancing resource (to smooth fluctuations in variable renewable energy generation). While natural gas brings environmental benefits over coal, the increased coupling between electricity and gas systems and markets have been difficult to model with existing tools. This research program will develop tractable computational tools and supporting data sets to enable analysis of the operational or economic risks associated with this increasing interdependence and to articulate the economic and social value from increased coordination in system planning, operations and clearing of markets. The research will be beneficial to the energy systems and policy research communities through the development and dissemination of analytical tools, and will help to inform evolving U.S. policy on coordination between energy infrastructures, including electric power and natural gas. This effort will involve an integrated and interdisciplinary collaboration between experts in energy economics, computer science and optimization that will enrich not only the research community but also the education of energy systems scholars from multiple fields. \n\nThe research team will create a framework for computable market and system equilibrium models that: capture non-linear aspects of AC power systems and the Weymouth gas flow equations; accurately represent the market institutions in electricity and natural gas that lead to price formation; and capture the cyclic dependence between price formation in spot natural gas markets and the price calculations in spot electricity markets. This framework will embed a market equilibrium model for natural gas within an optimization framework for the joint planning of natural gas and electric power systems. The approach will be implemented on a model of the U.S. northeastern power grid and natural gas pipeline system that is drawn from publicly-available information. The awarded research will demonstrate that this framework can produce sensible outcomes with acceptable computational performance, illustrate the framework on a series of test systems including the U.S. northeast test system, and demonstrate the value of integrated infrastructure planning for natural gas and electric power systems.",1638331,6,public data
Yong Zheng,,yzheng66@iit.edu,Illinois Institute of Technology,Cyber-Human Systems (CHS),"Support for U.S. Students to Present at the 2018 ACM User Modeling, Adaptation and Personlization (UMAP) Conference","This is funding to support travel for about 10 students enrolled in MS or PhD programs in U.S. educational institutions, to present their accepted papers and posters and/or to attend the Doctoral Consortium associated with the 26th International Conference on User Modeling, Adaptation and Personalization (UMAP 2018), to be held July 8-11, 2018 at Nanyang Technological University in Singapore, and which is sponsored by ACM SIGCHI and SIGWEB. User interfaces that adapt themselves to available user information (such as special needs or individual preferences) are becoming increasingly important, so much so that adaptability has become a selling point for software products. A system with the ability to construct and consult a user model (an explicit representation of properties of a particular user or group of users) can adapt diverse aspects of its performance and enhance its effectiveness, usability and/or acceptance in a variety of situations (e.g., to reduce information overload, to improve the quality of information retrieval, filtering and annotation, and to generate useful information visualizations). Applications for user modeling range from electronic commerce and intelligent learning environments to health care and assistive technologies. Relevant platforms for user modeling include mobile and wearable systems and smart environments, as well as individual desktop systems, groupware, adaptive hypermedia, and other web-based systems. The annual UMAP conference is the premier forum at which academic and industrial researchers from all these fields gather to present their latest research and to exchange their complementary insights on user modeling issues. Attending and presenting their work at UMAP will have a significant impact on the careers of the future generation of user modeling, adaptation, and personalization researchers. More information about the conference is available online at http://www.um.org/umap2018/. \n\nStudent authors at UMAP and attendees in the Doctoral Consortium will both have the opportunity to present their work to a knowledgeable audience and through lively discussions get useful comments at an early stage of their research when it will be most useful. Just as importantly, they will have a chance to meet established researchers and other graduate students doing similar work, to exchange ideas and to make contacts that will be invaluable to them as they progress in their scientific careers. Interacting with young researchers is also beneficial to experienced investigators, by providing fresh ideas and new perspectives. Thus, the Doctoral Consortium is a great confidence builder for the students involved, and highly stimulating to the senior researchers as well. Students whose work has been selected for presentation at the Doctoral Consortium will be invited to write a paper that will be published in the UMAP 2018 conference proceedings, which will be included in the ACM Digital Library. They will have a 30-minute window in which to present their work (with a short demonstration where appropriate), including time for questions and discussion. During the question/discussion period and in subsequent informal interactions, organizing committee members and other participants will provide constructive comments on each student's work and attempt to address aspects on which the student has requested advice. The organizers have reaffirmed the long-standing and demonstrated UMAP commitment to diversity; to this end, they will make special efforts to recruit women and members of under-represented groups when selecting the candidates to receive financial support.\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",1830908,20,user information
David Chin,,chin@hawaii.edu,University of Hawaii,Cyber-Human Systems (CHS),"WORKSHOP: Doctoral Consortium at the 2017 ACM User Modeling, Adaptation and Personalization Conference (UMAP 2017)","This is funding to support travel for about 8 students enrolled in PhD programs in U.S. institutions, to present their accepted papers and posters and/or to attend the Doctoral Consortium associated with the 25th International Conference on User Modeling, Adaptation and Personalization (UMAP 2017), to be held July 9-12 at the Faculty of Informatics and Information Technologies of the Slovak University of Technology in Bratislava, Slovakia, and which is sponsored by ACM SIGCHI and SIGWEB. User interfaces that adapt themselves to available user information (such as special needs or individual preferences) are becoming increasingly important, so much so that adaptability has become a selling point for software products. A system with the ability to construct and consult a user model (an explicit representation of properties of a particular user or group of users) can adapt diverse aspects of its performance and enhance its effectiveness, usability and/or acceptance in a variety of situations (e.g., to reduce information overload, to improve the quality of information retrieval, filtering and annotation, and to generate useful information visualizations). Applications for user modeling range from electronic commerce and intelligent learning environments to health care and assistive technologies. Relevant platforms for user modeling include mobile and wearable systems and smart environments, as well as individual desktop systems, groupware, adaptive hypermedia, and other web-based systems. The annual UMAP conference is the premier forum at which academic and industrial researchers from all these fields gather to present their latest research and to exchange their complementary insights on user modeling issues. To encourage student participation, UMAP has established two annual awards valued at $1,000 each endowed by the family of the late James Chen, an active User Modeling researcher, for best student papers (papers that represent primarily student research work and where students are lead authors). More information about the conference is available online at http://www.um.org/umap2017/. \n\nAttending and presenting their work at UMAP will have a significant impact on the careers of the future generation of user modeling, adaptation, and personalization researchers. First, they will have the opportunity to present their work to a knowledgeable audience and through lively discussions get useful comments at an early stage of their research when it will be most useful. Just as importantly, they will have a chance to meet established researchers and other graduate students doing similar work, to exchange ideas and to make contacts that will be invaluable to them as they progress in their scientific careers. Interacting with young researchers is also beneficial to experienced investigators, by providing fresh ideas and new perspectives. Thus, the Doctoral Consortium is a great confidence builder for the students involved, and highly stimulating to the senior researchers as well. Many past participants in the UMAP Doctoral Consortium have gone on to become well-regarded researchers and practitioners in the field. Students whose work has been selected for presentation at the Doctoral Consortium will benefit from that experience in a number of additional ways. They will be invited to write a paper that will be published in the UMAP 2017 conference proceedings, which will be included in the ACM Digital Library. And they will have a 30-minute window in which to present their work (with a short demonstration where appropriate), including time for questions and discussion. During the question/discussion period and in subsequent informal interactions, organizing committee members and other participants will provide constructive comments on each student's work and attempt to address aspects on which the student has requested advice. The organizers have reaffirmed the long-standing and demonstrated UMAP commitment to diversity; to this end, they will make special efforts to recruit women and members of under-represented groups when selecting the candidates to receive financial support.",1738024,20,user information
Sidney D'Mello,,sdmello@nd.edu,University of Notre Dame,Cyber-Human Systems (CHS),"WORKSHOP: Doctoral Consortium at the 2016 ACM User Modeling, Adaptation and Personalization Conference (UMAP 2016)","This is funding to support travel for up to 12 students enrolled in PhD programs in U.S. institutions, to present their accepted papers and posters and/or to attend the Doctoral Consortium associated with the 24th International Conference on User Modeling, Adaptation and Personalization (UMAP 2016), to be held at Dalhousie University in Halifax, Nova Scotia (Canada) on July 13-17, and which is sponsored by ACM SIGCHI and SIGWEB. User interfaces that adapt themselves to available user information (such as special needs or individual preferences) are becoming increasingly important, so much so that adaptability has become a selling point for software products. A system with the ability to construct and consult a user model (an explicit representation of properties of a particular user or group of users) can adapt diverse aspects of its performance and enhance its effectiveness, usability and/or acceptance in a variety of situations (e.g., to reduce information overload, to improve the quality of information retrieval, filtering and annotation, and to generate useful information visualizations). Applications for user modeling range from electronic commerce and intelligent learning environments to health care and assistive technologies. Relevant platforms for user modeling include mobile and wearable systems and smart environments, as well as individual desktop systems, groupware, adaptive hypermedia, and other web-based systems. The annual UMAP conference is the premier forum at which academic and industrial researchers from all these fields gather to present their latest research and to exchange their complementary insights on user modeling issues. More information about the conference is available online at http://www.um.org/umap2016/. \n\nAttending and presenting their work at UMAP will have a significant impact on the careers of the future generation of user modeling, adaptation, and personalization researchers. First, they will have the opportunity to present their work to a knowledgeable audience and through lively discussions get useful comments at an early stage of their research when it will be most useful. Just as importantly, they will have a chance to meet established researchers and other graduate students doing similar work, to exchange ideas and to make contacts that will be invaluable to them as they progress in their scientific careers. Interacting with young researchers is also beneficial to experienced investigators, by providing fresh ideas and new perspectives. Thus, the Doctoral Consortium is a great confidence builder for the students involved, and highly stimulating to the senior researchers as well. Many past participants in the UMAP Doctoral Consortium have gone on to become well-regarded researchers and practitioners in the field. Students whose work has been selected for presentation at the Doctoral Consortium will benefit from that experience in a number of additional ways. They will be invited to write a paper that will be published in the UMAP 2016 conference proceedings, which will be included in the ACM Digital Library. They will have 15 minutes to present their work (which may include a short demonstration if appropriate), to be followed by an additional 15 minutes for questions and discussion. During the question/discussion period and in subsequent informal interactions, organizing committee members and other participants will provide constructive comments on each student's work and attempt to address aspects on which the student has requested advice. The organizers have reaffirmed the long-standing and demonstrated UMAP commitment to diversity; to this end, they will make special efforts to recruit women and members of under-represented groups when selecting the candidates to receive financial support.",1642486,20,user information
Bamshad Mobasher,,mobasher@cs.depaul.edu,DePaul University,"INFORMATION TECHNOLOGY RESEARC, Cyber-Human Systems (CHS)","Student Support for the 2015 User Modeling, Adaptation and Personalization Conference","This is funding to support travel for up to 5 students enrolled in PhD programs in U.S. institutions, to present their accepted papers and posters and/or to attend the Doctoral Consortium associated with the 23rd International Conference on User Modeling, Adaptation and Personalization (UMAP 2015), to be held in Dublin, Ireland, June 29 through July 3. User interfaces that adapt themselves to available user information (such as special needs or individual preferences) are becoming increasingly important, so much so that adaptability has become a selling point for software products. A system with the ability to construct and consult a user model (an explicit representation of properties of a particular user or group of users) can adapt diverse aspects of its performance and enhance its effectiveness, usability and/or acceptance in a variety of situations (e.g., to reduce information overload, to improve the quality of information retrieval, filtering and annotation, and to generate useful information visualizations). Applications for user modeling range from electronic commerce and intelligent learning environments to health care and assistive technologies. Relevant platforms for user modeling include mobile and wearable systems and smart environments, as well as individual desktop systems, groupware, adaptive hypermedia, and other web-based systems. The annual UMAP conference is the premier forum at which academic and industrial researchers from all these fields gather to exchange their complementary insights on user modeling issues. More information about the conference is available online at http://www.um.org/umap2015/.\n\nAttending and presenting their work at UMAP will have a significant impact on the careers of the future generation of user modeling, adaptation, and personalization researchers. First, they will have the opportunity to present their work to a knowledgeable audience and through lively discussions get useful comments at an early stage of their research when it will be most useful. Just as importantly, they will have a chance to meet established researchers and other graduate students doing similar work, to exchange ideas and to make contacts that will be invaluable to them as they progress in their scientific careers. Interacting with young researchers is also beneficial to experienced investigators, by providing fresh ideas and new perspectives. Thus, the Doctoral Consortium is a great confidence builder for the students involved, and highly stimulating to the senior researchers as well. Many past participants in the UMAP Doctoral Consortium have gone on to become well-regarded researchers and practitioners in the field. Students whose work has been selected for presentation at the Doctoral Consortium will benefit from that experience in a number of additional ways. They will be invited to write a paper that will be published in the UMAP 2015 conference proceedings. They will have 15 minutes to present their work (which may include a short demonstration if appropriate), to be followed by an additional 15 minutes for questions and discussion. During both the question/discussion period and in subsequent informal interactions, organizing committee members and other participants will provide constructive comments on each student's work and attempt to address aspects on which the student has requested advice. The organizers have reaffirmed the long-standing and demonstrated UMAP commitment to diversity; to this end, they will make special efforts to recruit participants who are women and members of under-represented groups, and they will ensure institutional diversity by supporting no more than one student from any given university.",1542865,20,user information
Sheila Tejada,,sheilatejada@gmail.com,University of Southern California,Cyber-Human Systems (CHS),HCC: EAGER: Trust and Collaboration in 3D Virtual Communities,"This potentially transformative study will investigate how players in a Massively Multi-player Online (MMO) game - specializing in social challenges, crafting, and trading - use trust in order to accomplish tasks and collaborate with other players. This is the first time a game of this kind has been evaluated. Most MMOs that have been studied, such as World of Warcraft and EverQuest, evaluate trust and collaboration of players engaged in battles or battle planning, rather than peaceful and constructive purposes. This research employs a well-established but rather unique game that primarily focuses on a wide array of social challenges and tests. It is called ""A Tale in the Desert"" (ATITD), in which each player controls the avatar of an ancient Egyptian. ATITD is the ideal choice for this research because of the strong social aspect inherent in the game. \n\nBoth because the game creators have agreed to cooperate, and because ATITD incorporates databases such as a constantly updated census and extensive information about all players and their overlapping group memberships, the research will have access to fine grained data of players' actions, chats, and memberships in guilds, marriages and bureaucracies. Starting with an initial set of hypotheses of what data will be analyzed, the research team will then able to iteratively choose more data on the basis of early discoveries. To address the issues involved in human-agent interaction, this study of trust and collaboration is very important. Information obtained in this study can be used in future work to create agents to interact with human players in ATITD and other virtual environments. \n\nIn addition to log data from the game, this research will also collect standard psychological measures of individual players, using a voluntary questionnaire, including the Extroversion scale from the Big Five Inventory, the Trust-Suspicion Scale, and the Empathy Quotient. Of particular interest will be to learn if the psychological factors predict how well individuals interact with other players in the game. Game play data will be analyzed to identify specific behavioral patterns related to the central themes of trust and collaboration, and, conversely, betrayal and competition. The research will also gather data from ATITD about how the players trust and collaborate with each other, using data mining techniques, interviews of players, and examination of wikis created by players. \n\nStudying the nature of trust and collaboration in 3D virtual communities will be of great interest to game designers, as well as to the research community of computer-mediated environments. The information gained from this study we will be the basis for detailed suggestions on how to improve trust and collaboration in an MMO. Methods also will be developed to detect untrustworthy behavior happening in the game and flag it. Data gathered from ATITD will be useful for creating agents that interact with human players, and for designing educational activities that exploit the unique opportunities for student collaboration offered by virtual worlds like A Tale in the Desert.",1338580,20,log data
Tawanna Dillahunt,,tdillahu@umich.edu,University of Michigan Ann Arbor,Cyber-Human Systems (CHS),EAGER: Identifying Barriers and Opportunities for Building SocioTechnical Capital,"This project advances a scientific understanding of how ""socio-technical capital"" - ties that are created, maintained, or exploited through the use of information and communication technologies - is developed and used across different socioeconomic groups and populations. It is hypothesized that, like other valuable resources, the benefits of computer-mediated opportunities for building socio-technical capital are unequally distributed in society. For example, it appears as if far more effort has gone into building social networking tools and online markets for highly paid professionals (such as LinkedIn) than for handymen or day laborers. And online labor markets that exist for low-skill, low-commitment jobs (such as Amazon Mechanical Turk) do not appear to offer a path toward building socio-technical capital that might lead to more-stable, higher-wage jobs.\n\nThis project seeks to understand the prospects of tailoring the technologies of social networking tools and online labor markets to meet the needs of specific socio-economic populations, such as populations in Detroit, Michigan, and other cities in economic decline. The project will follow a human-centered approach of contextual inquiry, conducting interviews and focus groups employing a range of ""design probes"". These probes will examine technologies that currently support the employment process (such as LinkedIn, CareerBuilder, ODesk, TaskRabbit, and Angie's List) as a springboard to identify fundamental barriers to usage, and also to generate ideas for features that might be especially useful. In later sessions, design probes will include low-fidelity prototypes embodying features generated in earlier sessions. The end result will be an articulation of the special needs, barriers, and opportunities for using technology to help people in economically vulnerable communities to build, maintain, and use social capital to start moving up the economic ladder. The project will investigate how information and communication technology can help to create and maintain of social and economic bridges between individuals within specific economic communities and people who can provide access to employment opportunities outside of those communities.\n\nBroader Impacts: The project will have broad social impact by informing approaches to cultivating pathways to upward mobility in communities hit hardest by economic decline. If the research finds promising opportunities and surmountable barriers to the use of social networking tools and online labor markets, it will inform the design of future technologies and computer-mediated approaches to help these populations better prosper. If the research finds limited opportunities or insurmountable barriers, practitioners will know to look elsewhere to help vulnerable populations find essential socio-technical capital.",1352915,20,linkedin
Saurabh Amin,,amins@mit.edu,Massachusetts Institute of Technology,CYBER-PHYSICAL SYSTEMS (CPS),"CAREER: Resilient Design of Networked Infrastructure Systems: Models, Validation, and Synthesis","This project advances the scientific knowledge on design methods for improving the resilience of civil infrastructures to disruptions. To improve resilience, critical services in civil infrastructure sectors must utilize new diagnostic tools and control algorithms that ensure survivability in the presence of both security attacks and random faults, and also include the models of incentives of human decision makers in the design process. This project will develop a practical design toolkit and platform to enable the integration of resiliency-improving control tools and incentive schemes for Cyber-Physical Systems (CPS) deployed in civil infrastructures. Theory and algorithms will be applied to assess resiliency levels, select strategies to improve performance, and provide reliability and security guarantees for sector-specific CPS functionalities in water, electricity distribution and transportation infrastructures. The main focus is on resilient design of network control functionalities to address problems of incident response, demand management, and supply uncertainties. More broadly, the knowledge and tools from this project will influence CPS designs in water, transport, and energy sectors, and also be applicable to other systems such as supply-chains for food, oil and gas. The proposed platform will be used to develop case studies, test implementations, and design projects for supporting education and outreach activities. \n\nCurrent CPS deployments lack integrated components designed to survive in uncertain environments subject to random events and the actions of strategic entities. The toolkit (i) models the propagation of disruptions due to failure of cyber-physical components, (ii) detects and responds to both local and network-level failures, and (iii) designs incentive schemes that improve aggregate levels of public good (e.g., decongestion, security), while accounting for network interdependencies and private information among strategic entities. The validation approach uses real-world data collected from public sources, test cases developed by domain experts, and simulation software. These tools are integrated to provide a multi-layer design platform, which explores the design space to synthesize solutions that meet resiliency specifications. The platform ensures that synthesized implementations meet functionality requirements, and also estimates the performance guarantees necessary for CPS resilience. This modeling, validation, exploration, and synthesis approach provides a scientific basis for resilience engineering. It supports CPS education by providing a platform and structured workflow for future engineers to approach and appreciate implementation realities and socio-technical constraints.",1453126,29,private information
Yingying Chen,,yingche@scarletmail.rutgers.edu,Stevens Institute of Technology,Secure &Trustworthy Cyberspace,"EAGER: Collaborative Research: Towards Understanding Smartphone User Privacy: Implication, Derivation, and Protection","This project aims to address privacy concerns of smartphone users. In particular, it investigates how the usages of the smartphone applications (apps) may reshape users' privacy perceptions and what is the implication of such reshaping. There has been recent work that investigates privacy leakage and potential defense mechanisms. However, so far there is only limited understanding on the consequences of such privacy losses, especially when large amount of privacy information leaked from smartphone users across many apps. The project seeks to investigate how the mobile technology (i.e., smartphone and smartphone apps) can reveal users' personal information and identify the consequences of privacy violations, by taking users' social relationships into consideration. \n\nThe project facilitates a deep understanding of user privacy in the age of mobile devices and further develops appropriate protective mechanisms. Smartphone user privacy across different levels are analyzed including individual, social and community relationships based on different levels of information leakage. Statistical models, such as Bayesian networks and hidden Markov models, are developed to understand users' temporal privacy leakage patterns based on large-scale trace-driven investigation and experimental study. Data visualization tools are developed to capture and display the spatial-temporal patterns and summary statistics of different types of privacy leakage in real time, which helps users gain better insights on potential privacy losses. The statistical modeling and the data visualization techniques further enable the social scientists to study the psychological or social consequences of privacy violations, and identify factors encouraging attention or inattention to smartphone user privacy.",1450091,3,personal information
Gang Wang,,gangwang@vt.edu,Virginia Polytechnic Institute and State University,Secure &Trustworthy Cyberspace,CAREER: Machine Learning Assisted Crowdsourcing for Phishing Defense,"This project aims to address the growing threat of phishing attacks, messages that try to trick people into revealing sensitive information, by combining human and machine intelligence. Existing detection methods based on machine learning and blacklists are both brittle to new attacks and somewhat lenient, in order to avoid blocking legitimate messages; as a result, widely used email systems are vulnerable to carefully crafted phishing emails. To address this, the project team will develop systems that automatically block obvious scams while forwarding less certain cases to groups of crowd workers trained to detect phishing mails. To support these workers' decision-making, the team will develop novel explanations of the system's decision making that will highlight the aspects of both the message and its algorithm that triggered the need for human judgment. The system will also aggregate these crowd decisions to generate real-time phishing alerts that can be shared to both individual users and to email systems. The project will lead to advances in interpretable machine learning, an important topic given the increasing role that artificial intelligence and machine learning systems play in society, and also increase our ability to characterize the evolution of phishing attacks and the vulnerability of internet platforms and users to those attacks over time. The project team will also use the work as an important component of new courses on usable security and outreach programs to high school teachers and students to both educate them about and increase their participation in cybersecurity research.\n\nThe work is organized around three main objectives: empirical characterization of phishing risks, developing accurate and interpretable machine learning models for phishing detection, and developing reliable crowdsourcing systems for phishing alerts. The team will assess phishing risks through developing analytics tools on the effective adoption and configuration of anti-spoofing protocols in email systems, using adversarial machine learning methods to conduct black box testing on existing phishing detectors, and creating reactive honeypots that entice and respond to phishing attacks in order to collect data on not just the initial phishing emails but on attackers' behaviors throughout the course of a successful phishing attack. The data collected on phishing emails will be used to develop the machine learning models, using Convolutional Neural Network and Long Short-Term Memory based deep learning techniques to generate both suspicious features and confidence estimates of individual decisions. The suspicious features will be used to generate interpretable security cues such as text annotations or icons by first creating simpler and more interpretable machine learning models such as decision trees that mimic the local detection boundary near the target emails in the feature space. Rules in the decision tree will be mapped back to interface elements and email content to provide the warnings, and these will be compared to generic email security warnings in a series of user studies that also model people's ability to detect phishing using a variety of cues, features, and media. Those individual models, along with the confidence estimates from the phishing detection model, will then be used to drive a crowdsourcing-based system where the models of individual users' quality will be aggregated to make reliable judgments around emails the models judge as too suspicious to pass but not suspicious enough to automatically filter.\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",1750101,3,sensitive information
Raquel Hill,,ralhill@indiana.edu,Indiana University,Secure &Trustworthy Cyberspace,EAGER: Leveling the Digital Playing Field for the Job Seeker,"This project aims to assess how online data impacts the hiring process. In an ideal situation, one might imagine that employers hire the most skilled applicant, but sociological research indicates that this may not be the case. A job applicant's similarity to the interviewer in class background and class-based leisure activities often matters as much or more to employers than a job applicant's skills or work experience. The ability of a recruiter or employer to learn such information from seemingly unrelated data has led researchers to express concerns about privacy, job relevance, and the potential for discrimination. This is the first stage in a larger project that aims to illuminate how online information impacts the ability of job seekers to find employment in post-recession United States. The project creates a framework for identifying systematic patterns of discrimination in regionally specific job markets, and also provide a fuller picture of precisely when in the hiring process are certain forms of discrimination likely to take place (upon submission of resume, at interview stage, and so on). In addition, the collected data will enable job seekers to discover how online information affects their employability, and aid the development of strategies to align online data and professional profiles. \n\nThe researchers will develop a novel mixed-methods framework to better understand the hiring process and study hiring discrimination by combining ethnographic studies of employers and companies that aggregate applicant profiles; surveys of applicants' background, skillset, and job-seeking history; online profile aggregation; and traditional data mining techniques. This project will contribute to the understanding of how employment works in the United States, and the types of online information that may limit employability. The problem will be addressed across populations that have varying demographic profiles and skill sets, and whose primary industries vary greatly. The proposed analysis will capture regionally specific hiring practices and reveal insights into the kinds of demographic indicators that work for or against job seekers in different regions.",1537768,3,online profile
Kevin Chang,,kcchang@cs.uiuc.edu,University of Illinois at Urbana-Champaign,INFO INTEGRATION & INFORMATICS,III: Small: Social Discovery of Users and Content in Social Media Through Similarity-Based and Graph-Based Inference of Attributes and Queries,"This project aims to develop graph-based tools to discover the people and content to pay attention to in social media that are most relevant to a given question or goal. This is an important problem for applications including marketing, event detection, civic participation and governance, and disaster management, and a hard problem because there is so much content and so many people but not enough information about their attributes and attitudes to make good choices about who to listen to. The research team proposes to label people and content using similarity or generalized ""homophily"" principle, the idea that the closer people are to each other (like friends or neighbors or Twitter followers), the more likely they are to have things in common. The research team will develop SocialSense, a tool that uses this idea to guess labels for people and content based on how they are connected to their neighbors; these inferred labels will allow users to create more complex and accurate queries in social media. The team will work with existing partners to develop SocialSense and validate that it does better than current social media tools; they will also use the tool to support undergraduate and graduate classes around web search and databases.\n\nFor the problem of finding users, the team will represent users and content as nodes in a large social graph, where each of these and the edges between them has a set of demographic and attitudinal attributes. This project develops novel algorithms for graph-based learning/mining over social graphs as well as content graphs. Through mining patterns of connection in the network, the team will identify a set of structural motifs of homophily and use those motifs, as well as underlying probabilities of the occurrence of attributes, to propagate inferences about those attributes to other nodes, and check the quality and fairness of those inferences using a rejection sampling technique. For finding content, the team will represent content and queries in a graph and again mine common patterns, this time to create query templates that will support the creation of future queries as new topics and entities arise. Finally, the team will integrate these components, creating a system that supports querying across people and content and suggests interesting new queries based on discovering patterns of connected attributes that match the motifs and templates described above. The team will evaluate the methods and system through both offline back-end performance measurements and online deployments that evaluate usability, expressiveness, and simplicity of the systems in the context of their partnerships with a smart nation/citizen input project and a social mapping cloud service run by their institution.",1619302,19,social graph
Gari Clifford,,gari.clifford@emory.edu,Emory University,BD Spokes -Big Data Regional I,BD Spokes: SPOKE: SOUTH: Large-Scale Medical Informatics for Patient Care Coordination and Engagement,"This project brings together six universities to design and construct a patient-focused and personalized health system that addresses the fractured nature of healthcare information, and the lack of engagement of individuals in their own healthcare. By taking advantage of the enormous amount of information being created about our environment, through the confluence of real-time, mobile and wearable devices and the availability of rich social media data on patient behavior, the team will create a detailed and comprehensive picture of a patient's health, and a tool to help manage patients' engagement with their health care providers. The system has four key aims to: (1) provide a human-centered approach for integrating electronic health record data generated by traditional methods with data collected ""in the wild"" (such as personal fitness devices, mobile phone usage, local weather, pollution or even fast food restaurant maps, etc.); (2) develop a framework for deciding which data sources are trustworthy; (3) create a cloud-based system to allow users to view and track their own data over time and improve healthcare outcomes; and (4) provide educational outreach and community participation, particularly in minority populations, to design a system which benefits users in both the short term (through employment and education) and the long term (through increased engagement and trust).\n\nThis project will leverage modern distributed cloud-based computing infrastructure (including mobile phones and Amazon Web Services), and the unique capacities of the South BD Hub to house and analyze the enormous volumes of health-related data that are generated every day by people, and their environment. By linking electronic medical records, external databases and data 'in the wild' harvested from patient's Internet-enabled devices, the project will address several issues related to the integration of high-resolution data for longitudinal tracking of patients. These include acceptability of the technology, particularly by vulnerable groups, usability, veracity of data collected, and scalability/integration across a large heterogeneous landscape. By employing patient-centric agile development, the team will work with communities to implement a cloud-based architecture to improve tracking of study participants, increase the ease with which data can be captured, improve patient engagement, and facilitate care coordination. The resultant platform will integrate big data analytics, real time scalable data collection, and social media analytics on patient behavior to analyze cardiovascular disease outcomes among disadvantaged African American and Hispanic patient populations. Additionally, the team will implement data fusion techniques to ensure the veracity of the varying qualities of data collected, and develop machine learning models to identify at-risk patient populations in order to reduce health disparities. Finally, patient engagement and health outcomes will be measured to assess the validity and success of the system.",1636933,15,social media data
Liam O'Neill,,Liam.ONeill@unthsc.edu,University of North Texas Health Science Center at Fort Worth,Smart and Connected Health,SCH: EXP: Collaborative Research: Privacy-Preserving Framework for Publishing Electronic Healthcare Records,"This project builds a novel privacy-preserving framework with both new algorithms and software tools to: 1) evaluate the effectiveness of current identifier-suppression techniques for Electronic Healthcare Record (EHR) data; 2) de-identify and anonymize EHR data to protect personal information without significantly reducing the utility of data for secondary data analysis. The proposed techniques eliminate the violation of privacy through re-identification, and facilitate the secondary usage, sharing, publishing and exchange of healthcare data without the risk of breaching protected health information (PHI). This new privacy-preserving framework injects the ICD-9-CM-aware constraint-based privacy-preserving techniques into EHRs to eliminate the threat of identifying an individual in the secondary use of research data. The proposed technique and development can be readily adapted to other types of healthcare databases in order to ensure privacy and prevent re-identification of published data. The project produces groundbreaking algorithms and tools for identifying privacy leakages and protecting personal privacy information in EHRs to improve healthcare data publishing. New privacy-preserving techniques developed in this project lead towards a new type of healthcare science for EHRs. The project also delivers fundamental advancements to engineering by showing how to integrate biomedical domain knowledge with a computationally advanced quantitative framework for preserving the privacy of published EHRs. HIPAA has established protocols and industry standards to protect the confidentiality of PHI. However, our results demonstrate that, even with regard to health data that meets HIPAA requirements, the risk of re-identification is not completely eliminated. By identifying the security vulnerabilities inherent in the HIPAA standards, our research develops a more rigorous security standard that greatly improves privacy protections by applying state-of-the-art algorithms.\n \nThe developed data privacy-preserving framework has significant implications for the future of US healthcare data publishing and related applications. Specifically, the transition from paper records to EHRs has accelerated significantly since the passage of the HITECH Act of 2009. The Act provides monetary incentives for the ""meaningful use"" of EHRs. As a result, the quality and quantity of healthcare databases has risen sharply, which has renewed the public's fear of a breach of privacy of their medical information. This research work is innovative and crucial not only for facilitating EHR data publishing, but also for enhancing the development and promotion of EHRs. At the educational front, this project facilitates the development of novel educational tools to construct entirely new courses and laboratory classes for healthcare, data privacy, data mining, and a wide range of applications. As a result, it enhances the current instructional methods for teaching data privacy and data mining, and has compelling biomedical and healthcare applications that can facilitate learning of computational algorithms. This project involves both undergraduate and graduate students in the three participating institutions. The PIs make a strong effort to engage minority graduate and undergraduate students in research activities in order to increase their exposure to cutting-edge research.",1344072,7,personal information
Christina Fragouli,,christina.fragouli@ucla.edu,University of California-Los Angeles,CYBER-PHYSICAL SYSTEMS (CPS),CPS: Medium: Distorting the adversary's view: a CPS approach to privacy and security,"This project develops a novel Cyber Physical System (CPS) centric approach to privacy and security for wireless networked CPS systems, by reconciling the low-delay and low-jitter requirements of CPS applications with the requirements imposed by security and privacy. Our starting observation is that, in CPS, an adversary's primary goal is not to learn all the raw data, but instead core attributes, such as the state or control actions that are derived from data. Building on this observation, we propose to use a distortion measure for security that maximizes the difference between the eavesdropper's estimate and the true value of the function computing the attributes of interest, reducing the adversary's ability to disrupt normal operation of CPS. We posit that we can protect these core attributes with fewer resources than needed to protect all the raw data. Ensuring secure and private information exchange over networked CPS systems is essential to building a thriving ecosystem of applications that range from autonomous cars and drones, to the Internet-of-Things (IoT), to immersive environments such as augmented reality for health, education, and collaboration. Our educational plan engages not only graduate students and postdocs but also high school and undergraduate students. It also reaches out to engineers and the lay public, by providing open source implementations of our algorithms making them available both to industry and hobbyists. \n\nThe project considers both passive and active attacks. We will quantify novel privacy and security measures for CPS systems that are based on distortion measurements in a metric space; we will develop fundamental bounds as well as low complexity and low overhead coding schemes; we will quantify the disruptive power of active adversaries and design pro-active and retro-active defense mechanisms; and we will illustrate our approach over a flagship application, drone localization. Our approach will offer an alternative to wireless network encryption methods, by designing for low-delay, low-jitter requirements of CPS.",1740047,29,private information
Xue-Wen Chen,,xwchen@wayne.edu,Wayne State University,ROBUST INTELLIGENCE,EAGER: Large-Scale Distributed Learning of Noisy Labels for Images and Video,"This project develops algorithms for learning from images and video with noisy labels. The overwhelming amounts of images and video freely available online present unprecedented challenges for machine learning and computer vision research communities. They also bring tremendous opportunities and great potentials for addressing human-machine semantic gaps in image understanding and for revolutionizing our ways to index, retrieve, and interact with images and video. Inaccurate labels and mislabeled data are common problems for image and video datasets. Noisy labels would cause problems with the existing learning algorithms. This project can have broad impacts on other big data problem. The project is integrated with education by training students, ensuring broad participation of underrepresented groups, and outreaching general public.\n\nThis research exploring distributed learning methods for large-scale images and video with noisy labels. The PI investigates the learning problem of loss functions with both smooth and non-smooth regularization terms, and accordingly develops new distributed learning algorithms that are capable of leveraging the abundance of images that are too large to fit into a single machine. The research has an immense potential in image and video analysis, and computer vision applications. Specifically, this research emphasizes both algorithmic and theoretic aspects by (1) developing distributed learning based approaches for optimization and learning of noisy labels; and (2) investigating issues such as guaranteed convergence, convergence rate, and scalability. This work provides new methods that are widely applicable to many economically, medically and scientifically important large-scale datasets for novel discoveries across many domains.",1554264,18,freely available online
Katherine Heller,,kheller@gmail.com,Duke University,"METHOD, MEASURE & STATS, ROBUST INTELLIGENCE",CAREER: Interacting Dynamic Bayesian Models for Social Behavior and Reasoning,"This project develops machine learning methods to analyze online data and illuminate social behavior. People spend a great deal of their lives socializing, or interacting with other people. Social interactions are inherently part of most of our activities, and, therefore, understanding social interactions is a fundamental part of understanding human behavior. As the amount of time people spend online rapidly grows, social interactions are increasingly occurring online. This project uses Dynamic Bayesian models to analyze time series data of social behavior that inherently involves interactions over time. The project provides tools for understanding social behavior and better methods for data analysis. The project trains graduate students and helps high school students learn more about data analysis. The project principal investigator also maintains a strong commitment to actively involving in the organization of Women in Machine Learning Workshop to ensuring the advancement of women in STEM.\n\nThis research focuses on understanding the underlying structure of the interactions themselves. The research team elucidates this underlying structure through hierarchically modeling the interactions between multiple dynamic processes. More specifically, the project develops: (1) interacting dynamic Bayesian methods that can be used to model social interactions that jointly capture the temporal dynamics and the linguistic content of interactions between individuals, and discover latent attributes of individuals, such as power and influence, or roles such as bullies and victims; and (2) methods for analyzing epidemiological social network data.",1553465,18,social network data
Melanie Mitchell,,mm@cs.pdx.edu,Portland State University,ROBUST INTELLIGENCE,RI: Small: Visual Situation Recognition: An Integration of Deep Networks and Analogy-Making,"This project investigates a novel approach to building computer systems that can recognize visual situations. While much effort in computer vision has focused on identifying isolated objects in images, what people actually do is recognize coherent situations--collections of objects and their interrelations that, taken together, correspond to a known concept, such as ""a child's birthday party,"" or ""a man walking a dog on the beach,"" or ""two people about to fight,"" or ""a blind person crossing the street."" Situation recognition by humans may appear on the surface to be effortless, but it relies on a complex dynamic interplay among human abilities to perceive objects, systems of relationships among objects, and analogies with stored knowledge and memories. No computer vision system yet comes close to capturing these human abilities. Enabling computers to flexibly recognize visual situations would create a flood of important applications in fields as diverse as medical diagnosis, interpretation of scientific imagery, enhanced human-computer interaction, and personal information organization.\n\nThe approach explored in this project integrates two previously studied approaches: brain-inspired neural networks for lower-level vision and cognitive-level models of concepts and analogy-making. In this integrated architecture, recognizing situations--via analogies with stored conceptual structures--will be a dynamic process in which bottom-up (perceptual) and top-down (conceptual) influences affect one another as perception unfolds. If successful, this system will be able to recognize visual situations in a way that scales well with the complexity of the scene and the abstract concept being recognized. As part of this project, a number of benchmark image datasets--reflecting different abstract visual situations--will be collected to evaluate the recognition system. In addition, the PI will design and run a public competition on automated recognition of visual situations, using the collected datasets. This competition will spur research on this topic, and help researchers working in this area evaluate the success of various methods and gauge the current state of the art on abstract visual recognition. All source code and benchmarking databases developed in this project will be made publicly available via the web.",1423651,18,personal information
Anand Sarwate,,anand.sarwate@rutgers.edu,Rutgers University New Brunswick,Secure &Trustworthy Cyberspace,TWC: Small: PERMIT: Privacy-Enabled Resource Management for IoT Networks,"This project investigates how privacy can be used to inform the design and management of future data sensing systems. Networked systems that collect data about individuals will play an increasingly important role in our lives, with applications including industrial monitoring and control, ""smart"" homes/cities, and personalized health care. These systems will gather private information about individuals, which creates many coupled engineering challenges. This work seeks to understand the interplay between two of these: managing the massive amounts of data that must be collected and protecting the privacy of individuals in the system. For applications or services that rely on populations of individuals, reducing the amount of information transmitted can save bandwith while enhancing privacy. The objectives of this work are to use ideas from data privacy technologies and wireless resource management techniques to jointly manage privacy and bandwidth in wireless sensing systems.\n\nThe technical objectives of this project involve reformulating distributed data collection and estimation problems under privacy constraints in wireless network settings. Differential privacy gives a framework for quantifying the privacy risk of different strategies for compression and data reduction, leading to a privacy-bandwidth-quality tradeoff. At the system level, total privacy risk is a resource that can be managed; this project uses that insight to design novel privacy-allocation schemes. Ultimately, this project seeks to formulate new tradeoffs between privacy and quality-of-service that can generalize to other networking problems. To see how these approaches work in practice, this work involves prototyping and testing the methods in a wireless networking testbed.",1617849,3,private information
Lauren Wilcox,,wilcox@cc.gatech.edu,Georgia Tech Research Corporation,Cyber-Human Systems (CHS),"CAREER: Adaptive, Collaborative User Interfaces for Chronically Ill Adolescents' Personal Data Management","This project is about developing collaborative technologies for tracking personal information, in the context of helping teenagers with chronic health conditions track data to manage their illnesses. Increasingly, data such as sleep duration and physical activity can be inferred through fitness trackers and smartphone sensors, but personal experiences such as symptom tracking, pain, and sleep quality still need to be tracked manually. This is effortful, especially for teenagers with immature self-care abilities. Thus, one key aim of the project is to design collaborative strategies for collecting self-report data that vary the timing, method, and person used to collect the data to maximize the quality and consistency of data collected for particular teens, given their condition, self-care abilities, and family situation. A second key aim is to align this self-report data with data sensed by smartphones and fitness trackers, developing algorithms and visual displays that help teens and families review the data together and make good decisions about the teens' condition. For both data collection and review, the tools developed will need to be sensitive to problems such as differences in parents' and teens' views of the condition and teens' needs to become independent and establish personal identity. To do this, the research team will work very closely with families with chronically-ill teens, doing field studies of how families currently manage this sort of tracking, interviews with teens and parents both together and separately, and short-term studies with prototype tools as they build and improve them. The project will end with a long-term evaluation of the tools to see how they affect teens' attitudes and abilities related to self-care, as well as how they work with their families and doctors. The work will inform the lead investigator's courses on human-computer interaction and personal health informatics, and support the development of summer camp outreach activities to K-12 students, using health informatics to increase their interest in STEM research.\n\nThe research work will proceed in three main phases, working closely with adolescent patients and doctors at multiple medical centers in the Atlanta area. The first phase involves formative work around the first aim of eliciting personal health care information and defining requirements for mobile sensing applications that embody collaborative data collection, presentation, and management techniques. This will involve interview studies with teens, parents, and caregivers, to elicit their attitudes toward and perceptions of collaborative personal health management technologies. It will also utilize pilot deployments of strategies for collecting data through ecological momentary assessments and experience sampling, with frequency and timing guided in part by automatically-sensed data. The second phase aims at building a fully functional prototype system, with three main components: (1) implementation of data collection strategies proven effective in the first phase; (2) development of augmented data fusion techniques that triangulate self-reported with sensed data; and (3) creation of both algorithms for analyzing the fused data and visualizations that support these analyses. This prototype will then drive the third phase, an 18-month study with 60 families in three groups: one that will use the full prototype, one a version that replaces the collaborative elicitation strategies with an existing, standard sampling strategy, and one as a control group. The analysis will examine how the system affects both adolescents' and parents' privacy concerns, acceptance of both self-reported and sensed mobile data collection in general and with this tool in particular, the effort required to use the tool, and adolescents' self-efficacy and health locus of control. The evaluation will utilize both standard survey instruments for quantitative measures and interviews with adolescents as they use the tools over time.",1652302,20,personal information
Malte Jung,,mfj28@cornell.edu,Cornell University,Cyber-Human Systems (CHS),CHS: Small: How recommendation and explanation affect preferences in social networks,"This project seeks to advance research on a novel perspective on recommendation called network-centric recommender systems. Sharing information with others is a key driver of social media activity. But, surprisingly, relatively little is known about how people make choices around what to share and to read, or whom to share with and pay attention to. Unlike traditional e-commerce and research contexts that focus on accurate prediction for individuals at a given moment, the network-centric perspective sees recommendation as a dynamic social process. Understanding more about the factors that influence recommendation processes will have practical impact on the design of social media platforms and recommender systems, leading to tools that better serve both individual and societal goals around sharing information. It will also set the stage for an understanding of how information spreads in social networks that is deeper than current diffusion models that fail to account for these factors, allowing scientists to build more accurate and more general models of sharing behavior.\n\nA series of laboratory experiments will ask people to make and take recommendations from each other. These experiments will use real social media data such as Facebook Likes and Twitter hashtag usage, but control the algorithms, interfaces, and partners people interact with in the experiment. Doing this will help weigh the importance of factors that influence people's sharing and reading behavior. Among these factors are how users are influenced by goals such as self-expression and helping people, system characteristics such as filtering algorithms and social explanations for shared items, and social forces such as similarity and trust between a sharer and a receiver.",1422484,20,social media data
Grant Schoenebeck,,schoeneb@umich.edu,University of Michigan Ann Arbor,ALGORITHMIC FOUNDATIONS,"CAREER: Social Networks - Processes, Structures, and Algorithms","This project seeks to develop a rigorous theoretical understanding of complex and strategic network processes, network structure, and algorithms for network properties. Social networks are an abstraction used to study social structure via pair-wise social interactions, and have proven useful in analyzing how local actions affect global trends. Better understanding of social networks promises a better understanding of and the ability to influence a wide range of phenomena, including: what technologies/practices people and firms adopt, how information is transmitted and aggregated, and how network structure relates to the agents' ability to search within the network. In all of these instances individuals' activities can have a global-scale impact, which is mediated by a network. The increasing presence of computer-accessible data (e.g., websites, user-generated content, usage data from telecommunications, apps, web-browsing, etc.) has rekindled an interest in this field because of the new ability to gather data to test theories on a large scale. This project seeks to develop new algorithms and theoretical frameworks to help fully make use of these data.\n \nThis project will develop and apply traditional tools, insights, and approaches from theoretical computer science including functional analysis, graph theory, combinatorics, linear algebra, probabilistic analysis, linear and semidefinite program hierarchies, complexity theory, and game theory to the study of network processes and structure. This project will transform the way we use social network data by: 1) developing the technical tools required to achieve a better understanding of specific complex and strategic processes (including those mentioned above), 2) identifying network structures that are efficiently verifiable and are useful for understanding nuances in the network processes, and 3) improving our understanding of certain network processes by explicitly accounting for agents' strategic reasoning. The technical content of this project will have direct applications to related fields such as probability, economics, sociology, and statistical physics. Additionally, a key goal of this project is to move beyond worse-case analysis; if successful, this will pave the way to exporting theoretical computer ideas to many disciplines where their current application is limited due to its fixation on worst-case hardness---in particular fields that feature networks such as biology and epidemiology. The PI plans to develop curriculum to introduce computer science topics to high school students and involve undergraduates in his research.",1452915,27,social network data
Katrina Ligett,,katrina@caltech.edu,California Institute of Technology,Secure &Trustworthy Cyberspace,CAREER: The Value of Privacy,"This project takes a new approach to problems involving sensitive data, by focusing on rigorous mathematical modeling and characterization of the value of private information. By focusing on quantifying the loss incurred by affected individuals when their information is used -- and quantifying the attendant benefits of such use -- the approaches advanced by this work enable concrete reasoning about the relative risks and rewards of a wide variety of potential computations on sensitive data. \n\nSpecifically, this work has four main technical thrusts. The first is the development of new models and definitions, enabling privacy considerations to be incorporated into agent utility functions. The second is analysis of the feasibility and costs of eliciting sensitive information, in light of these models. The third focus is on enabling more sophisticated computations in settings where individuals value their privacy. Finally, more complex settings incorporate the interests of additional actors.\n\nOne of the goals of this project is not only to develop a science of the value of private information, but to build bridges between computer science and economics that will enable such work. Further, the models and algorithms developed by this project could inform future regulation regarding the use, exchange, and monetization of sensitive data. The project supports and is supported by a wide variety of educational goals, including significant research involvement of students at a range of stages, development of a course series with a substantial research component, and assessment of a pedagogical technique created to facilitate meaningful engagement with research literature.",1254169,3,"sensitive information, private information"
Glynda Hull,,glynda@berkeley.edu,University of California-Berkeley,"ITEST, Cyberlearn & Future Learn Tech",Collaborative Research: Designing the Impact Studio -- Dynamic Visualizations in the Write4Change Networked Community,"This project will advance efforts of the Innovative Technology Experiences for Students and Teachers (ITEST) program to better understand and promote practices that increase students' motivations and capacities to pursue careers in fields of science, technology, engineering, or mathematics (STEM) by engaging them in quantitative data analysis to support their own communication skills. The amount and variety of information generated and shared online requires young people to be adept at effectively producing, analyzing, assessing, using, visualizing, and circulating data. They must be data literate. This project investigates how adolescents develop data literacy by teaching young people to analyze their own online participation and to learn how to make their digital footprints have greater impact. Specifically, researchers will build and test a suite of visualization tools called the ""Impact Studio"" that will give students access to data about their online interactions. The ""Impact Studio"" will be developed and tested with students and educators who participate in a global online community called ""Write4Change"" (W4C). In this virtual community, youth and their teachers communicate online about issues of local and global concern -- for example, climate change, immigration, peace and conflict -- and explore how their own ideas and activities can contribute to solutions. Using the ""Impact Studio,"" students will learn to deploy data visualizations as a means to test, revise, and more clearly express their concepts and proposals. Designing, developing, and testing digital tools that support students in becoming productive, civically engaged 21st century citizens, this project will contribute to scientific knowledge about how young people learn to strategically leverage data in order to produce, assess, and circulate information in a global arena.\n\nAs a design-based research study, the project develops and tests theories about how youth engage in data literacy practices to write for impact. Though the use of data is an essential part of analyzing, producing, and circulating information in STEM fields, this study broadens that focus to consider how the strategic use of data can inform digital writing. Across four iterative design phases that focus on building, testing, enhancing, and implementing the integrated visualization tools in the Impact Studio, the researchers will study 1) how the Impact Studio's visualized data displays influence student collaboration; 2) how students use data from the Impact Studio in communicating cross-culturally; and 3) how the capacity to create, critique, and manage multimodal representations allows students to leverage visualized data in their writing process (including the content of what they write, how they share and circulate their writing, and how they revise). The research team will first conduct design workshops and user testing on a prototype to refine the feature requirements for Version 1 of the Impact Studio. Researchers will then collect quantitative and qualitative data that track network activity and Impact Studio usage from W4C members (including interviews; participant observation; content analysis; temporal, frequency, citation, and interaction data from the network; surveys; digital competencies; skill inventories; and knowledge measures). After a period of data analysis, tool redesign, and user testing, the research team will undertake another period of data collection and analysis. This project will develop key principles about how to support students in generating and navigating vast amounts of data, critically interpreting and creating visual and textual representations of those data, and measuring the impact of their work on cross-cultural audiences.",1623468,5,interaction data
Kenneth Anderson,,kena@cs.colorado.edu,University of Colorado at Boulder,Cyber-Human Systems (CHS),CHS: Medium: Hyperlocal and Hypertemporal Information in Mass Emergencies Events: Next Generation Crisis Informatics Data Collection and Analytics,"This project will advance the empirical study of crisis informatics by moving the extensive analytics developed for post processing of social media-based crisis information to the point of data collection to create a set of intelligent data collection techniques. Crisis informatics is a multidisciplinary field combining human-computer interaction and social computing; its central tenet is that people use personal information and communication technology to respond to disaster in creative ways to cope with uncertainty. This research will reveal how people respond in disasters, both collectively and individually, by examining their digital traces. This will help create better emergency management practice and policy. The research will also advance the delivery of the information people need in disasters by leveraging the vast social media data sets that are produced during them. The safety of people and provision of emergency care as a function of improved situational awareness are the direct benefits of conducting this research.\n\nPast research shows that people local to an event seek and provide information online to help themselves and each other, and that this information is highly localized and temporalized when compared to the global conversation that otherwise arises. Current techniques to derive useful information from the social media stream are limited because they usually cannot scale to the vast growth in social media production; they are often restricted to examining only the language of social media posts and not their metadata; and social media are often collected in ways that prohibit the kinds of analyses that are needed for deep exploration and re-use of the data. This research relies on the body of work that the investigators have empirically conducted on the nature of social media behavior in disasters, and incorporates those rigorous deductive and inductive analytical methods at the point of data collection of the social media streams. This will enable intelligent data collection that can perform real-time analysis that bore in on the hypertemporal and hyperlocal features of information communicated via social media. With agility and rigor, solutions developed in this research will be able to pursue multiple threads of emergent concern that will spawn multiple simultaneous data collection trajectories. This will better enable real-time analysis and visualization of what is happening in the social media record during disaster events as they unfold.",1564275,20,"social media data, personal information"
Milos Zefran,,mzefran@uic.edu,University of Illinois at Chicago,ROBUST INTELLIGENCE,RI: Medium: An Interaction Manager for Language and Force Exhanges in Human-Robot Physical Collaboration,"This project will develop a computational and data-driven framework for a robot assistant to collaborate with humans in everyday tasks that involve physical interaction, such as handing over or moving an object together. Using models learned from observing humans perform such tasks, the robot will engage in back-and-forth communication, where turns can be both spoken utterances and force exchanges. Robots that can collaborate to perform physical tasks could provide assistance in a variety of settings, such as performing household chores, supporting the elderly to remain independent, and assisting human workers on the factory floor.\n\nThe transformative idea of the proposal is to generalize the methodology of dialog processing to include physical interaction. The fundamental challenge is how to bridge the gap between the symbolic processing of language and the low-level control of force exchanges. The concept of interaction primitives (IPs) is introduced to model physical interactions. Further, a planning and execution framework in the form of an interaction manager is proposed. The proposed interaction manager broadens the traditional dialogue modeling paradigm so that information can flow from more abstract to lower levels, and vice versa: language affects physical interaction, and physical interaction affects what is said. To build the interaction manager, a targeted data collection where humans perform tasks of interest will be performed. Since physical interaction data is invariably sparse, statistical learning on the data will be complemented by model-based generalizations, allowing robots to collaborate with humans in highly variable and unstructured environments. The research will inform new course development, and involve several undergraduate and graduate students.\n",1705058,18,interaction data
Shaowen Wang,,shaowen@illinois.edu,University of Illinois at Urbana-Champaign,"GEOGRAPHY AND SPATIAL SCIENCES, DATANET",CIF21 DIBBs: Scalable Capabilities for Spatial Data Synthesis,"This project will develop a set of tools for spatial data synthesis through scalable data aggregation and integration based on cloud computing, CyberGIS, and other existing tools. Many scientific problems require the aggregation and integration of large and varied spatial data from a multitude of sources, yet existing approaches and software cannot effectively synthesize the enormous amounts of spatial data that often are available. This project will resolve problems associated with the use of massive spatial data, thus facilitating work dependent on this type of data for scientific problem solving, such as research on population dynamics and urban sustainability. Learning materials derived from the research activities will be openly accessible through the CyberGIS Science Gateway. Targeted massive open online course development will provide inexpensive and efficient ways to teaching students about the capabilities and underlying scientific principles of spatial data synthesis. A summer school will be offered during the second half of the project to provide a more focused and in-depth training event.\n\nThis research project will create scalable capabilities for spatial data synthesis enabled by cloud computing and CyberGIS. The project will begin by developing the capabilities for solving specific scientific problems and then move on to engage a broader community for validating and improving the core capabilities. The research will incorporate two interrelated themes: (1) measuring urban sustainability based on a number of social, environmental, and physical factors and processes; and (2) examining population dynamics by synthesizing multiple population data sources with social media data. Spatial data synthesis capabilities that the project will provide include extracting metadata and dealing with problems of spatial references and units. The project also will develop a fundamental capability to characterize uncertainty in data and its propagation.",1443080,9,social media data
Sharad Mehrotra,,sharad@ics.uci.edu,University of California-Irvine,INFO INTEGRATION & INFORMATICS,III: Small: Linking and Resolving Entities in Big Data,"This project will explore the challenge of cleaning data in the context of analysis pipelines over big data. Data cleaning has traditionally been designed to improve data quality in ETL systems where enterprise data is collected, prepared, staged, transformed, and loaded into a data warehouse to support offline data analysis. In the era of big data, such back-end processes are quickly giving way to interactive exploratory data analysis where analysts immerse themselves in data (possibly collected from heterogeneous data sources) in order to drive online (near-) real-time decision making. Existing systems do not scale to the volume, velocity, or the variability of the dynamically generated data (e.g., social media streams) and the offline architecture is unsuited for the online real-time nature of analysis. The market is abuzz with innovations in data transformation technologies, e.g., TriFacta allows analysts to visually manipulate data to generate complex analytical transformations and Data Tamer is exploring scalable data curation from diverse sources. Data quality (and hence data cleaning technologies) remain at the core of big-data analytics. Many popular media (as well as academic) articles have highlighted challenges such as entity linking and resolution as among the most important and immediate roadblocks for big data analytics. The key insight on which this project is based is that data cleaning to support analytics over big data is not simply a matter of scaling up known approaches to larger data sets by exploiting more hardware. While scale up is important, big data analytics in streaming, real-time, and interactive settings requires a paradigm shift in how data cleaning is performed. This project will significantly impact and change the modern practices of data cleaning and the way cleaning is integrated in the Big Data analysis pipeline and will explore broader impact through: (a) technology transfer opportunities with a relevant industrial partner whose existing products could benefit from the proposed research; and (b) open source effort in the context of the ongoing social media analytics system (SoDAS), currently under development, in which the proposed research algorithms will be integrated.\n\nThis research will explore two new innovations that will help advance data cleaning to enable Big Data analysis. The first innovation explores a progressive approach to entity resolution to support progressive analysis. The research will explore an approach where progressiveness is pervasive spanning all the phases of the cleaning process especially in scenarios when cleaning is based on complex logic possibly requiring dynamic acquisition of additional contextual information. The second innovation is the analysis-aware data cleaning that is developed for structured queries (e.g., Hive and SQL) for both one-time and continuous query scenarios that are issued on top of static and streaming data. The project will address these methodologies at the higher conceptual level as well as implement them on modern highly-parallel computing platforms and frameworks that run on a cluster of machines. The project will exploit two concrete contexts to guide the research exploration: (a) supporting analytical queries over structured web data sources such as fusion tables; and (b) online analysis of social media data. These application contexts will serve as vehicles for testing and demonstrating the research. The planned research, system development, and educational activities (e.g., curriculum changes to incorporate projects related to big data and data quality in the CS curriculum at UCI) will significantly enhance the educational experience of students, preparing them for a brighter future in the today?s knowledge driven society. More information about the project can be found at http://sherlock.ics.uci.edu.",1527536,19,social media data
Kate Starbird,,kstarbi@uw.edu,University of Washington,Cyber-Human Systems (CHS),"CAREER: Unraveling Online Disinformation Trajectories: Applying and Translating a Mixed-Method Approach to Identify, Understand and Communicate Information Provenance","This project will improve our understanding of the spread of disinformation in online environments. It will contribute to the field of human-computer interaction in the areas of social computing, crisis informatics, and human centered data science. Conceptually, it explores relationships between technology, structure, and human action - applying the lens of structuration theory towards understanding how technological affordances shape online action, how online actions shape the underlying structure of the information space, and how those integrated structures shape information trajectories. Methodologically, it enables further development, articulation and evaluation of an iterative, mixed method approach for interpretative analysis of ""big"" social data. Finally, it aims to leverage these empirical, conceptual and methodological contributions towards the development of innovative solutions for tracking disinformation trajectories.\n \nThe online spread of disinformation is a societal problem at the intersection of online systems and human behavior. This research program aims to enhance our understanding of how and why disinformation spreads and to develop tools and methods that people, including humanitarian responders and everyday analysts, can use to detect, understand, and communicate its spread. The research has three specific, interrelated objectives: (1) to better understand the generation, evolution, and propagation of disinformation; (2) to extend, support, and articulate an evolving methodological approach for analyzing ""big"" social media data for use in identifying and communicating ""information provenance"" related to disinformation flows; (3) to adapt and transfer the tools and methods of this approach for use by diverse users for identification of disinformation and communication of its origins and trajectories. More broadly, it will contribute to the advancement of science through enhanced understandings and conceptualization of the relationships between technological affordances, social network structure, human behavior, and intentional strategies of deception. The program includes an education plan that supports PhD student training and recruits diverse undergraduate students into research through multiple mechanisms, including for-credit research groups and an academic bridge program.\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",1749815,20,"social data, social media data"
Chirag Shah,,chirags@rutgers.edu,Rutgers University New Brunswick,DATANET,Collaborative Research: BCC-SBE: Building Communities for Transforming Social Media Research with SOCRATES: SOcial and CRowdsourced AcTivities Extraction System,"This project will result in a novel system called SOCRATES that will help transform social media research for scholars working in diverse fields by building a community of researchers and practitioners around various issues of data-intensive research. Social media services such as Twitter and Facebook, used by millions of people worldwide, expose vast amounts of data about people's beliefs, ideas, opinions, behaviors, and activities. At the same time, the sheer scale and volume of the data make them extremely difficult for scholars to study effectively. SOCRATES will address this issue by incorporating a set of socio-computational tools that will allow researchers from multiple fields to collect large-scale social media data; explore and visualize the resulting content items, and analyze the collected content. A community- and human-centered approach to developing the new system will ensure that SOCRATES matches researchers' work practices and mental models, is easy to use, and produces outcomes that significantly contribute to the researchers' goals, especially in solving multi-disciplinary problems. Importantly, the SOCRATES system will employ a social-computational approach--crowdsourcing--to handle some of the challenges of social media research. Thus, the project will take advantage of the intelligence of both computers and people to study online social activities. SOCRATES proposes to use the labor of humans to assist in the collection of data (e.g., by refining and filtering information collected by an automatic crawler); to help explore the data and generate insights (e.g., by allowing the public to view and comment on visualizations of the collected data); and to analyze and annotate the data (e.g., by creating a controlled environment where coders can annotate content items with high reliability). As a result, SOCRATES will provide a first-of-its-kind, end-to-end environment where social media can be studied effectively, with high validity, and at immense scale.",1244704,30,social media data
Jed Brubaker,,jed.brubaker@colorado.edu,University of Colorado at Boulder,Cyber-Human Systems (CHS),CRII: CHS: Humanizing Algorithms: Empirical and Design Investigations of Sensitive Algorithmic Encounters,"This project will study algorithmic interactions and develop strategies for the human-centered design of systems that incorporate algorithms and their underlying data. Software developers of platforms of all kinds are creating features that make use of algorithmically curated content that leverage data about people's relationships, behavior, and identities. However, algorithms usually make decisions based on system metrics that are readily calculable, such as the number of likes, plays, and clicks. Even more sophisticated algorithms are limited by the social information explicitly given or inferred from provided data. As a result, algorithms can fail to capture the social context and human meaning that is important to the acceptability and success of the interactions these algorithms are meant to support. The research will investigate both algorithmic and human understandings of social data, especially when they diverge. By attending to divergence, the research can examine human expectations of algorithms, how misunderstandings might be reframed, and how subsequent action is informed by those divergences. \n\nSpecifically, this project will identify (1) how people navigate sensitive algorithmic encounters; (2) how these encounters impact people; (3) what social concepts algorithms are failing to understand; and (4) what design strategies are needed to address sensitive content in algorithmic curation. To focus this work, the specific context of inquiry will be algorithmic encounters with content related to loss of life, given its prevalence and sensitivity at both communal and individual levels. The broader impacts of the work include: (1) developing guidelines around the curation of and interactions with social data related to loss of life, which can also be applied to other groups and experiences where algorithms should be sensitive; (2) demonstrating how designs that incorporate social data can adopt human-centered approaches to sensitize encounters with algorithmically curated content; (3) contributing to the development of design practices that encompass the design of interactions, systems, algorithms, and data; and (4) engaging students in multiple fields, including Information Science, Computer Science, Media Studies, and Communication through research and curricular activities focused on human-centered approaches to studying and designing social algorithms.\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",1756028,20,social data
Kapil Madathil,,kmadath@clemson.edu,Clemson University,Cyber-Human Systems (CHS),EAGER: CHS: Examining Self-Harm and Suicide Contagion Risks of Viral Social Media Challenges on Youth and Young Adults,"This project will study potentially harmful social media challenges, in which participants record themselves engaging in specific activities, share the experience in social networks, and encourage others to participate. Some challenges have positive effects. For example, the ALS Ice Bucket challenge involved people dumping a bucket of ice water over their heads to raise awareness of and funding for Lou Gehrig's disease. Others, however, encourage people to engage in behaviors that risk physical or psychological harm. For example, the Cinnamon challenge involves eating a spoonful of cinnamon, which can cause severe respiratory distress, while the Blue Whale challenge encourages a series of increasingly self-harmful behaviors culminating in suicide. Although there is much folklore and hearsay around such harmful challenges, actual studies and data about them are sparse. To address this, the project team will conduct a series of interview studies along with quantitative analysis of social network data around the Cinnamon and Blue Whale challenges. Analyses will examine both individual-level and message-level factors that lead people to participate in and spread these challenges. The results will inform the development of preventative measures to mitigate the spread of harmful viral Internet challenges. Findings will also provide a better understanding of how to protect vulnerable individuals who are exposed to the challenges from both the direct potential risks of participating and indirect potential risks (e.g., through the normalization of self-harm and peer pressure to engage in it).\n\nThe project will address a number of specific research questions, including: 1) better assessing the extent to which viral social media challenges have caused real harm and pose a public health risk to social media users, and 2) identifying characteristics of both the messages containing these challenges and the individuals receiving and spreading them that predict risk, adoption, and spread of the challenges. The work is organized into two main thrusts. In the first, the team will conduct semi-structured, retrospective interviews with approximately 30 adolescent and young adult social media users (ages 13-25) and/or their families, who report being harmed or committing self-harm after engaging in social media challenges prevalent at the time of the studies. The questions will be structured to assess factors known to contribute to self-harm contagion effects (such as graphic depiction of the challenge, peer pressure, and support for self-harmful activities), as well as to probe people's own motivations and decision-making around participating in such challenges. The second main thrust involves larger-scale quantitative analysis of digital trace data about self-harmful challenges from five popular social media sites. This portion of the study will first include constructing and cleaning the dataset (both in terms of protecting personal information and capturing data relevant to the challenges). Next, the researchers will employ qualitative coding of how the challenge messages deviate from the Suicide Prevention Resource Center's evidence-based safe messaging guidelines. These analyses are intended to identify strategies that authors use to spread the challenges as well as the potential risk each poses. Finally, computational modeling will relate message characteristics to measures of message reach such as likes, views, and shares. Together, these research activities will provide much-needed empirical evidence of real-world social media behavior and individual decision-making around these harmful challenges. The data can be used to inform theory development regarding the spread of viral challenges, as well as interventions to mitigate their future harms.\n\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",1832904,20,"personal information, social network data"
Emma Spiro,,espiro@uw.edu,University of Washington,Cyber-Human Systems (CHS),CHS: Small: Tracking and Unpacking Rumor Permutations to Understand Collective Sensemaking Online,"This research addresses empirical and conceptual questions about online rumoring, asking: (1) How do online rumors permute, branch, and otherwise evolve over the course of their lifetime? (2) How can theories of rumor spread in offline settings be extended to online interaction, and what factors (technological and behavioral) influence these dynamics, perhaps making online settings distinct environments for information flow? The dynamics of information flow are particularly salient in the context of crisis response, where social media have become an integral part of both the formal and informal communication infrastructure. Improved understanding of online rumoring could inform communication and information-gathering strategies for crisis responders, journalists, and citizens affected by disasters, leading to innovative solutions for detecting, tracking, and responding to the spread of misinformation and malicious rumors. This project has the potential to fundamentally transform both methods and theories for studying collective behavior online during disasters. Techniques developed for tracking rumors as they evolve and spread over social media will aid other researchers in addressing similar problems in other contexts.\n\nTo answer this project's core questions, researchers will develop novel methods to identify and track rumors over time, detecting threads within a rumor story as well as permutations in the rumor itself. This will allow researchers to map rumor trajectories, providing insight into the structure of specific rumors and broader rumor types. Using these maps, researchers will employ complementary quantitative and qualitative analysis, together with conceptual modeling, to better understand online collective sensemaking processes through the lens of rumor permutations. Qualitative analysis will involve classifying and understanding the underlying rationale for different types of permutations, and quantitative models will help researchers understand aggregate patterns of different kinds of permutations across rumors, to draw conclusions about collective sensemaking at scale. Through analysis of social media data and interviews, researchers will refine and extend emerging methods for doing mixed-method analysis on large-scale, online interaction. Work to enhance empirical and conceptual understandings of online rumoring and collective sensemaking will provide an opportunity to extend classical theories of rumor distortion based on ideas of leveling, sharpening and assimilation and to develop new theories for the spread of complex stories online, incorporating story dynamics and factors specific to online spaces.",1715078,20,social media data
Ryan Henry,,henry@indiana.edu,Indiana University,Secure &Trustworthy Cyberspace,SaTC-BSF: CORE: Small: Collaborative: Making Blockchains Scale Privately and Reliably,"This research explores ways to simultaneously improve both the scalability and privacy of blockchain technologies. A blockchain is a massively distributed, append-only log of transactions that is cryptographically protected from tampering. Thanks to their capacity towards facilitating fast and inexpensive transactions across the globe and their powerful scripting-language support for complex financial instruments, blockchains have already proven to be a highly disruptive force in the finance and e-commerce sectors. Nevertheless, at least two major hurdles still stand in the way of mainstream blockchain acceptance: 1) 'traditional' blockchain architectures are not sufficiently scalable to handle the ever-growing base of blockchain users and the resulting proliferation of transactions, and 2) despite myriad efforts to improve blockchain privacy, users of public blockchains remain susceptible to devastating attacks on the privacy of their accounts and transactions, which often lead to security breaches causing financial losses for the victims. The approaches explored in this project provide privacy for blockchain users where previous efforts have failed by rethinking the one-size-fits-all approach of using Tor to solve all problems requiring anonymity and/or unlinkability. Indeed, running complex protocols over Tor is fraught with risks and can open users to subtle-yet-devastating deanonymization attacks; the tailor-made solutions developed in this project leverage domain-specific knowledge to mitigate these risks.\n\nThe project addresses privacy concerns as they relate both to traditional blockchain transactions and to newer 'payment channel network' transactions. Payment channel networks promise greatly improved scalability by allowing secure (off-chain) payment requiring no interaction with the blockchain ledger. In the context of traditional blockchain transactions, this research develops innovative ways to 1) privately publish transactions to a blockchain by integrating tailor-made anonymous communication protocols directly into the blockchain communication infrastructure, and 2) privately retrieve transactions from a blockchain using carefully optimized private information retrieval (PIR) protocols that support expressive blockchain queries. In the context of payment channel networks, the project 1) explores the (im)possibility of performing off-chain transactions privately, and 2) develops a new theoretical framework and toolkit of algorithms for ensuring availability and quality of service for payment channel transactions in extreme adversarial conditions. Additionally, as part of this project, the multi-institution and transnational team of PIs are deploying a distributed instantiation of the new private blockchain transaction retrieval solutions, which will be open to use by the public. Along with training graduate students, the project puts a major emphasis on undergraduate involvement in this emerging area of blockchain research.",1718595,3,private information
Quanyan Zhu,,quanyan.zhu@nyu.edu,New York University,Secure &Trustworthy Cyberspace,EAGER: Behavior-Based Incentive Mechanism Design for Crowd Defense against Phishing Attacks,"This research is aimed at preventing harm from phishing attacks. Phishing attacks have been on the rise in the last few years with nearly 450,000 attacks and record estimated losses of over USD $5.9 billion just in the year 2013 alone. These attacks attempt to acquire personal information, such as username and passwords, through fraudulent emails. Phishing emails are becoming more targeted, using personal information about their intended victims, in an attempt to seem like authentic emails and improve the response rate to the attacks. If a large number of participants who receive a phishing email can respond to it strategically using fake or deceptive information, the attacker will be overwhelmed and misled by the number of responses, and thus making it more costly to locate the victim. In this way, vulnerable users who tend to fall victim to phishing scams will be hidden in a number of fake responses. The cooperative large-population behavior of the defenders can increase significantly the cost of attack, and hence reduce the economic incentives for the attacker to send phishing email. This type of defense mechanism is called crowd defense. This project will holistically explore the psychological, economic, behavioral, and technical aspects of crowd defense mechanism design.\n\nCrowd defense is a critical defense mechanism to reverse the asymmetric advantage from the attacker to the defender. The research aims to understand the psychological and economic factors in the behavior of crowd defenders, create essential behavioral game-theoretic design frameworks, and develop a proof-of-concept automated software system that enables users to coordinate and respond automatically to phishing. The analytical and design methodologies developed for anti-phishing crowd defense can be broadly applied to other security problems, such as distributed denial of service attacks (DDoS), advanced persistent threats (APT), and coordinated reconnaissance. The tools created in this project will be released as open-source for building a more collaborative and trustworthy cyberspace. The PIs are committed to public education through outreach activities that will further increase the participation of women and minorities as graduate and undergraduate students in the project.",1720230,3,personal information
Ben Hardekopf,,benh@cs.ucsb.edu,University of California-Santa Barbara,Secure &Trustworthy Cyberspace,SHF: Small: Static Analysis for Safe Browser Addons,"This research project helps secure the privacy of web browser users. It specifically targets the browser addon framework, which allows third-party developers to extend the browser's functionality. These addons are written in JavaScript and have extraordinary privileges and access to sensitive user information. Therefore, vetting third-party addons to prevent malicious or accidental security violations is critical. However, the current vetting process for browser addons is manual and ad-hoc, making this process both tedious and error-prone. The goal of this research project is to enhance and automate addon vetting by using static analysis for JavaScript to enforce formal security policies.\n\nThe approach taken by this project is three-fold: (1) design formal security policies to provide provable guarantees; (2) create a provably-sound static security analysis for JavaScript-based browser addons; and (3) develop new tools for explaining security problems in addon code so that third-party developers can revise insecure add ons to eliminate vulnerabilities. This work benefits society as a whole by giving people assurance that their sensitive information is being treated securely. The work also benefits academia and industry by providing the first-ever provably-sound JavaScript static analysis for browser addons. The techniques that are developed will advance understanding of how to usefully analyze dynamic languages such as JavaScript, and the analysis framework itself will enhance research infrastructure by providing a platform for researchers to develop static analyses for JavaScript.",1319060,3,"user information, sensitive information"
Aleksandra Slavkovic,,sesa@psu.edu,Pennsylvania State Univ University Park,"METHOD, MEASURE & STATS, DATANET, Secure &Trustworthy Cyberspace, SCIENCE RESOURCES STATISTICS",Collaborative Research: Record Linkage and Privacy-Preserving Methods for Big Data,"This research project will develop sound statistical and machine learning techniques for preserving privacy with linked data. Social entities and their patterns of behavior is a crucial topic in the social sciences. Research in this area has been invigorated by the growth of the modern information infrastructure, ease of data collection and storage, and the development of novel computational data analyses techniques. However, in many application areas relevant and sensitive information is commonly located across multiple databases. Data analysis is inherently impossible without merging databases, but at the cost of increasing the risk of a privacy violation. This research will address the problem of how to perform valid statistical inference in the presence of multiple data sources, data sharing, and privacy in the age of ""big data."" The investigators' new modeling construct for inference and uncertainty quantification will contribute to both statistics and the many disciplines for which statistics is a principal tool. The methods will have a wide range of applications in the social, economic, and behavioral sciences, including medicine, genetics, official statistics, and human rights violations. The investigators will collaborate with post-doctoral researcher and with graduate and undergraduate students. The statistical methods will be encapsulated in open-source software packages, allowing off-the-shelf use by practitioners while facilitating more detailed control and extensions.\n\nThis interdisciplinary research project will improve upon methods in record linkage and privacy using state-of-the-art techniques from statistics and machine learning. Record linkage is the process of merging possible noisy databases with the goal of removing duplicate entries. Privacy-preserving record linkage (PPRL) tries to identify records that refer to the same entities from multiple databases without compromising the privacy of the entities represented by these records. The research will focus on three aims: (1) development of new Bayesian methods for PPRL, where the error can be propagated exactly across the entire linkage process and into statistical inference, including new privacy measures to capture a tradeoff between utility and risk of any individual risk in a linked database; (2) development of new robust methods for realizing synthetic data releases post-linkage with differential privacy guarantees and its relaxations to address additional layers of privacy and support broader data sharing; and (3) exploration of ""big data"" methods such as variational inference to address scalability and latent cluster exchangeability issues existing within linkage and privacy, such that the new methods can scale to multiple and large databases. The new methods will be scalable and assess uncertainty throughout the entire linkage and privacy process and can be evaluated using Bayesian disclosure risk and Bayesian differential privacy. The project is supported by the Methodology, Measurement, and Statistics Program and a consortium of federal statistical agencies as part of a joint activity to support research on survey and statistical methodology.",1534433,3,sensitive information
Prakash Narayan,,prakash@eng.umd.edu,University of Maryland College Park,COMM & INFORMATION FOUNDATIONS,CIF: Small: Secure and Private Function Computation by Interactive Communication,"This research takes an information theoretic approach to develop principles that govern secure or private function computation by multiple terminals that host user data. The goal of the terminals is to compute locally and reliably, a given function of all the possibly correlated user data, using an interactive communication protocol. The protocol is required to satisfy separate security and privacy conditions. The former stipulates for each terminal that a coalition of the remaining terminals should glean no more information about the data at the terminal from their own data and the communication -- than can be obtained from the function value. The latter protects each individual user's data at a terminal from a similar coalition. A common framework is developed for analyzing the distinct concepts of security and privacy, and new information theoretic formulations and approaches are proposed with the objective of understanding basic underlying principles. Potential applications arise, for instance, in: hospital databases that store clinical drug trial results or university databases with student performance records; private information retrieval from user data stored in private clouds; and security and privacy certifications for the identities/locations of communities and individuals participating in crowd-sourced traffic and navigation services. \n\nThe investigators' technical approach involves the development of a theory with three main distinguishing features. It (i) establishes a key role for interactive communication in reducing communication complexity, and in enhancing security and privacy; and formulates computable measures of security and privacy in terms of conditional Renyi entropy; (ii) provides a common framework for formulating and analyzing problems of secure and private function computation with prominent roles for classical Shannon theory as well as zero-error combinatorial information theory; and introduces the concept of a multiuser privacy region for quantifying privacy tradeoffs among users; and (iii) develops a new method for obtaining converse bounds for communication complexity, upon analyzing the common randomness or shared information generated in function computation with an interactive communication protocol. Rooted in information theory, estimation theory and theoretical computer science, a central objective of the research is to elucidate tradeoffs among computation accuracy, terminal security and user privacy; key to these tradeoffs is the essential role of interactive communication. Furthermore, it aims at creating advances in information theory through the introduction of new models and concepts. Expected outcomes are precise characterizations of the mentioned fundamental tradeoffs, and associated algorithms for secure and private computing.",1527354,26,"private information, user data"
Geraldine Gay,,gkg1@cornell.edu,Cornell University,Cyber-Human Systems (CHS),CHS: Small: Non-use as a Transformative Lens for Understanding Social Technology,"This research will analyze the factors associated with non-use of Facebook, in order to develop a deep, rich understanding of how social technologies more generally mediate and are embedded in complex sociotechnical milieux. It will address three primary research questions: How can our understanding of technology use and ""the user"" be advanced by exploring its relationship with different types, degrees, and varieties of non-use? How is privacy conceptualized and enacted through non-use, and how do those practices help us reconsider the definition and constitution of privacy? What are the processes and experiences of leaving online groups and communities? When particular technologies become nearly pervasive, intentional and pointed absence from them becomes both analytically conspicuous and potentially informative. Questions of group persistence or dissolution apply broadly to a wide variety of domains and contexts integral to the composition of society. This research will help understand how social technologies mediate these processes, thus contributing to the design of systems that better meet the needs of people.\n\nFour lines of research will explore these themes. First, a pair of large-scale surveys, one each during the first and third years, will establish the prevalence of different practices of and motivations for non-use. Second, statistical modeling and network analysis, using both survey data and samples of usage data, will examine how both individual and group predictors relate to these motivations and practices. Third, a series of focus groups involving participants with diverse experiences of use and non-use will help understand conversations and conflicts that may occur around non-use. Fourth, an intervention wherein study participants will be asked to deactivate their Facebook account will enable observation of the processes and experiences of non-use as they happen. \n\nThis project will contribute to and expand on the previous limited work on technology non-use, deepening our conceptualizations both of non-use and of ""the user."" The combination of quantitative and qualitative analyses will build an understanding of the influences on and practices of social technology avoidance. To facilitate research in this area, the anonymized survey data will be made available to other researchers. Finally, this work offers non-use as a potentially transformative lens through which to examine long-standing issues of privacy and groups in technologically-mediated systems.",1421498,20,facebook account
Sarita Schoenebeck,,yardi@umich.edu,University of Michigan Ann Arbor,Cyber-Human Systems (CHS),CAREER: Protecting the Future of Children's Online Identities,"This research will investigate how children's online identities are established and managed as they grow up, especially in the context of technologies that facilitate indexing and resurfacing of personal data. Parents share extensive personal information about their children on social media sites, often starting before their children are even born. As a result, children are now growing up with their online identities formed and shaped from birth through adulthood. However, upon becoming adults, they have little ability to revise or remove what has already been posted about them online. Furthermore, computational advancements like tagging, facial recognition, and voice recognition enable searching and indexing of personal data, yet provide little support for families to make decisions about how that data should be preserved, accessed, or controlled. This work casts a lens on children's online identities as part of a complex sociotechnical system that is formed and shaped by parents, extended familial and social networks, technology companies, policy makers, and children themselves. \n\nThis project will first investigate 1) what information families share about children online; 2) how families think about past identities and future identities being available online; and 3) technology companies' data management and archival practices around children. It will then 4) design and develop prototypes that support families making preservation, access, and control decisions about children's online identities. This research program bridges advances in computation, human-computer interaction, and social computing. It addresses numerous conceptual and technical challenges, including how to support preservation and access decisions about children's identities online, and how to support children's ownership of their own online identities as they grow up. This project includes partnerships with nonprofit agencies to educate families about social media use and sharing personal information online.",1552503,20,personal information
Orville Burton,,vburton@clemson.edu,Clemson University,Cyber-Human Systems (CHS),EAGER: Prototype Tool for Visualizing Online Polarization,"This study will use the 2012 election cycle as a testbed for examining and developing techniques to analyze the implications of the social web for national elections. The social web - broadly defined as the array of technologies that allow individuals to post their thoughts, pictures, and comments in a public forum - has profoundly changed the way in which political candidates, elected officials, and government agencies engage with potential supporters. As an ever-growing number of people join the plethora of available social networks (including Facebook, Twitter, Pinterest, Tumblr, Flickr, and Instagram), politicians across the world sought to develop increasingly sophisticated social web strategies that maximize their ability to engage directly with the public. At the same time, the social web has facilitated the ability of individuals to share ideas, form communities, and coordinate their actions and responses to political campaigns across time and space. Yet the sheer volume of data produced daily through this online civil discourse is overwhelming for researchers and, until recently, has defied our ability to collect, analyze, and comprehend in its entirety. \n\nThis research will develop a prototype visualization tool that will allow researchers to explore the online discourse surrounding elections. This tool will capture social media posts related to selected races in the 2012 Congressional election, both incumbent districts and open seats. Monitoring and analyzing the conversations relative to these races, this study will seek to determine any correlation between social media strategies employed by political candidates in the United States and any increase in polarization in the online discourse. To analyze this discourse, the research will explore the extent that those participating in an online discourse move towards a group polarization with more extreme policies and platforms. Group Polarization is a subset of research on Choice Shifts, which reflect instances where individuals alter their opinions based on commonality, unique information surfacing, or other outside influences. Experimental research suggests that group polarization occurs because the individual has had interaction with a group of like-minded peers. The theory suggests that, after discussion among the group, the individuals will come to a consensus opinion together. Thus, when competing groups form and engage in discourse separately, we are more likely to witness increasingly polarizing opinions between the two groups. This study is relevant for computer scientists, who need to develop strategies for managing, archiving, and providing access to large and dynamic datasets, and is particularly important for social scientists because this online social discourse reflects the moods, values, and attitudes of citizens towards participating in offline civil society. Moreover, scholars need to study how the new social media affect the democratic process of elections. \n\nThis study will provide an opportunity to explore strategies for collecting, aggregating, visualizing, and storing data culled from the social web. It will develop a prototype visualization tool that can be connected via APIs to visualize polarization as manifested through the social web and that will be made available to other researchers interested in studying conversations, sentiment, and the social web. Moreover, this tool will act as a first step in developing a deeper understanding of how to visually map sentiment, political action, and civic discourse over time and space. Additionally, the data collected through this project will provide the basis for future research that will enable a more detailed analysis of the data collected during the election and congressional session.",1247198,30,"flickr, pinterest"
Deborah Estrin,,destrin@cs.cornell.edu,Cornell University,Cyber-Human Systems (CHS),CHS: Medium: Immersive Recommendation Systems: User-Centric Recommendation Models and Applications,"This work strives to create a future in which hyper-personalized content, digital services, and personal information management tools let individuals benefit from the data they generate more directly, selectively, and transparently. Individuals will then be empowered to gain insights into their own behavior, personalize their own experiences, and ultimately more effectively utilize the services to achieve their goals. Moreover, the systems that engage end-users with the data they generate can promote local processing and selective sharing of personal information. Given the pervasiveness of online tools in a person's work, personal, and social life, the actions they take are increasingly shaped by recommendation systems. Today's approaches to personalization and recommendation are provider centric. Society as a whole will benefit from broader exploration of personalization and recommendation from the consumers' perspective. Also along these lines, there is increasing concern about the shifts in expectations for individual control over data sharing. This project's user-centered and personal-sharing-policy-aware design is a potential solution to address this tension between providers and users, allowing people to more directly benefit from their data.\n\nThe research objective is to develop novel user modeling techniques, policy-aware systems, and rich user interactions that allow individuals to harness their own diverse digital traces (""small data""), enable novel applications, and receive more personally-relevant recommendations while limiting privacy exposure. This research will contribute the novel user modeling and interaction techniques needed to put the individual at the center of their personalization, in particular: (1) Immersive user modeling techniques that analyze diverse types of user data, including social media streams, private text communications, web browsing, geo-location traces, and personal images, to incorporate users' diverse and idiosyncratic interests. (2) Novel recommendation models and policy-aware software architecture that consists of open source building blocks designed to facilitate generalization of this approach to ingest diverse personal data traces and feed diverse application targets. (3) Methods to understand and address key human-centered challenges in immersive recommender systems through participatory design of the user experience, as well as qualitative and quantitative evaluation of deployed systems and applications.",1700832,20,"user data, personal information"
Xenofon Koutsoukos,,xenofon.koutsoukos@vanderbilt.edu,Vanderbilt University,Secure &Trustworthy Cyberspace,2014 SaTC CyberSpace 2025 Workshop (Cyber2025),"This workshop is to foster discussions among the leading researchers toward a roadmap for foundational research in cybersecurity and cyberspace over the next ten years with a broad perspective, taking into consideration relevant science, technology, and policy issues, and with an emphasis on the social sciences and education. The previous ten years has seen cyberspace become ubiquitous and essential for modern life. It has also seen the emergence of a criminal cyber-underground and exfiltration of corporate and personal information on a massive scale. The next ten years will likely see equally dramatic change too. \n\nThis proposal is for support of this workshop, for travel, and for post-workshop report preparation and publication.",1446142,3,personal information
Ranjit Jhala,,jhala@cs.ucsd.edu,University of California-San Diego,"SOFTWARE & HARDWARE FOUNDATION, Secure &Trustworthy Cyberspace",SHF: Small: Refinement Types For Verified Web Frameworks and Applications,"Title: SHF: Small: Refinement Types For Verified Web Frameworks and Applications\n\nWeb applications play a crucial role in every aspect of our lives, including banking, commerce, education and healthcare and travel. Despite their importance and ubiquity, they remain notoriously hard to design, develop and deploy as they span several tiers and languages: an HTML/JavaScript client that runs on users' browsers, a central server that implements the application's logic in the ""cloud"" and a database that stores persistent user data. The goal of this research is to build upon recent advances in SMT-based software verification to develop reliable and secure web frameworks. The intellectual merits of this research are new techniques for specifying and verifying multi-lingual systems spanning databases, scripting languages and service protocols, which are essential ingredients of large software systems. Thus, the project's broader significance and importance is that it will lower the cost of constructing robust web applications, with strong guarantees about the protection of sensitive data about the users' health, finances and other personal information.\n\nTo mitigate the impedance mismatch across tiers, developers use web frameworks which have simplified the construction of web applications. However, reliability and security concerns still cut across multiple tiers and languages, making them hard to achieve. This research will use refinement types to specify and verify properties for frameworks and end-to-end safety and security properties for applications. In particular, it will build on the refinement type system of LiquidHaskell, developed in preliminary research, to verify existing frameworks like Haskell's popular SNAP framework, and use it to develop a suite of applications with strong guarantees, thereby showing how the idea can be widely adopted in mainstream frameworks and languages.",1422471,3,user data
Qi Han,,qhan@mines.edu,Colorado School of Mines,CYBER-PHYSICAL SYSTEMS (CPS),CPS: Synergy: Collaborative Research: Enabling Smart Underground Mining with an Integrated Context-Aware Wireless Cyber-Physical Framework,"To reduce reliance on other countries for minerals (e.g., coal, rare-earth metals), the USA has seen an invigoration of mining activity in recent years. Unfortunately, miners often have to work in dangerous environments where there is risk of mine explosions, fires, poisonous gases, and flooding in tunnels. Mine accidents have killed over 500 US and 40,000 mine workers worldwide in the past decade. Most of these accidents occurred in structurally diverse underground mines with extensive labyrinths of interconnected tunnels, where the environment continually changes as mining progresses and machinery is repositioned, complicating search and rescue efforts. In recognition of the severity of the problem, the Mine Improvement and New Emergency Response Act passed in 2006 mandated mines to monitor levels of methane, carbon monoxide, smoke, and oxygen to warn miners of possible danger due to air poisoning, fire, or explosions. The Act also mandated plans to rapidly and safely respond in post-accident scenarios, involving two-way, wired or semi-wired tracking and communication systems that could save lives during entrapment and water inundation emergencies. But the high cost of deploying such a safety infrastructure encourages companies today to meet only the bare minimum required safeguards. This project will involve transformative, foundational, and synergistic research that is necessary to overcome monitoring, communication, and tracking challenges in the underground mining context, to realize a cost-effective safety infrastructure that can be deployed in any type of underground mine. Such a framework will not only minimize the risks facing hundreds of thousands of miners in the USA today, but the foundational research outcomes will also be applicable to a wide range of applications in the realms of Smart and Connected Communities (S&CC) and Internet of Things (IoT), wherever the emphasis is on creating smart workplaces, sustainably operating in harsh environments, and improving human safety.\n\nThe principal objective of this proposal is to devise, design, prototype, and test a fundamentally novel wireless cyber-physical framework of low-cost, energy-efficient, and reliable sensor nodes and commodity smartphones for monitoring, tracking, and communication, to improve miner safety in underground mines. This synergy project contributes to the science and engineering principles needed to realize Cyber-Physical Systems and seeks to grow at the intersection of three research thrusts: quality-aware voice and data streaming, mobile computing assisted location tracking, and computational electromagnetics driven wireless signal characterization. These three thrusts (1) introduce novel mechanisms to enable the co-existence of high quality voice streams with environmental sensor data streams in low-power wireless mesh networks of sensor nodes operating in noisy underground environments; (2) develop schemes for energy-efficient scheduling of location queries and error-tolerant indoor localization to locate individual miners and groups of miners underground; and (3) characterize wireless signal behavior with electromagnetic modeling in highly complex and uncertain environments, based on measurements from a real underground mine, to guide optimal placement of wireless nodes in mining tunnels. Not only is the convergence of these thrusts novel as a whole, but also the techniques and insights developed for each thrust are transformative and go beyond conventional approaches. Collaboration with a mining company for technology transfer will enable rapid real-world deployment of the proposed research. The broader impacts of the research will tightly integrate research results into all levels of teaching, including graduate, undergraduate, and K-12 education; broaden the participation of women and minority students in Cyber-Physical research; and integrate research into the syllabi of existing and new courses.",1646576,29,location tracking
Douglas Oard,,oard@glue.umd.edu,University of Maryland College Park,INFO INTEGRATION & INFORMATICS,III: Small: Safely Searching Among Sensitive Content,"Today's search engines are designed principally to help people find what they want to see. Paradoxically, the fact that search engines do this well means that there are many collections that can't be searched. Citizens can not yet search some government records because of intermixed information that may need to be protected. Scholars are not yet allowed to see much of the growing backlog of unprocessed archival collections for similar reasons. These limitations, and many more, are direct consequences of the fact that current search engines can only protect sensitive content if that sensitive content has been marked in advance. As the volume of digital content continues to increase, current approaches based on manually finding and marking all of the sensitive content in a collection simply cannot affordably accommodate the scale of the challenge. This project will address that challenge by creating a new class of search algorithms that are designed to balance the searcher's interest in finding relevant content with the content provider's interest in protecting sensitive content. This technology will benefit society by dramatically altering the way we approach challenges such as government transparency, personal and enterprise information management, civil litigation and regulatory investigations, and scholarly access to archival materials.\n\nThe project will leverage evaluation-driven information retrieval techniques to optimize a unified objective function that balances the value of finding relevant content with the imperative to protect sensitive information. This will require developing a new class of evaluation measures that are sensitive to both value (relevance) and cost (sensitivity). Factorial vignette survey techniques will be used to elicit the context-appropriate balance of access and restriction for representative applications. The survey results will then be used to inform the design of the feature sets on which evaluation-driven information retrieval techniques depend. Initial experiments will be conducted in protected environments, both locally and as shared-task evaluations on collections that can be licensed for research use under terms that preclude inappropriate disclosure. Ultimately, the project seeks to develop a process for evaluating algorithms for search among sensitive content using an algorithm deposit model in which the executable search algorithm is sent to the protected data, and only manually vetted evaluation results will be returned to participants.",1618695,19,sensitive information
Emerson Murphy-Hill,,emerson@csc.ncsu.edu,North Carolina State University,Cyber-Human Systems (CHS),EAGER: Cognitive modeling of strategies for dealing with errors in mobile touch interfaces,"Touch interfaces on mobile phones and tablets are notoriously error prone. One plausible reason for slow progress in improving the usability of touchscreen devices is that research and design efforts in human-computer interaction take a relatively narrow focus on identifying, understanding, and eliminating human error, focusing primarily on the specific individual touch interactions rather than the broader task knowledge that a person needs to use a device to do a real-world task. This project takes a different perspective, suggesting that usage errors represent breakdowns in a person's efforts to adapt to the complexity of the tasks and devices, and that many of these problems can be traced to interface designers not inadequately considering the cognitive task knowledge that a user will need to do a task. Many touchscreen design guidelines are arbitrary and based on common practice and established user expectations. This project aims to change the shape of research and practice for touch interaction on mobile devices by introducing sound scientific principles of cognitive task analysis and error analysis to the design and analysis of such interfaces. This project could benefit society by dramatically improving the user interface design of touchscreen interfaces.\n\nThe project will develop cognitive models, which are computer programs that behave in some way like humans. The project will specifically develop cognitive models of touch interaction that could be used by designers when designing new touchscreen interfaces, to identify where users are likely to encounter problems with the new interface. The project includes three stages. The first stage will develop a small set of instrumented interfaces on mobile platforms to collect human data for error-prone tasks. The experiments will examine the variation of the probability of error at specific points during a sequence, the visual feedback provided for specific actions, and the user knowledge of error recovery and avoidance actions. The second stage will follow other successful efforts in cognitive modeling to analyze user data from our exploratory experiment with the goal of developing a representation of the strategies we have observed. An automated process will explore a search space of plausible cognitive strategies to identify models that explain the performance data collected during the first stage. The third stage will validate the hypotheses pertaining to the cognitive strategies that contribute to the successful and unsuccessful use of touchscreen devices that are established during the first two stages. The results will have much wider implications than current models of touch accuracy (which produce little more than target size and spacing guidelines) by focusing on the bigger picture of expertise and failure. The project takes a large step into the cognitive modeling of pervasive but not-yet-unexplored interface problems.",1451172,20,user data
Viacheslav Lyubchich,,lyubchic@umces.edu,University of Maryland Center for Environmental Sciences,Big Data Science &Engineering,BIGDATA: Collaborative Research: IA: Novel Bootstrap Procedures for Efficient Large Social Network Analysis,"Understanding the structure and dynamics of social networks is crucial for detecting any anomalous behavior and for managing its impacts. Most existing approaches view a network as a series of snapshots, where a snapshot represents the state of a network in a given time period. Therefore, different network operations need to be individually performed over each snapshot. In reality, online social networks are continuously evolving and therefore, network operations should be automatically performed as networks evolve and need to be done efficiently and reliably. Viewing the problem from this perspective allows us to create a solution that supports advanced, real-world use cases such as tracking the neighborhood of a given node or tracking how network connections evolve in time to determine effective marketing campaigns. These examples indicate the need for efficient computing techniques for important network statistics as the large networks evolve over time. To address this problem, the researchers in this project complement existing distributed evolving social graph analysis techniques with bootstrap and other statistical re-sampling based approaches. The ultimate goal is to develop novel data-driven tools so that when needed, not only certain estimates of statistical network models could be computed efficiently but their estimation errors are reliably quantified. \n\nThis project primarily targets development of new efficient and robust methods for anomaly and outlier detection on large sparse networks. The resulting methodology provides the following functions: 1) a computationally efficient finite sample inference for an extensive range of network topology statistics; 2) a flexible data-driven characterization of network structure and dynamics, and 3) comprehensively quantifying uncertainty in modeling and estimation of large networks, without imposing restrictive conditions on network model specification. The expected advances are both in research methods - new approaches to data-driven nonparametric inference for large sparse networks and in substantial enhancement of knowledge of network dynamics and formation in the era of digital communication. The project can significantly benefit students by providing a broad exposure to interdisciplinary applications of large network and fostering awareness of interdisciplinary relationships -- hence enhancing their capacity for critical thinking and opening up new career paths.",1633355,2,social graph
Bamshad Mobasher,,mobasher@cs.depaul.edu,DePaul University,Cyber-Human Systems (CHS),"Student Support for User Modeling, Adaptation and Personalization Conference","User interfaces that adapt themselves to available user information (such as special needs or individual preferences) are becoming increasingly important, so much so that adaptability has become a selling point for software products. A system with the ability to construct and consult a user model (an explicit representation of properties of a particular user or group of users) can adapt diverse aspects of its performance and enhance its effectiveness, usability and/or acceptance in a variety of situations (e.g., to reduce information overload, to improve the quality of information retrieval, filtering and annotation, and to generate useful information visualizations). Applications for user modeling range from electronic commerce and intelligent learning environments to health care and assistive technologies. Relevant platforms for user modeling include mobile and wearable systems and smart environments, as well as individual desktop systems, groupware, adaptive hypermedia, and other web-based systems. \n\nThe annual International Conferences on User Modeling, Adaptation and Personalization (UMAP) are the premier forum at which academic and industrial researchers from all these fields gather to exchange their complementary insights on user modeling issues. UMAP is a merger of the long-running and successful International Conferences on User Modeling (UM, 1986-2007), and the more recent important series of Adaptive Hypermedia and Adaptive Web-Based Systems Conferences (AH, 2000-2008). This is funding to support travel for up to 5 students currently enrolled in PhD programs in U.S. institutions, to present their accepted papers and posters and/or to attend the Doctoral Consortium associated with the 22nd UMAP Conference (UMAP 2014), to be held in Aalborg, Denmark, on July 7-11. More information about the conference is available at http://www.um.org/umap2014/. \n\nThis year's UMAP will once again include a Doctoral Consortium session, thereby continuing the tradition established at UMAP 2009 and before that at both the UM and AH conferences. Lively and useful discussions have enabled students to receive suggestions about their ongoing research and allowed more experienced participants to hear some fresh ideas and view some of the new trends in the field. Students whose work has been selected for presentation at the Doctoral Consortium will be invited to write a paper that will be published in the UMAP 2014 conference proceedings. They will have 15 minutes to present their work (which may include a short demonstration if appropriate), to be followed by an additional 15 minutes for questions and discussion. During both the question/discussion period and in subsequent informal interactions, organizing committee members and other participants will provide constructive comments on each student's work and attempt to address aspects on which the student has requested advice. \n\nAttending and presenting their work at UMAP, the top conference in its field, will have a significant impact on the careers of the future generation of user modeling, adaptation, and personalization researchers. Students who participate in the Doctoral Consortium will also benefit from that experience in several ways. First, they will have the opportunity to present their work to a knowledgeable audience and get useful comments at an early stage of their research when it will be most useful. Just as importantly, they will have an opportunity to meet established researchers and other graduate students doing similar work, to exchange ideas, and to make contacts that will be invaluable to them as they progress in their scientific careers. Interacting with the young researchers is also useful to more experienced investigators, by providing new perspectives. Thus, the Doctoral Consortium is a great confidence builder for the students involved, and highly stimulating to the established researchers who participate. Many past participants in the UMAP Doctoral Consortium have gone on to become well-regarded researchers and practitioners in the field. The organizers have reaffirmed the long-standing and demonstrated UMAP commitment to diversity; to this end, they will make special efforts to recruit participants who are women and members of under-represented groups, and they will ensure institutional diversity by supporting no more than one student from any given university.",1444630,20,user information
Stephanie Teasley,,steasley@umich.edu,University of Michigan Ann Arbor,Cyberlearn & Future Learn Tech,CAP: Doctoral Consortium for the 2015 Learning Analytics and Knowledge Conference,"Using data analytics has revolutionized many academic disciplines, such as Astrophysics and Biology. In addition, it has changed the commercial world and critical services like healthcare. The field of education research is also beginning this change. The Society for Learning Analytics Research (SoLAR) is a key player in effecting this change. This project will support a Doctoral Consortium at the SoLAR 2015 conference - the 5th Annual International Learning Analytics and Knowledge (LAK) Conference, to be held in Poughkeepsie, NY in March 2015. Participating in the consortium will prepare current doctoral students from the diverse research backgrounds that make up the interdisciplinary field of Learning Analytics (LA), including computer science, information science, psychology, communication, education, artificial intelligence, and cognitive science. It will also allow them to network with each other and with professors and practitioners who are currently engaged in learning analytics research and related work to ensure the continued development of this community. These activities are critical and timely. The upcoming generation of LA researchers will play a vital role in realizing the potential of using data to improve outcomes for elementary, secondary, and post-secondary students in learning, motivation, and perseverance.\n\nLearning Analytics is an interdisciplinary field whose goal is to advance and apply knowledge about learning sciences and education to improve all aspects of learning. LA methods include data visualization, data mining, data science, and mixed methods approaches combining qualitative and quantitative methods (e.g., interviews and back-end, clickstream data analysis). Capacity building is a central concern within the LA community. SoLAR has historically addressed these needs, in part, through specialized workshops held in conjunction with the Society's major conference. The Doctoral Consortium workshops host PhD students who are grappling with their dissertation research. This grant provides travel support to US scholars selected through a competitive application process to participate in this event. They present their work for feedback both in the context of workshop, where they receive advice from a panel of expert mentors, and in a poster session in which they interact with the full conference audience. In addition, their work is published in the proceedings of the conference.",1523316,5,clickstream
Jiawei Han,,hanj@cs.uiuc.edu,University of Illinois at Urbana-Champaign,INFO INTEGRATION & INFORMATICS,"III: Small: Multi-Dimensional Structuring, Summarizing and Mining of Social Media Data","Various kinds of social media have impacted billions of users on their ways of obtaining and sharing information across the globe. This creates great opportunities but also poses tremendous challenges on understanding, summarizing, and mining of such data due to its huge volume as well as dynamic and unstructured nature of its text contents. In response to such challenges, this project focuses on text-based social media, proposes a multi-dimensional data structuring approach, which mines unstructured social media data to uncover its hidden multi-dimensional structures. The project investigates principle, methodologies and algorithms for social media structuring, summarizing and mining, and develops effective and scalable technology for multi-dimensional social media data analysis. The principles and methodologies developed in this study can be extended to scalable and multi-dimensional analysis of other kinds of massive unstructured data as well.\n\nTo conduct effective multi-dimensional social media structuring, this project develops a distant supervision-based methodology with minimal effort of human curation and labeling. It takes data in Wikipedia, Freebase, or other knowledge-bases as references, integrates social media data with the corresponding news or other relevant documents, conducts phrase mining, entity and event discovery and typing, and uncover critical aspects, attributes, and values associated with such entities and events from social media. By organizing social media data in a structured way, massive social media can be summarizing effectively in a context-aware semantic OLAP (online analytical processing) framework and can be analyzed systematically under a general multi-dimensional social media querying and mining framework for many tasks, such as modeling behavioral patterns and uncovering bursty events and detecting social frauds or anomalies.",1618481,19,social media data
Mehryar Mohri,,mohri@cims.nyu.edu,New York University,Algorithms in the Field,AitF: FULL: Collaborative Research: PEARL: Perceptual Adaptive Representation Learning in the Wild,"Vast amounts of digitized images and videos are now commonly available, and the advent of search engines has further facilitated their access. This has created an exceptional opportunity for the application of machine learning techniques to model human visual perception. However, the data often does not conform to the core assumption of machine learning that training and test images are drawn from exactly the same distribution, or ""domain."" In practice, the training and test distributions are often somewhat dissimilar, and distributions may even drift with time. For example, a ""dog"" detector trained on Flickr may be tested on images from a wearable camera, where dogs are seen in different viewpoints and lighting conditions. The problem of compensating for these changes--the domain adaptation problem--must therefore be addressed both in theory and in practice for algorithms to be effective. This problem is not just a second-order effect and its solution does not constitute a small increase in performance.  Ignoring it can lead to dramatically poor results for algorithms ""in the field.""\n\nThis project will develop a core suite of theory and algorithms for PErceptual Adaptive Representation Learning (PEARL), which, when given a new task domain, and previous experience with related tasks and domains, will provide a learning architecture likely to achieve optimal generalization on the new task. We expect PEARL to have a significant impact on the research community by providing a much-needed theoretical and computational framework that takes steps toward unifying the subfields of domain adaptation theory and domain adaptation practice. Our theoretical and practical advancements will impact many application areas by allowing the use of pre-trained perceptual models (visual and otherwise) in new situations and across space and time. For example, in mobile technology and robotics, PEARL will help personal assistants and robots better adapt their perceptual interfaces to individual users and particular situated environments.  At the core of this project are three main research thrusts: 1) making theoretical advances for domain adaptation by developing generalized discrepancy distance minimization; 2) using the theoretical guarantees of generalized discrepancy distance to develop algorithms for key adaptation scenarios of deep perceptual representation learning, domain adaptation with active learning, and time-dependent adaptation; 3) advancing the theory and developing algorithms for the multiple-source adaptation scenario. In addition to our core aims, we plan to implement our algorithms within a scalable open-source framework, and evaluate our algorithms on large-scale visual data sets.",1535987,1,flickr
Limin Jia,,liminjia@cmu.edu,Carnegie-Mellon University,Secure &Trustworthy Cyberspace,"SaTC: CORE: Medium: Towards a Usable, Practical, and Provably Secure Browser Infrastructure","Web browsers have become the predominant software platform for interacting with services such as shopping, banking, and health care. Many of the same technologies that enable these applications, such as third-party JavaScript and browser extensions, also have the unintended effect of making users' private information vulnerable to theft or misuse. Existing security policies and mechanisms---such as the same origin policy (SOP), the content security policy (CSP), and permission systems for extensions---are too coarse grained and not sufficient to protect users' data. Prior work has proposed information flow control (IFC) as a solution. However, existing IFC solutions are not yet practical: they suffer from poor usability and struggle to interoperate with other enforcement mechanisms that protect parts of web applications or browser state. The goal of this project is to construct a practical, usable, and secure browser infrastructure.\n\nThe project will develop a modular model for a provably secure browser infrastructure using compositional principles. This model will be instantiated with concrete enforcement mechanisms to develop a browser that meets the functionality standards of current browser platforms. An important part of the project is to understand users' perceptions and expectations with respect to security goals; to extract practical IFC policies based on user data; and to build mechanisms that help users safely navigate the web. Finally, the project will develop testing and validation benchmarks and use these benchmarks to compare the effectiveness and the cost of different enforcement mechanisms. The results of this project will lay the foundation for building practical web browsers that have information flow security built in. This project targets technology that people use every day and has the potential to help not only web application developers but all web users.",1704542,3,"private information, user data"
Apu Kapadia,,kapadia@indiana.edu,Indiana University,Secure &Trustworthy Cyberspace,SaTC: CORE: Medium: Collaborative: Scalable Dynamic Access Control for Untrusted Cloud Environments,"When users store their data in the cloud, they take many privacy risks: Will the cloud storage provider allow others to see that data? If the user sets sharing rules for the data, will the cloud storage system follow those rules? Recent news stories of user data exfiltration from cloud storage systems show that users have reason for concern. Encrypting files before storing them in the cloud would provide strong protection, but this approach makes it very difficult for users to share data with others and to change their sharing policies. This project is exploring techniques to cryptographically protect user files in cloud-based storage systems, while supporting advanced, dynamic sharing policies.\n\nWhile cryptographically protecting data under a static access control policy is well documented in the literature, existing constructions either do not efficiently support dynamic policies (e.g., changes to role/attribute assignments) or make heavy trust assumptions to support this dynamism. This project is (1) developing an open-source platform for prototyping, analyzing, and deploying dynamic access control enforcement solutions for untrusted environments; (2) creating cryptographic constructions that are capable of securely implementing popular role- and attribute-based access control models on untrusted storage platforms while supporting dynamic policy and data updates; (3) designing efficient, trusted hardware-assisted, cloud-scale implementations of popular access control models supporting dynamic policy and data updates in a variety of deployment scenarios; and (4) a carrying out a comprehensive evaluation that explores the trade-offs in trust between these cryptographically- and hardware-enforced approaches and examines the cryptographic, computational, and communication costs of the proposed constructions under a variety of real-world workloads.",1703853,3,user data
Yilu Zhou,,yzhou62@fordham.edu,Fordham University,Secure &Trustworthy Cyberspace,EAGER: Can You Trust Apps Age Recommendations? Inconsistent and Unreliable Maturity Ratings on Mobile Platforms,"While smart phones provide an excellent way for communication, entertaining and education, they also raise many privacy and security concerns. Children are facing the risks of being exposed to inappropriate content due to mis-rated Apps. Both Android and iOS apps come with maturity ratings that examine the existence and intensity of mature themes within each app. However, each mobile platform adopts its own rating policy and rating strategy which creates inconsistency and inaccurate ratings. The maturity ratings for Android apps are purely a result of app developers' self-report. Many claim that the Android rating policy is unclear, and it is difficult for developers to understand. A more critical risk resides in in-app advertisements. Many apps, especially the free ones, are connected to third party advertisements. Neither mobile platforms nor advertising networks apply these maturity policies to restrict the contents of in-app advertisements. However, this phenomenon has not been studied, nor have the factors that may lead to untruthful maturity ratings been explored. Thus, the risks associated with content inappropriateness are unknown. This project develops mechanisms to compare, analyze and verify the maturity ratings of mobile apps and in-app advertisements, and investigates possible reasons behind the inaccurate ratings. A variety of data will be collected to support the analysis including Web data crawled from the Web, App data from decompiled app code, and advertisement data collected in a number of ""demo apps.""\n\n\nThis project adopts a multi-disciplinary approach to compare and understand the maturity rating policy difference among different platforms. It plans to investigate the current maturity rating framework on Android, iOS and other third-party authorities such as ESRB. By comparing the same app that appears on both Android and iOS app ratings, the project studies if ratings are reflected in app descriptions, user reviews, developer information, etc. App log data will be collected to analyze content maturity of in-app advertisements. The project will then build an effective text mining approach to estimate the true rating of an app. Using this as a foundation, the project will further analyze and evaluate a large number of Android and iOS app ratings as well as in-app advertisement content. Statistical analysis will be performed to understand the factors that lead to mis-rated maturity ratings.",1551004,3,"app data, log data"
Chaitanya Baru,,baru@sdsc.edu,University of California-San Diego,INFO INTEGRATION & INFORMATICS,WBDB2012: Workshop on Big Data Benchmarking 2012,"With the exponential increase in the size, complexity, and rate of acquisition of diverse types of data, there an urgent need for new techniques for managing and analyzing such data. In this context, there is a critical need for benchmarks to facilitate evaluation of alternative solutions and provide for comparisons among different solution approaches targeted to big data applications. Benchmarks need to capture a variety of characteristics of big data storage, management, and analytics including new feature sets, enormous data size, largescale and evolving system configurations, shifting loads, and the heterogeneous technologies of big-data and cloud platforms. The benchmarks are inadequate for assessing emerging big data platforms, systems and in software such as SQL, NoSQL, and the Hadoop software ecosystem; different modalities or genres of big data, including graphs, streams, scientific data, document collections, and transaction data; new options in hardware including, HDD vs SSD, different types of HDD, SSD, and main memory, and large-memory systems; and, new platform options that include dedicated commodity clusters and cloud platforms.\n\nThe Workshop on Big Data Benchmarking 2012 represents an important step towards the development of a suite of benchmarks for providing objective measures of the effectiveness of hardware and software systems dealing with big data applications. The objective of this invitation-only workshop is to identify key issues and to launch an activity around the definition of reference benchmarks that can capture the essence of big data application scenarios. The effort aims to arrive at a set of objective measures and benchmark datasets to characterize and compare the performance of and the price/performance tradeoffs of alternative solutions for big data storage, retrieval, processing, and analysis problems. The workshop brings together a group of about 40 experts from academia and industry with backgrounds in big data, database systems, benchmarking and system performance, cloud storage and computing, and related areas.The industries represented range from hardware, software, analytics, and applications. The group will develop a draft of a report describing a big data benchmark suite that will be widely disseminated on the web and through presentations and oureach activities at the relevant conferences and workshops.\n\nBroader Impacts: The availability of the big data benchmark suite will facilitate research and technological advances by providing objective measures for comparing alternative solutions to key big data problems.",1241838,30,transaction data
Hongxin Hu,,hongxih@clemson.edu,Clemson University,INFO INTEGRATION & INFORMATICS,III: Small: Collaborative Research: Privacy-Aware Collaborative Data Sharing in Human-Centered Social Networks,"With the help of Online Social Networks (OSNs), people share personal and public information and make social connections with friends, coworkers, colleagues, family and even with strangers. As a result, OSNs store a huge amount of sensitive information about users and their interactions. To protect such information, privacy control has been treated as a central feature of OSNs. Although OSNs currently provide some privacy control mechanisms allowing users to regulate access to information they share, users, unfortunately, have no control over data others share. For instance, if a user posts a comment in a friend's space, s/he cannot specify which users can view the comment. Similarly, when a user uploads a photo and tags friends who appear in the photo, the tagged friends cannot restrict who can see this photo. Since multiple associated users may have different privacy concerns over the shared data, privacy conflicts occur and the lack of collaborative privacy control increases the potential risk of leaking sensitive information by friends to the public. To address such a critical issue, it is essential to accommodate diverse privacy control requirements coming from multiple associated users for collaboratively managing the shared data in OSNs. The new techniques developed in this project will substantially enhance the state-of-the-art in privacy-aware data sharing in OSNs and will have implications for the design of future collaborative sharing systems in OSNs. Moreover, since privacy practices in many other collaborative environments, such as electronic health records and financial data sharing also require multiple users to co-manage the privacy of information, the fundamental results generated by this project could be expanded to those collaborative environments beyond social networks. \n\nThe goal of this project is to seek an effective and flexible mechanism to enable privacy-aware collaborative data sharing in OSNs. To this end, the researchers will first analyze data sharing associated with multiple users in OSNs, and articulate several typical scenarios of privacy conflicts to understand the risks posed by those conflicts. To mitigate risks caused by privacy conflicts, the researchers will investigate a collaborative data sharing mechanism to support the specification and enforcement of multiple privacy concerns. In addition, a systematic conflict detection and resolution mechanism will be created and evaluated with respect to its ability to cope with privacy conflicts occurring in collaborative management of data sharing in OSNs. The conflict resolution in the project attempts to balance the need for privacy protection and users' desire for information sharing by quantitative analysis of privacy risk and sharing loss. Another compelling feature of the proposed approach is the support of both theoretical and empirical analyses on privacy control in OSNs. The researchers will analyze the strategic behaviors of rational users using a game-theoretic model, where each player aims at accommodating her/his privacy concerns as much as possible by adjusting her/his privacy setting in collaborative data sharing in OSNs. Furthermore, the researchers will carry out empirical analysis for practical user behaviors and contrast it with the theoretical analysis in collaborative data sharing, articulating the gap between the theoretic model and real user behaviors. More details about this project, including experimental data and curricular materials can be found on the project website (www.cs.clemson.edu/~hongxih/projects/gpc).",1527421,19,sensitive information
Domenic Forte,,dforte@ece.ufl.edu,University of Florida,Secure &Trustworthy Cyberspace,SaTC: STARSS: Small: iPROBE - An Internal Shielding Approach for Protecting against Frontside and Backside Probing Attacks,"With the proliferation of electronics into every day life, integrated circuits (ICs) process and store more sensitive information than ever before. The extraction of on-chip assets, such as keys, firmware, personal and information, threatens state-of-the-art military technologies, commercial industries, and society alike through counterfeiting, theft, fraud, development of exploits, and much more. Although protection against software and non-invasive methods of extraction has been widely investigated, physical probing has received little attention. In particular, Focused Ion Beam (FIB) is a powerful tool that allows attackers to not only to access and probe assets, but to destroy and/or bypass existing countermeasures. Since FIB capabilities are almost limitless, the best approaches should make probing as costly, time consuming, and frustrating as possible. However, a significant barrier in doing so lies in the fact that the time, effort, and cost to design a FIB-resistant chip must remain reasonable, especially to designers who are not security experts. \n\nThis project investigates iPROBE, the first ever computer-aided design (CAD) approach aimed at hindering frontside and backside probing attacks on integrated circuits. As a CAD solution, iPROBE relieves the designer's burden by automatically balancing the security and overhead of various countermeasures. Compared to ad hoc countermeasures such as top level meshes, it also allows protection to be concentrated on only the most sensitive portions of a design, thereby lowering cost. iPROBE takes design assets as input, and uses information-theoretic and test-inspired metrics to identify all nets requiring protection. During physical design, nets are ranked in terms of their sensitivity and vulnerability to probing. Internal shields are constructed using existing functional nets as well as additional test nets to surround the highest ranked nets. Cutting through the functional nets ideally renders the chip useless or destroys the sensitive data. Similarly, cutting through test nets can be detected and used to trigger self-destruction. t-private circuits and other countermeasures are integrated with the internal shield to further increase attack complexity. For evaluation, benchmark circuits are implemented with conventional flows and with iPROBE. Area, power, and timing between the two are compared to estimate the iPROBE's impact on performance. Security is evaluated using a custom-built IC probing evaluation tool previously developed by the PIs (with upgrades for backside attack evaluation) and using FIBs in the PIs' lab to execute real attacks on iPROBE-designed chips that are fabricated through MOSIS.",1717392,3,sensitive information
Gang Wang,,gangwang@vt.edu,Virginia Polytechnic Institute and State University,Secure &Trustworthy Cyberspace,SaTC: CORE: Small: Securing Web-to-Mobile Interface Through Characterization and Detection of Malicious Deep Links,"With the wide adoption of mobile devices, mobile websites and applications (apps) have become the primary interfaces to access online content. Mobile deep-linking, a key mechanism to index mobile content, has been widely used to enable users to navigate across websites, search engines and mobile apps and become instrumental to improving users' online experience. Unfortunately, mobile deeplinks are designed without security in mind, which allows malicious apps or websites to launch stealthy attacks to hijack user clicks, steal sensitive information and perform phishing attacks. With more apps and websites deep-linked every day, this becomes an emerging security threat to a broad Internet population. This project is developing methodologies and usable tools to measure, analyze and secure mobile deeplinks in today's mobile-web ecosystem.\n\nTo first understand the emerging security threat, the researchers are developing novel measurement frameworks to map out the empirical connections between mobile apps and the web by automatically extracting mobile deeplinks from the market-scale apps. In addition, the researchers are investigating automated techniques to detect deeplink-based abuse without relying on large-scale ground-truth data. One target approach is to construct deeplink graphs for apps and websites and explore detection techniques using semi-supervised learning and trust prorogation schemes. Finally, the researchers are developing analytics tools to extract context information from mobile deeplinks to enable intelligent alert systems to safeguard the deeplink usage. The researchers are investigating the tradeoffs between security and usability to design safer ways of using deeplinks without sacrificing the usability.",1717028,3,sensitive information
April Edwards,,april.edwards@elmhurst.edu,Elmhurst College,Secure &Trustworthy Cyberspace,SBE: Small: An Analysis of the Relationship Between Cyberaggression and Self-Disclosure among Diverse Youths,"Youths of the digital age live parallel lives online and in the real world, frequently disclosing personal information to cyberfriends and strangers, regardless of race, class or gender. Race and gender do make a difference, however, when these online disclosures lead to acts of cyberaggression. The PIs' previous work revealed that some youths are resistant to cyberaggression and that there are differences in perceptions of cyberbullying among youths from different cultural and racial backgrounds. This research aims to explore the relationship between youths' self-disclosures, cultural backgrounds, and their perceptions of cyberaggression.\n\nThe PIs conduct a longitudinal, interdisciplinary study that builds upon their ongoing cyberaggression pattern recognition research by: 1) using surveys and focus groups to test and refine their theories about self-disclosure, perception, cultural difference, and cyberaggression communication patterns, 2) using machine learning to develop detection and response technologies for use in applications designed to protect youths, 3) using focus groups to evaluate the applications, and 4) making the data collected from this project available to the research community. This work is important to understand the role of self-disclosure in cybervictmization among youths, and provides the theoretical groundwork for the development of effective response strategies that can be employed by youths when they are attacked online. The data from this study will provide a rich source of material for other researchers in both computer science and in the social and behavior sciences.",1812380,3,personal information
